# NPLinker configuration file
# ---------------------------

# general options
# log level (DEBUG/INFO/WARNING/ERROR)
loglevel = "INFO"
# if you want to redirect log messages from stdout to a file, set a valid path here
logfile = ""
# and if you still want to see log messages on stdout in addition to writing to
# the logfile, set this to true
log_to_stdout = true
repro_file = ""

# scoring_methods to be enabled. currently available:
#   - metcalf
#   - rosetta
#   - testscore (only for debug use)
#
# The default is to enable all available methods.
# scoring_methods = ["metcalf", "rosetta"]

# Dataset configuration
# ---------------------
# NPLinker supports two basic methods for loading datasets:
# 
#   1. All files stored locally
#   2. Some/all files retrieved from the paired omics platform (http://pairedomicsdata.bioinformatics.nl/)
#
# The method you want to use determines the values that should be populated in 
# the [dataset] section below. If working with a purely local dataset, nplinker
# defaults to looking for all the necessary files in a single directory, given
# by the "root" parameter. 
#
# Alternatively, to load metabolomics data from the paired platform, set the "root"
# parameter to "platform:<projectID>", where "<projectID>" is taken from the platform 
# project list. For example, "platform:MSV000079284" would select the dataset with
# the ID MSV000079284.
#
# For more details see below. 
#
# 1. Loading local datasets
# -------------------------
# Generally speaking the dataset layout the application expects matches the structure
# of the output from a GNPS job. Workflows that are known to work so far are:
# 
#  - METABOLOMICS-SNETS (version 1.2.3) 
#  - METABOLOMICS-SNETS-V2 (version release_14) 
#  - FEATURE-BASED-MOLECULAR-NETWORKING (version 1.2.3)
#
# The simplest starting point is to download the "Clustered Spectra as MGF" zip 
# file from GNPS and extract that into a folder to serve as the root directory for the dataset.
# antiSMASH and BiG-SCAPE output are then added as additional subfolders. 
#
# Typically all you will need to do is tell nplinker where the root directory
# is located. Otherwise you can customise the locations of the individual elements
# using the various override settings below. 
#
# The layout is as follows (see the documentation for more details):
# <root>
#   |- strain_mappings.csv (strain ID mappings)
#   |   (METABOLOMICS DATA)
#   |- clusterinfo*/<UID>.tsv (spectrum metadata, NOTE: NPLinker will search any folder beginning with "clusterinfo")
#   |- metadata_table/metadata_table-00000.txt (only present in FBMN GNPS jobs?)
#   |- networkedges_selfloop/<UID>.selfloop (the "edges" file for spectra network)
#   |- spectra/*.mgf (mass spec data)
#   |- quantification_table_reformatted/<UID>.csv ("extra" spectrum metadata, only present in FBMN jobs? TODO proper name for this?)
#   |- DB_result/*.tsv (GNPS and other spectral annotation files, optional)
#   |- result_specnets_DB/*.tsv (GNPS and other spectral annotation files, optional)
#   |- DB_result/annotations.tsv (annotation data to extract from each file, see docs for details)
#   |- params.xml (optional, params.xml from GNPS job output)
#       (GENOMICS DATA)
#   |- antismash/*.gbk (antiSMASH GenBank files for the BGCs in the dataset)
#   |- bigscape/<classes subfolders> (BiG-SCAPE clustering/annotation files in their subfolders)
#       (MISC DATA)
#   |- description.txt (a freeform optional text file containing information about a dataset)
#   |- include_strains.csv (optional list of strain IDs to explicitly include when loading, others excluded)
#
# 2. Loading platform datasets
# ----------------------------
# Given a platform ID, nplinker will retrieve the associated metabolomics data using the 
# GNPS task ID. By default, it will also attempt to retrieve any available 
# genomics data from the antiSMASH database using the RefSeq accession labels 
# in the platform project data. 
# 
# However, if you have local antismash results instead, you should also set the
# location of those files using the "antismash_dir" parameter in the 
# [dataset.overrides] section. nplinker can optionally also run bigscape on 
# these results during the loading process. If you already have bigscape results
# you can additionally set the "bigscape_dir" parameter to the appropriate 
# location to skip this step (or simply place them inside the <root>/bigscape folder)
#
# All files are downloaded and extracted to locations inside ~/nplinker_data. On Windows
# this corresponds to C:\Users\<username>\nplinker_data.

# Dataset configuration settings
[dataset]
# If the dataset has the expected directory structure, this value is all that's required.
# It should be set to the path of the local directory containing the various data files
# described above.
# 
# If you want to load a dataset from the paired platform, the value should be a string
# of the form: "platform:datasetID". For example, "platform:MSV000079284" would 
# load the dataset with ID MSV000079284. 
root = "<root directory of dataset>"

# you can optionally set the BIGSCAPE clustering cutoff value here. the default value
# is 30, but any of the valid BIGSCAPE clustering thresholds can be used assuming the
# corresponding files exist in the dataset. Also note that it's possible to change this
# value after the dataset has initially been loaded, which will cause only the affected
# data to be reloaded. Possibly only useful for the webapp
#bigscape_cutoff = 30

# For datasets using the GNPS FBMN workflow, NPLinker can optionally try to parse
# more data out of the "metadata table" file included in the output. This can both
# simplify the process of creating a strain mappings file and allow NPLinker to 
# extract growth media labels for each strain, which are then displayed in the 
# web application. This option defaults to being disabled to provide a consistent
# experience with other types of dataset, but you can choose to enable it if you
# have a compatible metadata table file. 
#
# For more information: https://github.com/sdrogers/nplinker/wiki/Strain-mappings
#extended_metadata_table_parsing = false

[antismash]
# antismash file structure. Should be either 'default' or 'flat'. 
# default = the standard structure with nested subdirectories
# flat = all .gbk files placed in a single flat directory
# TODO: is "flat" required any more? 
antismash_format = "default"

# NPLinker needs to know how to parse antiSMASH filenames from BiG-SCAPE output
# to identify strain labels as part of the process of loading a dataset. 
# Since the format of the filenames can vary from dataset to dataset, there isn't
# a single rule that can be applied here. In most cases, the required action is
# "take all text up to the first occurrence of <character>", e.g. the first 
# underscore or period character. By default NPLinker will try the set of characters
# defined below in sequence until a match is found. If you need to override this
# then define a new list of at least one character to search for (note that it 
# doesn't need to be a single character, e.g. ".abc" would also work)
# antismash_delimiters = [".", "_", "-"]

# if using the Docker version of NPLinker which contains BiG-SCAPE and can run it
# automatically, you can choose to have the application automatically rename any
# files under the <root>/antismash folder which contain spaces in their filenames
# (both subfolders and .gbk files). This is to avoid BiG-SCAPE throwing up errors
# because it can't handle filenames with spaces in them. If you already have 
# BiG-SCAPE results or don't want NPLinker to rename anything automatically, set
# this to true instead
ignore_spaces = false

[docker]
# this optional section contains various settings that only apply to the dockerised 
# version of nplinker. The Docker image contains BiG-SCAPE, and this tool can be run
# as part of the dataset loading process. If you want to enable/disable this, and 
# configure the parameters used, this can be done by modifying the settings below. 

# enable or disable running BiG-SCAPE on a dataset being used with the nplinker Docker
# image. If false, you must have existing BiG-SCAPE results for your dataset in the 
# format nplinker expects (described above). If true, the copy of BiG-SCAPE installed
# in the Docker image will be run on the directory containing the antiSMASH data for
# the selected dataset, using the parameters configured below. 
# The default value is true
run_bigscape = true

# when BiG-SCAPE is executed by nplinker, it will automatically configure the following
# parameters:
#   -i/--inputdir
#   -o/--outputdir
#   --pfam_dir
#   --clans-off
# 
# In addition, the --cutoffs parameter will default to:
#   --cutoffs=0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7
# 
# If you wish to change the set of cutoffs, or set any of the other parameters
# not listed above, you can do so by editing the value of this string. For example,
# to use a smaller set of cutoffs and enable verbose mode:
#   extra_bigscape_parameters = "--cutoffs 0.5,0.6 -v"
extra_bigscape_parameters = ""

# this optional section contains various settings that only affect the webapp interface
# for nplinker. If you're not using it, you can leave this out of your configuration. 
[webapp]
# A fundamental part of the webapp is a set of tables which display objects in the dataset
# that have been found to have links, based on the output of the Metcalf scoring method. 
# The value of this parameter is the minimum Metcalf score that a link must have for its
# associated objects to appear in the table. In other words, the higher this is set, the
# smaller the number of objects that will remain to be displayed in the tables. This may
# be useful for improving performance on larger datasets. 
# The default value is 2.0
tables_metcalf_threshold = 2.0

# a section for general scoring parameters (currently unused)
[scoring]

# the "Rosetta" scoring method involves some preprocessing steps that can take
# significant time. nplinker will automatically run these steps as it loads the
# dataset and cache the results. if you would like to adjust the parameters used 
# by the Rosetta method you can do by setting them below (note that changing
# these values will invalidate any cached data and force the preprocessing steps
# to be run again)
# 
# TODO document what these do
[scoring.rosetta]
# ms1_tol = 100
# ms2_tol = 0.2
# score_thresh = 0.5
# min_match_peaks = 1

# when loading, you can also override any combination of individual file paths
# as required (empty paths are ignored)
[dataset.overrides]
# strain ID mapping filename, default is <root>/strain_mapping.csv
#strain_mappings_file = ""

# MGF filename. This path is passed to glob.glob, default is <root>/spectra/*.mgf
#mgf_file = ""

# nodes filename. This path is passed to glob.glob, default is <root>/clusterinfo_summary/*.tsv
#nodes_file = ""

# don't know what to call this yet? TODO
# "extra" spectrum metadata file, default is <root>/*_quant.csv
#extra_nodes_file = ""

# edges filename. This path is passed to glob.glob, default is <root>/networkedges_selfloop/*.selfloop
#edges_file = ""

# metadata table filename. This path is passed to glob.glob, default is <root>/metadata_table/metadata_table-*.txt
#metadata_table_file = ""

# quantification table filename. This path is passed to glob.glob, default is <root>/quantification_table/quantification_table-*.csv
#quantification_table_file = ""

# GNPS spectral annotations directory, default is <root>/DB_result
#annotations_dir = ""

# annotation configuration file, default is <root>/annotations.tsv
#annotations_config_file = ""

# Antismash data directory, default is <root>/antismash
#antismash_dir = ""

# bigscape directory, default is <root>/bigscape
# it's expected that the various class subdirectory (NRPS etc) will exist at this location.
# within each class subdirectory, there should be a Network_Annotations_<class>.tsv file plus
# a set of <class>_clustering_<params>.tsv files
#bigscape_dir = ""

# directory containing MiBIG .json files, default is <root>/mibig_json
# (if needed, download the appropriate version of the archive in JSON format
# from https://mibig.secondarymetabolites.org/download and extract the contents)
#mibig_json_dir = ""

# list of strains to include when loading the dataset. this is optional but can be used
# to e.g. filter out most of a large dataset to focus on some specific objects
# default is <root>/include_strains.csv
#include_strains_file = ""
