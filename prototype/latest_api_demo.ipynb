{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nplinker.nplinker import NPLinker\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuring NPLinker in a notebook env is now done either by passing in the name of a config file,\n",
    "# or by passing in a dict which corresponds to the structure of the config file. Usually it will be\n",
    "# easier to edit the file and simply pass the filename like this:\n",
    "npl = NPLinker('latest_api_demo.toml')\n",
    "\n",
    "# the above step will attempt to discover the files to be loaded from the dataset and complain\n",
    "# if they're not as expected. Next, actually load the data files\n",
    "if not npl.load_data():\n",
    "    raise Exception('Failed to load data')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scoring methods are defined and configured in the default configuration file at \n",
    "# ~/.config/nplinker/nplinker.toml, but will be overridden by the config file you loaded above,\n",
    "# and the scoring methods can be easily changed once the NPLinker object has been created, e.g.:\n",
    "\n",
    "# ensure only metcalf scoring is enabled, and set a 99% significance percentile threshold\n",
    "print('Currently enabled scoring methods: {}'.format(npl.scoring.enabled()))\n",
    "npl.scoring.likescore.enabled = False\n",
    "# npl.scoring.likescore.cutoff = <scoring cutoff threshold>\n",
    "npl.scoring.hg.enabled = False\n",
    "# npl.scoring.hg.prob = <probability threshold>\n",
    "npl.scoring.metcalf.enabled = True\n",
    "npl.scoring.metcalf.sig_percentile = 99\n",
    "print('Currently enabled scoring methods: {}'.format(npl.scoring.enabled()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check if a spectrum has any of these can use .is_library, which is true \n",
    "# if it has GNPS annotation data\n",
    "spectra_with_gnps_matches = [s for s in npl.spectra if s.is_library]\n",
    "print('found {} spectra'.format(len(spectra_with_gnps_matches)))\n",
    "\n",
    "from nplinker.annotations import GNPS_KEY \n",
    "for spec in spectra_with_gnps_matches:\n",
    "    # for GNPS annotations, this will be a list containing a single dict, which \n",
    "    # is keyed by column name. for other annotation sources where a spectrum ID may\n",
    "    # appear on multiple rows, there will be one list entry per line, each containing\n",
    "    # a similar dict keyed by column name\n",
    "    annotation_data = spec.annotations[GNPS_KEY][0]\n",
    "    # shortcut for the above\n",
    "    annotation_data = spec.gnps_annotations\n",
    "    print(spec)\n",
    "    for k, v in annotation_data.items():\n",
    "        print(' -- {} = {}'.format(k, v))\n",
    "    # check for carnegie_rosetta_hits.tsv annotations\n",
    "    crh = 'carnegie_rosetta_hits.tsv'\n",
    "    if crh in spec.annotations:\n",
    "        print('Spectrum has {} rosetta hits'.format(len(spec.annotations[crh])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step generates scores for all objects and enabled scoring methods, so it can be\n",
    "# quite lengthy. The random_count parameter determines the number of randomised instances\n",
    "# of Spectrum <=> Strain mappings that will be generated during the process.\n",
    "if not npl.process_dataset(random_count=10):\n",
    "    raise Exception('Failed to process dataset')\n",
    "print('Completed generating scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get results once the scores are generated, first select an object you're interested \n",
    "# in, then call get_links with a specific scoring method. You can also pass a list of \n",
    "# objects as the first parameter. The method returns a list which contains only those\n",
    "# objects that satisfy the scoring criteria (so here only those with a significance \n",
    "# percentile score of >= 99 as set above)\n",
    "test_gcf = npl.gcfs[8]\n",
    "results = npl.get_links(test_gcf, npl.scoring.metcalf)\n",
    "if test_gcf not in results:\n",
    "    print('No results found!')\n",
    "else:\n",
    "    print('Found results for {}!'.format(test_gcf))\n",
    "    # to get the objects that scored highly against this GCF, use links_for_obj. By\n",
    "    # default it will return all objects, the type_ parameter can be used to filter\n",
    "    # by class, so here it will only return spectra\n",
    "    test_gcf_links = npl.links_for_obj(test_gcf, npl.scoring.metcalf, type_=Spectrum)\n",
    "    \n",
    "    # print the objects and their scores, plus common strains\n",
    "    for obj, score in test_gcf_links:\n",
    "        print('{} : score {}'.format(obj, score))\n",
    "        # returns a dict indexed by (Spectrum, GCF) tuples, with \n",
    "        # the values being lists of strain names shared between the two\n",
    "        common_strains = npl.get_common_strains(test_gcf, obj)\n",
    "        if len(common_strains) > 0:\n",
    "            strain_names = list(common_strains.values())[0]\n",
    "            print('   {} shared strains: {}'.format(len(strain_names), strain_names))\n",
    "        else:\n",
    "            print('   (no shared strains)')\n",
    "            \n",
    "    print('{} total links found'.format(len(test_gcf_links)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Rosetta-stone linking (nplinker version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nplinker.scoring.rosetta import rosetta\n",
    "ro = rosetta.Rosetta(npl.data_dir, npl.root_dir, npl.dataset_id)\n",
    "rhits = ro.run(npl.spectra, npl.bgcs)\n",
    "print('Rosetta hits: {}'.format(len(rhits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broken atm\n",
    "ro.generate_bgc_summary_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "spec_hits = ro._spec_hits\n",
    "# Write this out as a .tsv file to test the DB loading\n",
    "with open('carnegie_rosetta_hits.tsv','w') as f:\n",
    "    writer = csv.writer(f,delimiter='\\t')\n",
    "    heads = ['#Scan#','GNPS_ID','Score']\n",
    "    writer.writerow(heads)\n",
    "    for spec,hits in spec_hits.items():\n",
    "        for hit in hits:\n",
    "            writer.writerow([spec.spectrum_id, hit[0], hit[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in rosetta_hits:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "\n",
    "- At the moment we get lots of hits per GNPS,MiBIG pair because they are in lots of BGCs\n",
    "- We also should percolate the scores (both of the spectral match and the knownclusterblast) to the output\n",
    "- Parameterise (at least) two parameters in the spectral matching: score threshold and ms1_tol. At the moment, MS1_tol will only find things with near identical MS1 m/z, which precludes analogues.\n",
    "- The code for getting the knownclusterblast name and parsing the knownclusterblast file is horrific... :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
