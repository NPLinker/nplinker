{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, csv, os\n",
    "# if running from clone of the git repo\n",
    "# sys.path.append('../src')\n",
    "\n",
    "# import the main NPLinker class. normally this all that's required to work\n",
    "# with NPLinker in a notebook environment\n",
    "from nplinker.nplinker import NPLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:25:47 [INFO] config.py:117, Selected platform project ID MSV000079284\n",
      "11:25:47 [INFO] downloader.py:193, Downloader for MSV000079284, caching to /Users/clgeng/nplinker_data/pairedomics\n",
      "11:25:47 [INFO] downloader.py:199, Using existing copy of platform project data\n",
      "11:25:47 [DEBUG] downloader.py:215, platform_id MSV000079284 matched to pairedomics_id 4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4\n",
      "11:25:47 [INFO] downloader.py:222, Found project, retrieving JSON data...\n",
      "11:25:48 [DEBUG] downloader.py:727, Downloaded https://pairedomicsdata.bioinformatics.nl/api/projects/4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4 to /Users/clgeng/nplinker_data/pairedomics/MSV000079284.json\n",
      "11:25:48 [DEBUG] loader.py:163, DatasetLoader(platform:MSV000079284, MSV000079284, True)\n",
      "11:25:48 [DEBUG] nplinker.py:137, Enabled scoring method: metcalf\n",
      "11:25:48 [DEBUG] nplinker.py:137, Enabled scoring method: testscore\n",
      "11:25:48 [DEBUG] nplinker.py:137, Enabled scoring method: rosetta\n",
      "11:25:48 [DEBUG] nplinker.py:137, Enabled scoring method: npclassscore\n",
      "11:25:48 [DEBUG] nplinker.py:258, load_data (normal case, full load, met_only=False)\n",
      "11:25:48 [DEBUG] loader.py:173, remote loading mode, configuring root=/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284\n",
      "11:25:48 [INFO] downloader.py:247, Going to download the metabolomics data file\n",
      "11:25:48 [INFO] downloader.py:665, Found existing metabolomics_zip at /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/metabolomics_data.zip\n",
      "11:25:48 [INFO] downloader.py:686, Downloaded metabolomics data!\n",
      "11:25:48 [INFO] downloader.py:691, Extracting files to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284\n",
      "11:25:48 [INFO] downloader.py:716, Found OLD GNPS structure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 1/28, current genome ID=GCF_000515175.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000515175.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 2/28, current genome ID=GCF_000514635.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514635.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514635.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514635.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 3/28, current genome ID=GCF_000702345.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000702345.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 4/28, current genome ID=GCF_000375025.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000375025.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 5/28, current genome ID=GCF_000514975.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514975.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514975.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514975.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 6/28, current genome ID=GCF_000018265.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000018265.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000018265.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000018265.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 7/28, current genome ID=GCF_000514895.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514895.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514895.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514895.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 8/28, current genome ID=GCF_000373825.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000373825.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000373825.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000373825.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 9/28, current genome ID=GCF_000514875.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514875.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514875.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514875.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 10/28, current genome ID=GCF_000514755.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000514755.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 11/28, current genome ID=GCF_000514775.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514775.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514775.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514775.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 12/28, current genome ID=GCF_000374665.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000374665.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000374665.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000374665.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 13/28, current genome ID=GCF_000374685.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000374685.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000374685.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000374685.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 14/28, current genome ID=GCF_000424905.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000424905.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000424905.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000424905.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 15/28, current genome ID=GCF_000374725.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000374725.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 16/28, current genome ID=GCF_000514575.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000514575.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 17/28, current genome ID=GCF_000514515.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514515.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514515.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514515.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 18/28, current genome ID=GCF_000514455.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000514455.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 19/28, current genome ID=GCF_000514675.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000514675.1 skipped due to previous failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 20/28, current genome ID=GCF_000514435.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000514435.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 21/28, current genome ID=GCF_000375285.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000375285.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 22/28, current genome ID=GCF_000514855.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514855.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514855.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514855.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 23/28, current genome ID=GCF_000484695.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000484695.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 24/28, current genome ID=GCF_000377025.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000377025.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 25/28, current genome ID=GCF_000016425.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000016425.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000016425.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000016425.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 26/28, current genome ID=GCF_000515075.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000515075.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 27/28, current genome ID=GCF_000514695.1\n",
      "11:25:48 [INFO] downloader.py:434, Genome ID GCF_000514695.1 already downloaded to /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/GCF_000514695.1.zip\n",
      "11:25:48 [DEBUG] downloader.py:579, Extracting antismash data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash/GCF_000514695.1, exists_already = True\n",
      "11:25:48 [INFO] downloader.py:430, Checking for antismash data 28/28, current genome ID=GCF_000620325.1\n",
      "11:25:48 [INFO] downloader.py:438, Genome ID GCF_000620325.1 skipped due to previous failure\n",
      "11:25:48 [INFO] downloader.py:470, Dataset has 14 missing sets of antiSMASH data (from a total of 28)\n",
      "11:25:48 [INFO] downloader.py:651, Extracted 28 strains from JSON (met=48, gen=1)\n",
      "11:25:48 [INFO] downloader.py:170, Strain mappings already generated\n",
      "11:25:48 [DEBUG] downloader.py:94, Checking for existing MiBIG archive at /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/mibig_json_1.4.tar.gz\n",
      "11:25:48 [INFO] downloader.py:97, Found cached file at /Users/clgeng/nplinker_data/pairedomics/downloads/MSV000079284/mibig_json_1.4.tar.gz\n",
      "11:25:48 [DEBUG] downloader.py:119, Extracting MiBIG JSON data\n",
      "11:25:48 [INFO] downloader.py:268, Running BiG-SCAPE! extra_bigscape_parameters=\"--mibig --clans-off\"\n",
      "11:25:48 [INFO] runbigscape.py:27, run_bigscape: input=\"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/antismash\", output=\"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/bigscape\", extra_params=--mibig --clans-off\"\n",
      "11:25:48 [WARNING] downloader.py:272, Failed to run BiG-SCAPE on antismash data, error was \"Failed to find/run bigscape.py (path=/app/BiG-SCAPE/bigscape.py, err=[Errno 2] No such file or directory: '/app/BiG-SCAPE/bigscape.py')\"\n",
      "11:25:48 [WARNING] loader.py:50, WARNING: unable to find extra_nodes_file in path \"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/quantification_table_reformatted/*.csv\"\n",
      "11:25:48 [WARNING] loader.py:50, WARNING: unable to find metadata_table_file in path \"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/metadata_table/metadata_table*.txt\"\n",
      "11:25:48 [WARNING] loader.py:50, WARNING: unable to find quantification_table_file in path \"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/quantification_table/quantification_table*.csv\"\n",
      "11:25:48 [INFO] loader.py:84, Trying to discover correct bigscape directory under /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/bigscape\n",
      "11:25:48 [INFO] loader.py:647, Loaded global strain IDs (0 total)\n",
      "11:25:48 [INFO] loader.py:658, Loaded dataset strain IDs (27 total)\n",
      "11:25:53 [INFO] metabolomics.py:699, 25935 molecules parsed from MGF file\n",
      "11:25:55 [DEBUG] metabolomics.py:338, loading edges file: /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/networkedges_selfloop/6da5be36f5b14e878860167fa07004d6.pairsinfo [25935 spectra from MGF]\n",
      "11:25:55 [DEBUG] metabolomics.py:709, Nodes_file: /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/clusterinfosummarygroup_attributes_withIDs_withcomponentID/d69356c8e5044c2a9fef3dd2a2f991e1.tsv, quant_table_exists?: True\n",
      "11:25:55 [INFO] metabolomics.py:716, Found older-style GNPS dataset, no quantification table\n",
      "11:25:55 [WARNING] metabolomics.py:516, Unknown strain:  for cluster index 11542\n",
      "11:25:55 [WARNING] metabolomics.py:516, Unknown strain: 6b.mzXML for cluster index 5988\n",
      "11:25:55 [WARNING] metabolomics.py:516, Unknown strain: 6a.mzXML for cluster index 20258\n",
      "11:25:56 [WARNING] metabolomics.py:531, 3 unknown strains were detected a total of 27241 times\n",
      "11:25:56 [DEBUG] metabolomics.py:757, make_families: 29 molams + 25740 singletons\n",
      "11:25:56 [WARNING] loader.py:577, Writing unknown strains from METABOLOMICS data to /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/unknown_strains_met.csv\n",
      "11:25:56 [INFO] loader.py:584, Loading provided annotation files (/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/result_specnets_DB)\n",
      "11:25:56 [DEBUG] annotations.py:72, Parsed 0 annotations configuration entries\n",
      "11:25:56 [DEBUG] annotations.py:80, Found 1 annotations .tsv files in /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/result_specnets_DB\n",
      "11:25:56 [DEBUG] annotations.py:89, Parsing GNPS annotations from /Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/result_specnets_DB/885e4c5485ba42569e4876d1fe90d759.tsv\n",
      "11:25:56 [DEBUG] loader.py:434, Collecting .gbk files (and possibly renaming)\n",
      "11:25:56 [DEBUG] loader.py:443, Checking for spaces in antiSMASH folder names...\n",
      "11:25:56 [DEBUG] loader.py:466, .gbk collection took 0.009s\n",
      "11:25:56 [DEBUG] loader.py:477, make_mibig_bgc_dict(/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/mibig_json)\n",
      "11:25:56 [INFO] genomics.py:485, Found 1816 MiBIG json files\n",
      "11:25:56 [DEBUG] loader.py:479, mibig_bgc_dict has 1816 entries\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to find *any* BiGSCAPE Network_Annotations tsv files under \"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/bigscape\" (incorrect cutoff value? currently set to 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m npl \u001b[38;5;241m=\u001b[39m NPLinker(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnplinker_demo1.toml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# loading the actual data files can take some time depending on the dataset,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# so this is done separately by calling the load_data method.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# correctly. You can control the verbosity of these messages in the configuration\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# file if required, and/or redirect them to a file instead of stdout. \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mnpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mgene/nplinker/src/nplinker/nplinker.py:261\u001b[0m, in \u001b[0;36mNPLinker.load_data\u001b[0;34m(self, new_bigscape_cutoff, met_only)\u001b[0m\n\u001b[1;32m    258\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_data (normal case, full load, met_only=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(met_only))\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loader\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmet_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmet_only\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mgene/nplinker/src/nplinker/loader.py:263\u001b[0m, in \u001b[0;36mDatasetLoader.load\u001b[0;34m(self, met_only)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_metabolomics():\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m met_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_genomics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_class_info():\n",
      "File \u001b[0;32m~/mgene/nplinker/src/nplinker/loader.py:511\u001b[0m, in \u001b[0;36mDatasetLoader._load_genomics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# no files found here indicates a problem!\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(anno_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find *any* BiGSCAPE Network_Annotations tsv files under \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (incorrect cutoff value? currently set to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigscape_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bigscape_cutoff))\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_list_missing_files\u001b[39m(m, l):\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(l) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mException\u001b[0m: Failed to find *any* BiGSCAPE Network_Annotations tsv files under \"/Users/clgeng/nplinker_data/pairedomics/extracted/MSV000079284/bigscape\" (incorrect cutoff value? currently set to 30)"
     ]
    }
   ],
   "source": [
    "# the standard method of loading a dataset configuration is to pass the filename\n",
    "# of a TOML configuration file to the NPLinker constructor. \n",
    "npl = NPLinker('nplinker_demo1.toml')\n",
    "# loading the actual data files can take some time depending on the dataset,\n",
    "# so this is done separately by calling the load_data method.\n",
    "#\n",
    "# During the loading process, logging messages will be printed to stdout. This\n",
    "# can be useful for debugging problems with files not being discovered or parsed\n",
    "# correctly. You can control the verbosity of these messages in the configuration\n",
    "# file if required, and/or redirect them to a file instead of stdout. \n",
    "npl.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic functionality\n",
    "# ===================\n",
    "#\n",
    "# Once you have an NPLinker object with all data loaded, there are a collection of simple\n",
    "# methods and properties you can use to access objects and metadata. Some examples are \n",
    "# given below, see https://nplinker.readthedocs.io/en/latest/ for a complete API description.\n",
    "\n",
    "# configuration/dataset metadata\n",
    "# - a copy of the configuration as parsed from the .toml file (dict)\n",
    "print(npl.config) \n",
    "# - the path to the directory where various nplinker data files are located (e.g. the \n",
    "#   default configuration file template) (str)\n",
    "print(npl.data_dir)\n",
    "# - a dataset ID, derived from the path for local datasets or the paired platform ID\n",
    "#   for datasets loaded from that source (str)\n",
    "print(npl.dataset_id)\n",
    "# - the root directory for the current dataset (str)\n",
    "print(npl.root_dir)\n",
    "\n",
    "# objects\n",
    "# - you can directly access lists of each of the 4 object types:\n",
    "print('BGCs:', len(npl.bgcs))\n",
    "print('GCFs:', len(npl.gcfs)) # contains GCF objects\n",
    "print('Spectra:', len(npl.spectra)) # contains Spectrum objects\n",
    "print('Molecular Families:', len(npl.molfams)) # contains MolecularFamily objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functionality - part 1\n",
    "# ==============================\n",
    "# (again see https://nplinker.readthedocs.io/en/latest/ for API documentation)\n",
    "\n",
    "# NPLinker provides a set of scoring methods that can be used individually or \n",
    "# in combination to find interesting links in the current dataset. To get a\n",
    "# get a list of the names of the available scoring methods:\n",
    "print('Available scoring methods:')\n",
    "for m in npl.scoring_methods:\n",
    "    print(' - {}'.format(m))\n",
    "    \n",
    "# The first step in running a scoring operation is to get an instance of the\n",
    "# method(s) you want to use by calling scoring_method():\n",
    "mc = npl.scoring_method('metcalf')\n",
    "\n",
    "# Now mc is an instance of the class that implements Metcalf scoring. Once\n",
    "# you have such an instance, you may change any of the parameters it exposes.\n",
    "# In the case of Metcalf scoring, the following parameters are currently exposed:\n",
    "# - cutoff (float): the scoring threshold. Links with scores less than this are excluded\n",
    "# - standardised (bool): set to True to use standardised scores (default), False for regular\n",
    "mc.cutoff = 3.5\n",
    "mc.standardised = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functionality - part 2\n",
    "# ==============================\n",
    "\n",
    "# After creating and optionally configuring a scoring method, you need to call\n",
    "# get_links() to perform the operation on a selected set of objects. This method\n",
    "# takes 2-3 parameters, the third being optional:\n",
    "#  - a list of objects to find links from (or a list of lists of objects)\n",
    "#  - a list of scoring methods, or a single method as shorthand for a 1-element list\n",
    "#  - (optional) a boolean indicating if results from multiple methods should be \n",
    "#     ANDed together to produce the final results. If set to False, the results will\n",
    "#     contain links found by any method rather than all methods. \n",
    "# \n",
    "# This first example shows the simplest case: 1 set of objects and 1 scoring method. \n",
    "# If the and_mode parameter is not given it defaults to True, but the value doesn't \n",
    "# matter here because only one method is being used. \n",
    "results = npl.get_links(npl.gcfs[:10], mc, and_mode=True) \n",
    "\n",
    "# get_links returns an instance of a class called LinkCollection. This provides a wrapper\n",
    "# around the results of the scoring operation and has various useful properties/methods:\n",
    "#\n",
    "# - len(results) or .source_count will tell you how many of the input_objects were found to have links\n",
    "print('Number of results: {}'.format(len(results)))\n",
    "# - .sources is a list of those objects\n",
    "objects_with_links = results.sources\n",
    "# - .links is a dict with structure {input_object: {linked_object: ObjectLink}} \n",
    "objects_and_link_info = results.links\n",
    "# - .get_all_targets() will return a flat list of *all* the linked objects (for all sources)\n",
    "all_targets = results.get_all_targets() \n",
    "# - .methods is a list of the scoring methods passed to get_links\n",
    "methods = results.methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functionality - part 3\n",
    "# ==============================\n",
    "# \n",
    "# The link data inside the LinkCollection object is itself stored in ObjectLink objects.\n",
    "# Each instance of an ObjectLink represents a link between a given pair of objects as\n",
    "# determined by 1 or more scoring methods. \n",
    "#\n",
    "# ObjectLinks have the following basic attributes:\n",
    "# - .source: the input object provided to the method\n",
    "# - .target: the linked object\n",
    "# - .methods: a list of the methods that found this link\n",
    "# - .shared_strains: a list of Strain objects (possibly empty) shared between .source and .target\n",
    "# - .data(<method_object>): return the output of <method_object> for this link (e.g. any score values)\n",
    "# \n",
    "# You can also retrieve any method-specific info for a link by subscripting these objects with \n",
    "# the appropriate method object, e.g. metcalf_link_data = object_link[mc] \n",
    "\n",
    "# This shows how to iterate over the link information from result.links. In the body of the loop\n",
    "# <obj> will be one of  the original objects supplied to get_links and <result> will be a dict \n",
    "# with structure {linked_object: ObjectLink} (indicating <obj> is linked to <linked_object> according to\n",
    "# the information stored in the ObjectLink)\n",
    "for obj, result in results.links.items():\n",
    "    # display the object, the number of links it has, and the number of methods that were used to get them\n",
    "    print('Results for object: {}, {} total links, {} methods used'.format(obj, len(result), results.method_count))\n",
    "    \n",
    "    # sorting is method-dependent since they might have very different \"scores\", so you should\n",
    "    # use the original object to do this. For Metcalf scoring, this will return the ObjectLinks sorted\n",
    "    # by their Metcalf scores. \n",
    "    sorted_links = results.get_sorted_links(mc, obj)\n",
    "    # or if you wanted them in the reverse order:\n",
    "    # sorted_links = results.get_sorted_links(mc, obj, reverse=True)\n",
    "    \n",
    "    # Now display some link information for each link associated with <obj>.\n",
    "    # link_data[<method_object>] will return the per-link data generated by that \n",
    "    # method. Here the metcalf method simply returns the link score as a floating point value,\n",
    "    # but other methods may return more complex objects. \n",
    "    # \n",
    "    # Each scoring method also has a format_data method which should provide a relatively short \n",
    "    # human-readable summary of the data, as a quick way to print and examine results. \n",
    "    for link_data in sorted_links:\n",
    "        print('  --> [{}] {} | {} | shared strains = {}'.format(','.join(method.name for method in link_data.methods), \n",
    "                                                                link_data.target, \n",
    "                                                                mc.format_data(link_data[mc]), \n",
    "                                                                len(link_data.shared_strains)))\n",
    "        \n",
    "    # alternatively, if you don't care about ordering, you can just iterate directly over the \n",
    "    # linked objects like this:\n",
    "    # for link_target, link_data in result.items():\n",
    "    #    print(link_target, link_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functionality - part 4\n",
    "# ==============================\n",
    "#\n",
    "# The LinkCollection object supports performing various types of filtering on the original set of\n",
    "# results contained within it:\n",
    "# - .filter_no_shared_strains(): remove any links where the linked objects do not share strains\n",
    "# - .filter_sources(callable), .filter_targets(callable), .filter_links(callable): each of these\n",
    "#     simply execute callable(object) and filter out objects for which the return value is False/0. \n",
    "#     The <objects> in each case are respectively: the original input objects (sources), \n",
    "#     their linked objects (targets), and the ObjectLink objects (links).\n",
    "#\n",
    "# NOTE:\n",
    "# - these methods all modify the original LinkCollection in-place\n",
    "# - they will automatically remove any original results for which no links exist after filtering. For\n",
    "#    example, if there is a source object which starts off with 2 links, but has 0 after a filter is\n",
    "#    run, this object will not appear in the LinkCollection afterwards.\n",
    "#\n",
    "# Examples:\n",
    "# - exclude any sources for which an arbitrary function is false (sources are GCFs in this example)\n",
    "results.filter_sources(lambda gcf: gcf.id % 2 == 0)\n",
    "# - exclude any linked objects for which an arbitrary function is false (targets are Spectrum objects here)\n",
    "results.filter_targets(lambda spec: spec.id % 1 == 0)\n",
    "# - exclude any links for which an arbitrary function is false (<link> is an ObjectLink)\n",
    "results.filter_links(lambda link: link[mc] > 3.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring functionality - part 5\n",
    "# ==============================\n",
    "# \n",
    "# The get_links method can be passed more complex parameters types than the above example which \n",
    "# used a flat list of input objects and a single scoring method instance. \n",
    "\n",
    "ts = npl.scoring_method('testscore') # copy of Metcalf method, only for debug use\n",
    "\n",
    "# You can use the same set of objects with two different methods, and AND the results\n",
    "# together so that objects will only be returned which have links according to \n",
    "# BOTH of the supplied methods (if you provide 2 or more scoring methods but only a single \n",
    "# set of objects, that set will be used as input to every method).\n",
    "results = npl.get_links(npl.gcfs[:10], [mc, ts], and_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
