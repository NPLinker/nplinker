{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NPLinker","text":"<p>NPLinker is a python framework for data mining microbial natural products by integrating genomics and metabolomics data.</p> <p>For a deep understanding of NPLinker, please refer to the original paper.</p> <p>Under Development</p> <p>NPLinker v2 is under active development. The documentation is not complete yet. If you have any  questions, please contact us via GitHub Issues</p>"},{"location":"install/","title":"Installation","text":"Requirements <ul> <li>Linux, MacOS, or WSL on Windows<ul> <li>For Windows without WSL enabled, please use NPLinker docker image</li> </ul> </li> <li>Python version \u22653.9</li> </ul> <p>NPLinker is a python package that has both pypi packages and non-pypi packages as dependencies. Install <code>nplinker</code> package as following:</p> Install nplinker package<pre><code># Check python version (\u22653.9)\npython --version\n\n# Create a new virtual environment\npython -m venv env          # (1)!\nsource env/bin/activate\n\n# install nplinker package\npip install nplinker\n\n# install nplinker non-pypi dependencies and databases\ninstall-nplinker-deps\n</code></pre> <ol> <li>A virtual environment is required to install the the non-pypi dependencies. You can also use <code>conda</code> to create a new environment. But NPLinker is not available on conda yet.</li> </ol>"},{"location":"install/#install-from-source-code","title":"Install from source code","text":"<p>You can also install NPLinker from source code:</p> Install from latest source code<pre><code>pip install git+https://github.com/nplinker/nplinker@dev  # (1)!\ninstall-nplinker-deps\n</code></pre> <ol> <li>The <code>@dev</code> is the branch name. You can replace it with the branch name, commit or tag.</li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>NPLinker allows you to run in two modes:</p> <code>local</code> mode<code>podp</code> mode <p>The <code>local</code> mode assumes that the data required by NPLinker is available on your local machine.</p> <p>The required input data includes:</p> <ul> <li>GNPS molecular networking data from one of the following GNPS workflows<ul> <li><code>METABOLOMICS-SNETS</code>,</li> <li><code>METABOLOMICS-SNETS-V2</code></li> <li><code>FEATURE-BASED-MOLECULAR-NETWORKING</code></li> </ul> </li> <li>AntiSMASH BGC data</li> <li>BigScape data (optional)</li> </ul> <p>The <code>podp</code> mode assumes that you use an identifier of Paired Omics Data Platform (PODP) as the input for NPLinker. Then NPLinker will download and prepare all data necessary based on the PODP id which refers to the metadata of the dataset.</p> <p>So, which mode will you use? The answer is important for the next steps.</p>"},{"location":"quickstart/#1-create-a-working-directory","title":"1. Create a working directory","text":"<p>The working directory is used to store all input and output data for NPLinker. You can name this directory as you like, for example <code>nplinker_quickstart</code>:</p> Create a working directory<pre><code>mkdir nplinker_quickstart\n</code></pre> <p>Important</p> <p>Before going to the next step, make sure you get familiar with how NPLinker organizes data in the working directory, see Working Directory Structure page.</p>"},{"location":"quickstart/#2-prepare-input-data-local-mode-only","title":"2. Prepare input data (<code>local</code> mode only)","text":"Details <p>Skip this step if you choose to use the <code>podp</code> mode.</p> <p>If you choose to use the <code>local</code> mode, meaning you have input data of NPLinker stored on your local machine, you need to move the input data to the working directory created in the previous step.</p>"},{"location":"quickstart/#gnps-data","title":"GNPS data","text":"<p>NPLinker accepts data from the output of the following GNPS workflows:</p> <ul> <li><code>METABOLOMICS-SNETS</code></li> <li><code>METABOLOMICS-SNETS-V2</code></li> <li><code>FEATURE-BASED-MOLECULAR-NETWORKING</code>.</li> </ul> <p>NPLinker provides the tools <code>GNPSDownloader</code> and <code>GNPSExtractor</code> to download and extract the GNPS data with ease. What you need to give is a valid GNPS task ID, referring to a task of the GNPS workflows supported by NPLinker.</p> GNPS task id and workflow <p>Given an example of GNPS task at https://gnps.ucsd.edu/ProteoSAFe/status.jsp?task=c22f44b14a3d450eb836d607cb9521bb, the task id is the last part of this url, i.e. <code>c22f44b14a3d450eb836d607cb9521bb</code>. Open this link, you can find the worklow info at the row \"Workflow\" of the table \"Job Status\", for this case, it is <code>METABOLOMICS-SNETS</code>.</p> Download &amp; Extract GNPS data<pre><code>from nplinker.metabolomics.gnps import GNPSDownloader, GNPSExtractor\n\n# Go to the working directory\ncd nplinker_quickstart\n\n# Download GNPS data &amp; get the path to the downloaded archive\ndownloader = GNPSDownloader(\"gnps_task_id\", \"downloads\") # (1)!\ndownloaded_archive = downloader.download().get_download_file()\n\n# Extract GNPS data to `gnps` directory\nextractor = GNPSExtractor(downloaded_archive, \"gnps\") # (2)!\n</code></pre> <ol> <li>If you already have the downloaded archive of GNPS data, you can skip the download steps.</li> <li>Replace <code>downloaded_archive</code> with the actuall path to your GNPS data archive if you skipped the download steps.</li> </ol> <p>The required data for NPLinker will be extracted to the <code>gnps</code> subdirectory of the working directory.</p> <p>Info</p> <p>Not all GNPS data are required by NPLinker, and only the necessary data will be extracted. During the extraction, these data will be renamed to the standard names used by NPLinker. See the page GNPS Data for more information.</p> Prepare GNPS data manually <p>If you have GNPS data but it is not the archive format as downloaded from GNPS, it's recommended to re-download the data from GNPS.</p> <p>If (re-)downloading is not possible, you could manually prepare data for the <code>gnps</code> directory. In this case, you must make sure that the data is organized as expected by NPLinker. See the page GNPS Data for examples of how to prepare the data.</p>"},{"location":"quickstart/#antismash-data","title":"AntiSMASH data","text":"<p>NPLinker requires AntiSMASH BGC data as input, which are organized in the <code>antismash</code> subdirectory of  the working directory.</p> <p>For each output of AntiSMASH run, the BGC data must be stored in a subdirectory named after the NCBI accession number (e.g. <code>GCF_000514975.1</code>). And only the <code>*.region*.gbk</code> files are required by NPLinker.</p> <p>When manually preparing AntiSMASH data for NPLinker, you must make sure that the data is organized as expected by NPLinker. See the page Working Directory Structure for more information.</p>"},{"location":"quickstart/#bigscape-data-optional","title":"BigScape data (optional)","text":"<p>It is optional to provide the output of BigScape to NPLinker. If the output of BigScape is not provided, NPLinker will run BigScape automatically to generate the data using the AntiSMASH BGC data.</p> <p>If you have the output of BigScape, you can put its <code>mix_clustering_c{cutoff}.tsv</code> file in the <code>bigscape</code> subdirectory of the NPLinker working directory, where <code>{cutoff}</code> is the cutoff value used in the BigScape run.</p>"},{"location":"quickstart/#strain-mappings-file","title":"Strain mappings file","text":"<p>The strain mappings file <code>strain_mapping.json</code> is required by NPLinker to map the strain to genomics and metabolomics data. </p> `strain_mappings.json` example<pre><code>{\n    \"strain_mappings\": [\n        {\n            \"strain_id\": \"strain_id_1\", # (1)!\n            \"strain_alias\": [\"bgc_id_1\", \"spectrum_id_1\", ...] # (2)!\n        },\n        {\n            \"strain_id\": \"strain_id_2\",\n            \"strain_alias\": [\"bgc_id_2\", \"spectrum_id_2\", ...]\n        },\n        ...\n    ],\n    \"version\": \"1.0\" # (3)!\n}\n</code></pre> <ol> <li><code>strain_id</code> is the unique identifier of the strain.</li> <li><code>strain_alias</code> is a list of aliases of the strain, which are the identifiers of the BGCs and spectra of the strain.</li> <li><code>version</code> is the schema version of this file. It is recommended to use the latest version of the schema. The current latest version is <code>1.0</code>. </li> </ol> <p>The BGC id is same as the name of the BGC file in the <code>antismash</code> directory, for example, given a  BGC file <code>xxxx.region001.gbk</code>, the BGC id is <code>xxxx.region001</code>.</p> <p>The spectrum id is same as the scan number in the <code>spectra.mgf</code> file in the <code>gnps</code> directory,  for example, given a spectrum in the mgf file with a scan <code>SCANS=1</code>, the spectrum id is <code>1</code>. </p> <p>If you labelled the mzXML files (input for GNPS) with the strain id, you may need the function  extract_mappings_ms_filename_spectrum_id  to extract the mappings from mzXML files to the spectrum ids.</p> <p>For the <code>local</code> mode, you need to create this file manually and put it in the working directory. It takes some effort to prepare this file manually, especially when you have a large number of strains.</p>"},{"location":"quickstart/#3-prepare-config-file","title":"3. Prepare config file","text":"<p>The configuration file <code>nplinker.toml</code> is required by NPLinker to specify the working directory, mode, and other settings for the run of NPLinker. </p> <p>Once prepared, the <code>nplinker.toml</code> file must be put in the working directory created in step 2.</p> <p>The details of all settings can be found at this page Config File.</p> <p>To keep it simple, default settings will be used  automatically by NPLinker if you don't set them in your <code>nplinker.toml</code> config file.</p> <p>What you need to do is to set the <code>root_dir</code> and <code>mode</code> in the <code>nplinker.toml</code> file.</p> <code>local</code> mode<code>podp</code> mode nplinker.toml<pre><code>root_dir = \"absolute/path/to/working/directory\" # (1)!\nmode = \"local\"\n# and other settings you want to override the default settings \n</code></pre> <ol> <li>Replace <code>absolute/path/to/working/directory</code> with the absolute path to the working directory    created in step 2.</li> </ol> nplinker.toml<pre><code>root_dir = \"absolute/path/to/working/directory\" # (1)!\nmode = \"podp\"\npodp_id = \"podp_id\" # (2)!\n# and other settings you want to override the default settings \n</code></pre> <ol> <li>Replace <code>absolute/path/to/working/directory</code> with the absolute path to the working directory    created in step 2.</li> <li>Replace <code>podp_id</code> with the identifier of the dataset in the Paired Omics Data Platform (PODP).</li> </ol>"},{"location":"quickstart/#4-run-nplinker","title":"4. Run NPLinker","text":"<p>Before running NPLinker, make sure your working directory containing input data and config file has the correct directory structure and names described in the Working Directory Structure page.</p> <p>You need to run NPlinker in the working directory where the <code>nplinker.toml</code> file is located, and NPLinker will automatically load the config file and run in the mode specified in the config file. If NPLinker cannot find the <code>nplinker.toml</code> file, you will get an error message.</p> Run NPLinker in your working directory<pre><code>from nplinker.nplinker import NPLinker\n\n# create an instance of NPLinker\nnpl = NPLinker()\n\n# load data\nnpl.load_data()\n\n# check loaded data\nprint(npl.bgcs)\nprint(npl.gcfs)\nprint(npl.spectra)\nprint(npl.molfams)\nprint(npl.strains)\n\n# get the links generated by metcalf scoring\nnpl.get_links(input_objects=npl.gcfs, scoring_method=\"metcalf\")\n</code></pre> <p>For more info about the classes and methods, see the API Documentation.</p>"},{"location":"api/antismash/","title":"AntiSMASH","text":""},{"location":"api/antismash/#nplinker.genomics.antismash","title":"antismash","text":""},{"location":"api/antismash/#nplinker.genomics.antismash.AntismashBGCLoader","title":"AntismashBGCLoader","text":"<pre><code>AntismashBGCLoader(data_dir: str)\n</code></pre> <p>Build a loader for AntiSMASH BGC genbank (.gbk) files.</p> Note <p>AntiSMASH BGC directory must follow the structure below: <pre><code>antismash\n    \u251c\u2500\u2500 genome_id_1 (one AntiSMASH output, e.g. GCF_000514775.1)\n    \u2502\u00a0 \u251c\u2500\u2500 GCF_000514775.1.gbk\n    \u2502\u00a0 \u251c\u2500\u2500 NZ_AZWO01000004.region001.gbk\n    \u2502\u00a0 \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 genome_id_2\n    \u2502\u00a0 \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to AntiSMASH directory that contains a collection of AntiSMASH outputs.</p> required Source code in <code>src/nplinker/genomics/antismash/antismash_loader.py</code> <pre><code>def __init__(self, data_dir: str) -&gt; None:\n    \"\"\"Build a loader for AntiSMASH BGC genbank (.gbk) files.\n\n    Note:\n        AntiSMASH BGC directory must follow the structure below:\n        ```\n        antismash\n            \u251c\u2500\u2500 genome_id_1 (one AntiSMASH output, e.g. GCF_000514775.1)\n            \u2502\u00a0 \u251c\u2500\u2500 GCF_000514775.1.gbk\n            \u2502\u00a0 \u251c\u2500\u2500 NZ_AZWO01000004.region001.gbk\n            \u2502\u00a0 \u2514\u2500\u2500 ...\n            \u251c\u2500\u2500 genome_id_2\n            \u2502\u00a0 \u251c\u2500\u2500 ...\n            \u2514\u2500\u2500 ...\n        ```\n\n    Args:\n        data_dir: Path to AntiSMASH directory that contains a\n            collection of AntiSMASH outputs.\n    \"\"\"\n    self.data_dir = data_dir\n    self._file_dict = self._parse_data_dir(self.data_dir)\n    self._bgcs = self._parse_bgcs(self._file_dict)\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.AntismashBGCLoader.get_bgc_genome_mapping","title":"get_bgc_genome_mapping","text":"<pre><code>get_bgc_genome_mapping() -&gt; dict[str, str]\n</code></pre> <p>Get the mapping from BGC to genome.</p> <p>Note that the directory name of the gbk file is treated as genome id.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The key is BGC name (gbk file name) and value is genome id (the directory name of the</p> <code>dict[str, str]</code> <p>gbk file).</p> Source code in <code>src/nplinker/genomics/antismash/antismash_loader.py</code> <pre><code>def get_bgc_genome_mapping(self) -&gt; dict[str, str]:\n    \"\"\"Get the mapping from BGC to genome.\n\n    Note that the directory name of the gbk file is treated as genome id.\n\n    Returns:\n        The key is BGC name (gbk file name) and value is genome id (the directory name of the\n        gbk file).\n    \"\"\"\n    return {\n        bid: os.path.basename(os.path.dirname(bpath)) for bid, bpath in self._file_dict.items()\n    }\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.AntismashBGCLoader.get_files","title":"get_files","text":"<pre><code>get_files() -&gt; dict[str, str]\n</code></pre> <p>Get BGC gbk files.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The key is BGC name (gbk file name) and value is path to the gbk file.</p> Source code in <code>src/nplinker/genomics/antismash/antismash_loader.py</code> <pre><code>def get_files(self) -&gt; dict[str, str]:\n    \"\"\"Get BGC gbk files.\n\n    Returns:\n        The key is BGC name (gbk file name) and value is path to the gbk file.\n    \"\"\"\n    return self._file_dict\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.AntismashBGCLoader.get_bgcs","title":"get_bgcs","text":"<pre><code>get_bgcs() -&gt; list[BGC]\n</code></pre> <p>Get all BGC objects.</p> <p>Returns:</p> Type Description <code>list[BGC]</code> <p>A list of BGC objects</p> Source code in <code>src/nplinker/genomics/antismash/antismash_loader.py</code> <pre><code>def get_bgcs(self) -&gt; list[BGC]:\n    \"\"\"Get all BGC objects.\n\n    Returns:\n        A list of BGC objects\n    \"\"\"\n    return self._bgcs\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.GenomeStatus","title":"GenomeStatus","text":"<pre><code>GenomeStatus(original_id: str, resolved_refseq_id: str = '', resolve_attempted: bool = False, bgc_path: str = '')\n</code></pre> <p>A class to represent the status of a single genome.</p> <p>The status of genomes is tracked in a JSON file which has a name defined in variable <code>GENOME_STATUS_FILENAME</code>.</p> <p>Initialize a GenomeStatus object for the given genome.</p> <p>Parameters:</p> Name Type Description Default <code>original_id</code> <code>str</code> <p>The original ID of the genome.</p> required <code>resolved_refseq_id</code> <code>str</code> <p>The resolved RefSeq ID of the genome. Defaults to \"\".</p> <code>''</code> <code>resolve_attempted</code> <code>bool</code> <p>A flag indicating whether an attempt to resolve the RefSeq ID has been made. Defaults to False.</p> <code>False</code> <code>bgc_path</code> <code>str</code> <p>The path to the downloaded BGC file for the genome. Defaults to \"\".</p> <code>''</code> Source code in <code>src/nplinker/genomics/antismash/podp_antismash_downloader.py</code> <pre><code>def __init__(\n    self,\n    original_id: str,\n    resolved_refseq_id: str = \"\",\n    resolve_attempted: bool = False,\n    bgc_path: str = \"\",\n):\n    \"\"\"Initialize a GenomeStatus object for the given genome.\n\n    Args:\n        original_id: The original ID of the genome.\n        resolved_refseq_id: The resolved RefSeq ID of the\n            genome. Defaults to \"\".\n        resolve_attempted: A flag indicating whether an\n            attempt to resolve the RefSeq ID has been made. Defaults to False.\n        bgc_path: The path to the downloaded BGC file for\n            the genome. Defaults to \"\".\n    \"\"\"\n    self.original_id = original_id\n    self.resolved_refseq_id = \"\" if resolved_refseq_id == \"None\" else resolved_refseq_id\n    self.resolve_attempted = resolve_attempted\n    self.bgc_path = bgc_path\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.GenomeStatus.read_json","title":"read_json  <code>staticmethod</code>","text":"<pre><code>read_json(file: str | PathLike) -&gt; dict[str, 'GenomeStatus']\n</code></pre> <p>Get a dict of GenomeStatus objects by loading given genome status file.</p> <p>Note that an empty dict is returned if the given file doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to genome status file.</p> required <p>Returns:</p> Type Description <code>dict[str, 'GenomeStatus']</code> <p>Dict keys are genome original id and values are GenomeStatus objects. An empty dict is returned if the given file doesn't exist.</p> Source code in <code>src/nplinker/genomics/antismash/podp_antismash_downloader.py</code> <pre><code>@staticmethod\ndef read_json(file: str | PathLike) -&gt; dict[str, \"GenomeStatus\"]:\n    \"\"\"Get a dict of GenomeStatus objects by loading given genome status file.\n\n    Note that an empty dict is returned if the given file doesn't exist.\n\n    Args:\n        file: Path to genome status file.\n\n    Returns:\n        Dict keys are genome original id and values are GenomeStatus\n            objects. An empty dict is returned if the given file doesn't exist.\n    \"\"\"\n    genome_status_dict = {}\n    if Path(file).exists():\n        with open(file, \"r\") as f:\n            data = json.load(f)\n\n        # validate json data before using it\n        validate(data, schema=GENOME_STATUS_SCHEMA)\n\n        genome_status_dict = {\n            gs[\"original_id\"]: GenomeStatus(**gs) for gs in data[\"genome_status\"]\n        }\n    return genome_status_dict\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.GenomeStatus.to_json","title":"to_json  <code>staticmethod</code>","text":"<pre><code>to_json(genome_status_dict: dict[str, 'GenomeStatus'], file: str | PathLike | None = None) -&gt; str | None\n</code></pre> <p>Convert the genome status dictionary to a JSON string.</p> <p>If a file path is provided, the JSON string is written to the file. If the file already exists, it is overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>genome_status_dict</code> <code>dict[str, 'GenomeStatus']</code> <p>A dictionary of genome status objects. The keys are the original genome IDs and the values are GenomeStatus objects.</p> required <code>file</code> <code>str | PathLike | None</code> <p>The path to the output JSON file. If None, the JSON string is returned but not written to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The JSON string if <code>file</code> is None, otherwise None.</p> Source code in <code>src/nplinker/genomics/antismash/podp_antismash_downloader.py</code> <pre><code>@staticmethod\ndef to_json(\n    genome_status_dict: dict[str, \"GenomeStatus\"], file: str | PathLike | None = None\n) -&gt; str | None:\n    \"\"\"Convert the genome status dictionary to a JSON string.\n\n    If a file path is provided, the JSON string is written to the file. If\n    the file already exists, it is overwritten.\n\n    Args:\n        genome_status_dict: A dictionary of genome\n            status objects. The keys are the original genome IDs and the values\n            are GenomeStatus objects.\n        file: The path to the output JSON file.\n            If None, the JSON string is returned but not written to a file.\n\n    Returns:\n        The JSON string if `file` is None, otherwise None.\n    \"\"\"\n    gs_list = [gs._to_dict() for gs in genome_status_dict.values()]\n    json_data = {\"genome_status\": gs_list, \"version\": \"1.0\"}\n\n    # validate json object before dumping\n    validate(json_data, schema=GENOME_STATUS_SCHEMA)\n\n    if file is not None:\n        with open(file, \"w\") as f:\n            json.dump(json_data, f)\n        return None\n    return json.dumps(json_data)\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.download_and_extract_antismash_data","title":"download_and_extract_antismash_data","text":"<pre><code>download_and_extract_antismash_data(antismash_id: str, download_root: str | PathLike, extract_root: str | PathLike) -&gt; None\n</code></pre> <p>Download and extract antiSMASH BGC archive for a specified genome.</p> <p>The antiSMASH database (https://antismash-db.secondarymetabolites.org/) is used to download the BGC archive. And antiSMASH use RefSeq assembly id of a genome as the id of the archive.</p> <p>Parameters:</p> Name Type Description Default <code>antismash_id</code> <code>str</code> <p>The id used to download BGC archive from antiSMASH database. If the id is versioned (e.g., \"GCF_004339725.1\") please be sure to specify the version as well.</p> required <code>download_root</code> <code>str | PathLike</code> <p>Path to the directory to place downloaded archive in.</p> required <code>extract_root</code> <code>str | PathLike</code> <p>Path to the directory data files will be extracted to. Note that an <code>antismash</code> directory will be created in the specified <code>extract_root</code> if it doesn't exist. The files will be extracted to <code>&lt;extract_root&gt;/antismash/&lt;antismash_id&gt;</code> directory.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if download_root and extract_root dirs are the same.</p> <code>ValueError</code> <p>if /antismash/ dir is not empty. <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_and_extract_antismash_metadata(\"GCF_004339725.1\", \"/data/download\", \"/data/extracted\")\n</code></pre> Source code in <code>src/nplinker/genomics/antismash/antismash_downloader.py</code> <pre><code>def download_and_extract_antismash_data(\n    antismash_id: str, download_root: str | PathLike, extract_root: str | PathLike\n) -&gt; None:\n    \"\"\"Download and extract antiSMASH BGC archive for a specified genome.\n\n    The antiSMASH database (https://antismash-db.secondarymetabolites.org/)\n    is used to download the BGC archive. And antiSMASH use RefSeq assembly id\n    of a genome as the id of the archive.\n\n    Args:\n        antismash_id: The id used to download BGC archive from antiSMASH database.\n            If the id is versioned (e.g., \"GCF_004339725.1\") please be sure to\n            specify the version as well.\n        download_root: Path to the directory to place downloaded archive in.\n        extract_root: Path to the directory data files will be extracted to.\n            Note that an `antismash` directory will be created in the specified `extract_root` if\n            it doesn't exist. The files will be extracted to `&lt;extract_root&gt;/antismash/&lt;antismash_id&gt;` directory.\n\n    Raises:\n        ValueError: if download_root and extract_root dirs are the same.\n        ValueError: if &lt;extract_root&gt;/antismash/&lt;refseq_assembly_id&gt; dir is not empty.\n\n    Examples:\n        &gt;&gt;&gt; download_and_extract_antismash_metadata(\"GCF_004339725.1\", \"/data/download\", \"/data/extracted\")\n    \"\"\"\n    download_root = Path(download_root)\n    extract_root = Path(extract_root)\n    extract_path = extract_root / \"antismash\" / antismash_id\n    _check_roots(download_root, extract_root)\n\n    try:\n        if extract_path.exists():\n            _check_extract_path(extract_path)\n        else:\n            extract_path.mkdir(parents=True, exist_ok=True)\n\n        for base_url in [ANTISMASH_DB_DOWNLOAD_URL, ANTISMASH_DBV2_DOWNLOAD_URL]:\n            url = base_url.format(antismash_id, antismash_id + \".zip\")\n            download_and_extract_archive(url, download_root, extract_path, antismash_id + \".zip\")\n            break\n\n        # delete subdirs\n        for subdir_path in list_dirs(extract_path):\n            shutil.rmtree(subdir_path)\n\n        # delete unnecessary files\n        files_to_keep = list_files(extract_path, suffix=(\".json\", \".gbk\"))\n        for file in list_files(extract_path):\n            if file not in files_to_keep:\n                os.remove(file)\n\n        logger.info(\"antiSMASH BGC data of %s is downloaded and extracted.\", antismash_id)\n\n    except Exception as e:\n        shutil.rmtree(extract_path)\n        logger.warning(e)\n        raise e\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.parse_bgc_genbank","title":"parse_bgc_genbank","text":"<pre><code>parse_bgc_genbank(file: str) -&gt; BGC\n</code></pre> <p>Parse a single BGC gbk file to BGC object.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to BGC gbk file</p> required <p>Returns:</p> Type Description <code>BGC</code> <p>BGC object</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgc = AntismashBGCLoader.parse_bgc(\n...    \"/data/antismash/GCF_000016425.1/NC_009380.1.region001.gbk\")\n</code></pre> Source code in <code>src/nplinker/genomics/antismash/antismash_loader.py</code> <pre><code>def parse_bgc_genbank(file: str) -&gt; BGC:\n    \"\"\"Parse a single BGC gbk file to BGC object.\n\n    Args:\n        file: Path to BGC gbk file\n\n    Returns:\n        BGC object\n\n    Examples:\n        &gt;&gt;&gt; bgc = AntismashBGCLoader.parse_bgc(\n        ...    \"/data/antismash/GCF_000016425.1/NC_009380.1.region001.gbk\")\n    \"\"\"\n    fname = os.path.splitext(os.path.basename(file))[0]\n\n    record = SeqIO.read(file, format=\"genbank\")\n    description = record.description  # \"DEFINITION\" in gbk file\n    antismash_id = record.id  # \"VERSION\" in gbk file\n    features = _parse_antismash_genbank(record)\n    product_prediction = features.get(\"product\")\n    if product_prediction is None:\n        raise ValueError(f\"Not found product prediction in antiSMASH Genbank file {file}\")\n\n    # init BGC\n    bgc = BGC(fname, *product_prediction)\n    bgc.description = description\n    bgc.antismash_id = antismash_id\n    bgc.antismash_file = file\n    bgc.antismash_region = features.get(\"region_number\")\n    bgc.smiles = features.get(\"smiles\")\n    bgc.strain = Strain(fname)\n    return bgc\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.get_best_available_genome_id","title":"get_best_available_genome_id","text":"<pre><code>get_best_available_genome_id(genome_id_data: dict[str, str]) -&gt; str | None\n</code></pre> <p>Get the best available ID from genome_id_data dict.</p> <p>Parameters:</p> Name Type Description Default <code>genome_id_data</code> <code>dict[str, str]</code> <p>dictionary containing information for each genome record present.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>ID for the genome, if present, otherwise None.</p> Source code in <code>src/nplinker/genomics/antismash/podp_antismash_downloader.py</code> <pre><code>def get_best_available_genome_id(genome_id_data: dict[str, str]) -&gt; str | None:\n    \"\"\"Get the best available ID from genome_id_data dict.\n\n    Args:\n        genome_id_data: dictionary containing information for each genome record present.\n\n    Returns:\n        ID for the genome, if present, otherwise None.\n    \"\"\"\n    if \"RefSeq_accession\" in genome_id_data:\n        best_id = genome_id_data[\"RefSeq_accession\"]\n    elif \"GenBank_accession\" in genome_id_data:\n        best_id = genome_id_data[\"GenBank_accession\"]\n    elif \"JGI_Genome_ID\" in genome_id_data:\n        best_id = genome_id_data[\"JGI_Genome_ID\"]\n    else:\n        best_id = None\n\n    if best_id is None or len(best_id) == 0:\n        logger.warning(f\"Failed to get valid genome ID in genome data: {genome_id_data}\")\n        return None\n    return best_id\n</code></pre>"},{"location":"api/antismash/#nplinker.genomics.antismash.podp_download_and_extract_antismash_data","title":"podp_download_and_extract_antismash_data","text":"<pre><code>podp_download_and_extract_antismash_data(genome_records: list[dict[str, dict[str, str]]], project_download_root: str | PathLike, project_extract_root: str | PathLike)\n</code></pre> <p>Download and extract antiSMASH BGC archive for the given genome records.</p> <p>Parameters:</p> Name Type Description Default <code>genome_records</code> <code>list[dict[str, dict[str, str]]]</code> <p>list of dicts representing genome records. The dict of each genome record contains     - key(str): \"genome_ID\"     - value(dict[str, str]): a dict containing information about genome     type, label and accession ids (RefSeq, GenBank, and/or JGI).</p> required <code>project_download_root</code> <code>str | PathLike</code> <p>Path to the directory to place downloaded archive in.</p> required <code>project_extract_root</code> <code>str | PathLike</code> <p>Path to the directory downloaded archive will be extracted to. Note that an <code>antismash</code> directory will be created in the specified <code>extract_root</code> if it doesn't exist. The files will be extracted to <code>&lt;extract_root&gt;/antismash/&lt;antismash_id&gt;</code> directory.</p> required Source code in <code>src/nplinker/genomics/antismash/podp_antismash_downloader.py</code> <pre><code>def podp_download_and_extract_antismash_data(\n    genome_records: list[dict[str, dict[str, str]]],\n    project_download_root: str | PathLike,\n    project_extract_root: str | PathLike,\n):\n    \"\"\"Download and extract antiSMASH BGC archive for the given genome records.\n\n    Args:\n        genome_records: list of dicts\n            representing genome records. The dict of each genome record contains\n                - key(str): \"genome_ID\"\n                - value(dict[str, str]): a dict containing information about genome\n                type, label and accession ids (RefSeq, GenBank, and/or JGI).\n        project_download_root: Path to the directory to place\n            downloaded archive in.\n        project_extract_root: Path to the directory downloaded archive\n            will be extracted to.\n            Note that an `antismash` directory will be created in the specified\n            `extract_root` if it doesn't exist. The files will be extracted to\n            `&lt;extract_root&gt;/antismash/&lt;antismash_id&gt;` directory.\n    \"\"\"\n    if not Path(project_download_root).exists():\n        # otherwise in case of failed first download, the folder doesn't exist and\n        # genome_status_file can't be written\n        Path(project_download_root).mkdir(parents=True, exist_ok=True)\n\n    gs_file = Path(project_download_root, GENOME_STATUS_FILENAME)\n    gs_dict = GenomeStatus.read_json(gs_file)\n\n    for i, genome_record in enumerate(genome_records):\n        # get the best available ID from the dict\n        genome_id_data = genome_record[\"genome_ID\"]\n        raw_genome_id = get_best_available_genome_id(genome_id_data)\n        if raw_genome_id is None or len(raw_genome_id) == 0:\n            logger.warning(\n                f'Ignoring genome record \"{genome_record}\" due to missing genome ID field'\n            )\n            continue\n\n        # check if genome ID exist in the genome status file\n        if raw_genome_id not in gs_dict:\n            gs_dict[raw_genome_id] = GenomeStatus(raw_genome_id)\n\n        gs_obj = gs_dict[raw_genome_id]\n\n        logger.info(\n            f\"Checking for antismash data {i + 1}/{len(genome_records)}, \"\n            f\"current genome ID={raw_genome_id}\"\n        )\n        # first, check if BGC data is downloaded\n        if gs_obj.bgc_path and Path(gs_obj.bgc_path).exists():\n            logger.info(f\"Genome ID {raw_genome_id} already downloaded to {gs_obj.bgc_path}\")\n            continue\n        # second, check if lookup attempted previously\n        if gs_obj.resolve_attempted:\n            logger.info(f\"Genome ID {raw_genome_id} skipped due to previous failure\")\n            continue\n\n        # if not downloaded or lookup attempted, then try to resolve the ID\n        # and download\n        logger.info(f\"Beginning lookup process for genome ID {raw_genome_id}\")\n        gs_obj.resolved_refseq_id = _resolve_refseq_id(genome_id_data)\n        gs_obj.resolve_attempted = True\n\n        if gs_obj.resolved_refseq_id == \"\":\n            # give up on this one\n            logger.warning(f\"Failed lookup for genome ID {raw_genome_id}\")\n            continue\n\n        # if resolved id is valid, try to download and extract antismash data\n        try:\n            download_and_extract_antismash_data(\n                gs_obj.resolved_refseq_id, project_download_root, project_extract_root\n            )\n\n            gs_obj.bgc_path = str(\n                Path(project_download_root, gs_obj.resolved_refseq_id + \".zip\").absolute()\n            )\n\n            output_path = Path(project_extract_root, \"antismash\", gs_obj.resolved_refseq_id)\n            if output_path.exists():\n                Path.touch(output_path / \"completed\", exist_ok=True)\n\n        except Exception:\n            gs_obj.bgc_path = \"\"\n\n    missing = len([gs for gs in gs_dict.values() if not gs.bgc_path])\n    logger.info(\n        f\"Dataset has {missing} missing sets of antiSMASH data \"\n        f\" (from a total of {len(genome_records)}).\"\n    )\n\n    # save updated genome status to json file\n    GenomeStatus.to_json(gs_dict, gs_file)\n\n    if missing == len(genome_records):\n        raise ValueError(\"No antiSMASH data found for any genome\")\n</code></pre>"},{"location":"api/arranger/","title":"Dataset Arranger","text":""},{"location":"api/arranger/#nplinker.arranger","title":"arranger","text":""},{"location":"api/arranger/#nplinker.arranger.DatasetArranger","title":"DatasetArranger","text":"<pre><code>DatasetArranger()\n</code></pre> <p>Arrange the dataset required by NPLinker.</p> <p>This class is used to arrange the datasets required by NPLinker according to the configuration. The datasets include MIBiG, GNPS, antiSMASH, and BiG-SCAPE.</p> <p>If <code>config.mode</code> is \"local\", the datasets are validated. If <code>config.mode</code> is \"podp\", the datasets are downloaded or generated.</p> <p>It uses the default downloads directory <code>globals.DOWNLOADS_DEFAULT_PATH</code> to store the downloaded files. Default data paths for MIBiG, GNPS, antiSMASH, and BiG-SCAPE are defined in <code>nplinker.globals</code>.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Arrange the dataset required by NPLinker.\n\n    This class is used to arrange the datasets required by NPLinker according to the\n    configuration. The datasets include MIBiG, GNPS, antiSMASH, and BiG-SCAPE.\n\n    If `config.mode` is \"local\", the datasets are validated.\n    If `config.mode` is \"podp\", the datasets are downloaded or generated.\n\n    It uses the default downloads directory `globals.DOWNLOADS_DEFAULT_PATH` to store the\n    downloaded files. Default data paths for MIBiG, GNPS, antiSMASH, and BiG-SCAPE are defined\n    in `nplinker.globals`.\n    \"\"\"\n    # Prepare the downloads directory and/or PODP json file which are required for other methods\n    globals.DOWNLOADS_DEFAULT_PATH.mkdir(exist_ok=True)\n    self.arrange_podp_project_json()\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange","title":"arrange","text":"<pre><code>arrange() -&gt; None\n</code></pre> <p>Arrange the datasets according to the configuration.</p> <p>The datasets include MIBiG, GNPS, antiSMASH, and BiG-SCAPE.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange(self) -&gt; None:\n    \"\"\"Arrange the datasets according to the configuration.\n\n    The datasets include MIBiG, GNPS, antiSMASH, and BiG-SCAPE.\n    \"\"\"\n    # The order of arranging the datasets matters, as some datasets depend on others\n    self.arrange_mibig()\n    self.arrange_gnps()\n    self.arrange_antismash()\n    self.arrange_bigscape()\n    self.arrange_strain_mappings()\n    self.arrange_strains_selected()\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_podp_project_json","title":"arrange_podp_project_json","text":"<pre><code>arrange_podp_project_json() -&gt; None\n</code></pre> <p>Arrange the PODP project JSON file.</p> <p>If <code>config.mode</code> is \"podp\", download the PODP project JSON file if it doesn't exist. Then validate the PODP project JSON file if it exists or is downloaded.</p> <p>The validation is controlled by the json schema <code>schemas/podp_adapted_schema.json</code>.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_podp_project_json(self) -&gt; None:\n    \"\"\"Arrange the PODP project JSON file.\n\n    If `config.mode` is \"podp\", download the PODP project JSON file if it doesn't exist. Then\n    validate the PODP project JSON file if it exists or is downloaded.\n\n    The validation is controlled by the json schema `schemas/podp_adapted_schema.json`.\n    \"\"\"\n    if config.mode == \"podp\":\n        file_name = f\"paired_datarecord_{config.podp_id}.json\"\n        podp_file = globals.DOWNLOADS_DEFAULT_PATH / file_name\n        if not podp_file.exists():\n            download_url(\n                PODP_PROJECT_URL.format(config.podp_id),\n                globals.DOWNLOADS_DEFAULT_PATH,\n                file_name,\n            )\n\n        with open(podp_file, \"r\") as f:\n            json_data = json.load(f)\n        validate_podp_json(json_data)\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_mibig","title":"arrange_mibig","text":"<pre><code>arrange_mibig() -&gt; None\n</code></pre> <p>Arrange the MIBiG metadata.</p> <p>Always download and extract the MIBiG metadata if <code>config.mibig.to_use</code> is True. If the default directory has already existed, it will be removed and re-downloaded to ensure the latest version is used. So it's not allowed to manually put MIBiG metadata in the default directory.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_mibig(self) -&gt; None:\n    \"\"\"Arrange the MIBiG metadata.\n\n    Always download and extract the MIBiG metadata if `config.mibig.to_use` is True.\n    If the default directory has already existed, it will be removed and re-downloaded to ensure\n    the latest version is used. So it's not allowed to manually put MIBiG metadata in the\n    default directory.\n    \"\"\"\n    if config.mibig.to_use:\n        if globals.MIBIG_DEFAULT_PATH.exists():\n            # remove existing mibig data\n            shutil.rmtree(globals.MIBIG_DEFAULT_PATH)\n        download_and_extract_mibig_metadata(\n            globals.DOWNLOADS_DEFAULT_PATH,\n            globals.MIBIG_DEFAULT_PATH,\n            version=config.mibig.version,\n        )\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_gnps","title":"arrange_gnps","text":"<pre><code>arrange_gnps() -&gt; None\n</code></pre> <p>Arrange the GNPS data.</p> <p>If <code>config.mode</code> is \"local\", validate the GNPS data directory. If <code>config.mode</code> is \"podp\", download the GNPS data if it doesn't exist or remove the existing GNPS data and re-download it if it is invalid.</p> <p>The validation process includes:</p> <ul> <li>Check if the GNPS data directory exists.</li> <li>Check if the required files exist in the GNPS data directory, including:<ul> <li>file_mappings.tsv or file_mappings.csv</li> <li>spectra.mgf</li> <li>molecular_families.tsv</li> <li>annotations.tsv</li> </ul> </li> </ul> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_gnps(self) -&gt; None:\n    \"\"\"Arrange the GNPS data.\n\n    If `config.mode` is \"local\", validate the GNPS data directory.\n    If `config.mode` is \"podp\", download the GNPS data if it doesn't exist or remove the\n    existing GNPS data and re-download it if it is invalid.\n\n    The validation process includes:\n\n    - Check if the GNPS data directory exists.\n    - Check if the required files exist in the GNPS data directory, including:\n        - file_mappings.tsv or file_mappings.csv\n        - spectra.mgf\n        - molecular_families.tsv\n        - annotations.tsv\n    \"\"\"\n    pass_validation = False\n    if config.mode == \"podp\":\n        # retry downloading at most 3 times if downloaded data has problems\n        for _ in range(3):\n            try:\n                validate_gnps(globals.GNPS_DEFAULT_PATH)\n                pass_validation = True\n                break\n            except (FileNotFoundError, ValueError):\n                # Don't need to remove downloaded archive, as it'll be overwritten\n                shutil.rmtree(globals.GNPS_DEFAULT_PATH, ignore_errors=True)\n                self._download_and_extract_gnps()\n\n    if not pass_validation:\n        validate_gnps(globals.GNPS_DEFAULT_PATH)\n\n    # get the path to file_mappings file (csv or tsv)\n    self.gnps_file_mappings_file = self._get_gnps_file_mappings_file()\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_antismash","title":"arrange_antismash","text":"<pre><code>arrange_antismash() -&gt; None\n</code></pre> <p>Arrange the antiSMASH data.</p> <p>If <code>config.mode</code> is \"local\", validate the antiSMASH data directory. If <code>config.mode</code> is \"podp\", download the antiSMASH data if it doesn't exist or remove the existing antiSMASH data and re-download it if it is invalid.</p> <p>The validation process includes: - Check if the antiSMASH data directory exists. - Check if the antiSMASH data directory contains at least one sub-directory, and each     sub-directory contains at least one BGC file (with the suffix \".region???.gbk\" where ???     is a number).</p> <p>AntiSMASH BGC directory must follow the structure below: <pre><code>antismash\n    \u251c\u2500\u2500 genome_id_1 (one AntiSMASH output, e.g. GCF_000514775.1)\n    \u2502\u00a0 \u251c\u2500\u2500 GCF_000514775.1.gbk\n    \u2502\u00a0 \u251c\u2500\u2500 NZ_AZWO01000004.region001.gbk\n    \u2502\u00a0 \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 genome_id_2\n    \u2502\u00a0 \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre></p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_antismash(self) -&gt; None:\n    \"\"\"Arrange the antiSMASH data.\n\n    If `config.mode` is \"local\", validate the antiSMASH data directory.\n    If `config.mode` is \"podp\", download the antiSMASH data if it doesn't exist or remove the\n    existing antiSMASH data and re-download it if it is invalid.\n\n    The validation process includes:\n    - Check if the antiSMASH data directory exists.\n    - Check if the antiSMASH data directory contains at least one sub-directory, and each\n        sub-directory contains at least one BGC file (with the suffix \".region???.gbk\" where ???\n        is a number).\n\n    AntiSMASH BGC directory must follow the structure below:\n    ```\n    antismash\n        \u251c\u2500\u2500 genome_id_1 (one AntiSMASH output, e.g. GCF_000514775.1)\n        \u2502\u00a0 \u251c\u2500\u2500 GCF_000514775.1.gbk\n        \u2502\u00a0 \u251c\u2500\u2500 NZ_AZWO01000004.region001.gbk\n        \u2502\u00a0 \u2514\u2500\u2500 ...\n        \u251c\u2500\u2500 genome_id_2\n        \u2502\u00a0 \u251c\u2500\u2500 ...\n        \u2514\u2500\u2500 ...\n    ```\n    \"\"\"\n    pass_validation = False\n    if config.mode == \"podp\":\n        for _ in range(3):\n            try:\n                validate_antismash(globals.ANTISMASH_DEFAULT_PATH)\n                pass_validation = True\n                break\n            except FileNotFoundError:\n                shutil.rmtree(globals.ANTISMASH_DEFAULT_PATH, ignore_errors=True)\n                self._download_and_extract_antismash()\n\n    if not pass_validation:\n        validate_antismash(globals.ANTISMASH_DEFAULT_PATH)\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_bigscape","title":"arrange_bigscape","text":"<pre><code>arrange_bigscape() -&gt; None\n</code></pre> <p>Arrange the BiG-SCAPE data.</p> <p>If <code>config.mode</code> is \"local\", validate the BiG-SCAPE data directory. If <code>config.mode</code> is \"podp\", run BiG-SCAPE to generate the clustering file if it doesn't exist or remove the existing BiG-SCAPE data and re-run BiG-SCAPE if it is invalid. The running output of BiG-SCAPE will be saved to the directory \"bigscape_running_output\" in the default BiG-SCAPE directory, and the clustering file \"mix_clustering_c{config.bigscape.cutoff}.tsv\" will be copied to the default BiG-SCAPE directory.</p> <p>The validation process includes:</p> <ul> <li>Check if the default BiG-SCAPE data directory exists.</li> <li>Check if the clustering file \"mix_clustering_c{config.bigscape.cutoff}.tsv\" exists in the         BiG-SCAPE data directory.</li> </ul> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_bigscape(self) -&gt; None:\n    \"\"\"Arrange the BiG-SCAPE data.\n\n    If `config.mode` is \"local\", validate the BiG-SCAPE data directory.\n    If `config.mode` is \"podp\", run BiG-SCAPE to generate the clustering file if it doesn't\n    exist or remove the existing BiG-SCAPE data and re-run BiG-SCAPE if it is invalid.\n    The running output of BiG-SCAPE will be saved to the directory \"bigscape_running_output\"\n    in the default BiG-SCAPE directory, and the clustering file \"mix_clustering_c{config.bigscape.cutoff}.tsv\"\n    will be copied to the default BiG-SCAPE directory.\n\n    The validation process includes:\n\n    - Check if the default BiG-SCAPE data directory exists.\n    - Check if the clustering file \"mix_clustering_c{config.bigscape.cutoff}.tsv\" exists in the\n            BiG-SCAPE data directory.\n    \"\"\"\n    pass_validation = False\n    if config.mode == \"podp\":\n        for _ in range(3):\n            try:\n                validate_bigscape(globals.BIGSCAPE_DEFAULT_PATH)\n                pass_validation = True\n                break\n            except FileNotFoundError:\n                shutil.rmtree(globals.BIGSCAPE_DEFAULT_PATH, ignore_errors=True)\n                self._run_bigscape()\n\n    if not pass_validation:\n        validate_bigscape(globals.BIGSCAPE_DEFAULT_PATH)\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_strain_mappings","title":"arrange_strain_mappings","text":"<pre><code>arrange_strain_mappings() -&gt; None\n</code></pre> <p>Arrange the strain mappings file.</p> <p>If <code>config.mode</code> is \"local\", validate the strain mappings file. If <code>config.mode</code> is \"podp\", always generate the strain mappings file and validate it.</p> <p>The valiation checks if the strain mappings file exists and if it is a valid JSON file according to the schema defined in <code>schemas/strain_mappings_schema.json</code>.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_strain_mappings(self) -&gt; None:\n    \"\"\"Arrange the strain mappings file.\n\n    If `config.mode` is \"local\", validate the strain mappings file.\n    If `config.mode` is \"podp\", always generate the strain mappings file and validate it.\n\n    The valiation checks if the strain mappings file exists and if it is a valid JSON file\n    according to the schema defined in `schemas/strain_mappings_schema.json`.\n    \"\"\"\n    if config.mode == \"podp\":\n        self._generate_strain_mappings()\n\n    self._validate_strain_mappings()\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.DatasetArranger.arrange_strains_selected","title":"arrange_strains_selected","text":"<pre><code>arrange_strains_selected() -&gt; None\n</code></pre> <p>Arrange the strains selected file.</p> <p>Validate the strains selected file if it exists. The validation checks if the strains selected file is a valid JSON file according to the schema defined in <code>schemas/user_strains.json</code>.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def arrange_strains_selected(self) -&gt; None:\n    \"\"\"Arrange the strains selected file.\n\n    Validate the strains selected file if it exists.\n    The validation checks if the strains selected file is a valid JSON file according to the\n    schema defined in `schemas/user_strains.json`.\n    \"\"\"\n    strains_selected_file = config.root_dir / globals.STRAINS_SELECTED_FILENAME\n    if strains_selected_file.exists():\n        with open(strains_selected_file, \"r\") as f:\n            json_data = json.load(f)\n        validate(instance=json_data, schema=USER_STRAINS_SCHEMA)\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.validate_gnps","title":"validate_gnps","text":"<pre><code>validate_gnps(gnps_dir: Path) -&gt; None\n</code></pre> <p>Validate the GNPS data directory and its contents.</p> <p>The GNPS data directory must contain the following files:</p> <ul> <li>file_mappings.tsv or file_mappings.csv</li> <li>spectra.mgf</li> <li>molecular_families.tsv</li> <li>annotations.tsv</li> </ul> <p>Parameters:</p> Name Type Description Default <code>gnps_dir</code> <code>Path</code> <p>Path to the GNPS data directory.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the GNPS data directory is not found or any of the required files is not found.</p> <code>ValueError</code> <p>If both file_mappings.tsv and file_mapping.csv are found.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def validate_gnps(gnps_dir: Path) -&gt; None:\n    \"\"\"Validate the GNPS data directory and its contents.\n\n    The GNPS data directory must contain the following files:\n\n    - file_mappings.tsv or file_mappings.csv\n    - spectra.mgf\n    - molecular_families.tsv\n    - annotations.tsv\n\n    Args:\n        gnps_dir: Path to the GNPS data directory.\n\n    Raises:\n        FileNotFoundError: If the GNPS data directory is not found or any of the required files\n            is not found.\n        ValueError: If both file_mappings.tsv and file_mapping.csv are found.\n    \"\"\"\n    if not gnps_dir.exists():\n        raise FileNotFoundError(f\"GNPS data directory not found at {gnps_dir}\")\n\n    file_mappings_tsv = gnps_dir / globals.GNPS_FILE_MAPPINGS_TSV\n    file_mappings_csv = gnps_dir / globals.GNPS_FILE_MAPPINGS_CSV\n    if file_mappings_tsv.exists() and file_mappings_csv.exists():\n        raise ValueError(\n            f\"Both {file_mappings_tsv.name} and {file_mappings_csv.name} found in GNPS directory \"\n            f\"{gnps_dir}, only one is allowed.\"\n        )\n    elif not file_mappings_tsv.exists() and not file_mappings_csv.exists():\n        raise FileNotFoundError(\n            f\"Neither {file_mappings_tsv.name} nor {file_mappings_csv.name} found in GNPS directory\"\n            f\" {gnps_dir}\"\n        )\n\n    required_files = [\n        gnps_dir / globals.GNPS_SPECTRA_FILENAME,\n        gnps_dir / globals.GNPS_MOLECULAR_FAMILY_FILENAME,\n        gnps_dir / globals.GNPS_ANNOTATIONS_FILENAME,\n    ]\n    list_not_found = [f.name for f in required_files if not f.exists()]\n    if list_not_found:\n        raise FileNotFoundError(\n            f\"Files not found in GNPS directory {gnps_dir}: ', '.join({list_not_found})\"\n        )\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.validate_antismash","title":"validate_antismash","text":"<pre><code>validate_antismash(antismash_dir: Path) -&gt; None\n</code></pre> <p>Validate the antiSMASH data directory and its contents.</p> <p>The validation only checks the structure of the antiSMASH data directory and file names. It does not check</p> <ul> <li>the content of the BGC files</li> <li>the consistency between the antiSMASH data and the PODP project JSON file for the PODP     mode</li> </ul> <p>The antiSMASH data directory must exist and contain at least one sub-directory. The name of the sub-directories must not contain any space. Each sub-directory must contain at least one BGC file (with the suffix \".region???.gbk\" where ??? is the region number).</p> <p>Parameters:</p> Name Type Description Default <code>antismash_dir</code> <code>Path</code> <p>Path to the antiSMASH data directory.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the antiSMASH data directory is not found, or no sub-directories are found in the antiSMASH data directory, or no BGC files are found in any sub-directory.</p> <code>ValueError</code> <p>If any sub-directory name contains a space.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def validate_antismash(antismash_dir: Path) -&gt; None:\n    \"\"\"Validate the antiSMASH data directory and its contents.\n\n    The validation only checks the structure of the antiSMASH data directory and file names.\n    It does not check\n\n    - the content of the BGC files\n    - the consistency between the antiSMASH data and the PODP project JSON file for the PODP\n        mode\n\n    The antiSMASH data directory must exist and contain at least one sub-directory. The name of the\n    sub-directories must not contain any space. Each sub-directory must contain at least one BGC\n    file (with the suffix \".region???.gbk\" where ??? is the region number).\n\n    Args:\n        antismash_dir: Path to the antiSMASH data directory.\n\n    Raises:\n        FileNotFoundError: If the antiSMASH data directory is not found, or no sub-directories\n            are found in the antiSMASH data directory, or no BGC files are found in any\n            sub-directory.\n        ValueError: If any sub-directory name contains a space.\n    \"\"\"\n    if not antismash_dir.exists():\n        raise FileNotFoundError(f\"antiSMASH data directory not found at {antismash_dir}\")\n\n    sub_dirs = list_dirs(antismash_dir)\n    if not sub_dirs:\n        raise FileNotFoundError(\n            \"No BGC directories found in antiSMASH data directory {antismash_dir}\"\n        )\n\n    for sub_dir in sub_dirs:\n        dir_name = Path(sub_dir).name\n        if \" \" in dir_name:\n            raise ValueError(\n                f\"antiSMASH sub-directory name {dir_name} contains space, which is not allowed\"\n            )\n\n        gbk_files = list_files(sub_dir, suffix=\".gbk\", keep_parent=False)\n        bgc_files = fnmatch.filter(gbk_files, \"*.region???.gbk\")\n        if not bgc_files:\n            raise FileNotFoundError(f\"No BGC files found in antiSMASH sub-directory {sub_dir}\")\n</code></pre>"},{"location":"api/arranger/#nplinker.arranger.validate_bigscape","title":"validate_bigscape","text":"<pre><code>validate_bigscape(bigscape_dir: Path) -&gt; None\n</code></pre> <p>Validate the BiG-SCAPE data directory and its contents.</p> <p>The BiG-SCAPE data directory must exist and contain the clustering file \"mix_clustering_c{config.bigscape.cutoff}.tsv\" where {config.bigscape.cutoff} is the bigscape cutoff value set in the config file.</p> <p>Parameters:</p> Name Type Description Default <code>bigscape_dir</code> <code>Path</code> <p>Path to the BiG-SCAPE data directory.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the BiG-SCAPE data directory or the clustering file is not found.</p> Source code in <code>src/nplinker/arranger.py</code> <pre><code>def validate_bigscape(bigscape_dir: Path) -&gt; None:\n    \"\"\"Validate the BiG-SCAPE data directory and its contents.\n\n    The BiG-SCAPE data directory must exist and contain the clustering file\n    \"mix_clustering_c{config.bigscape.cutoff}.tsv\" where {config.bigscape.cutoff} is the\n    bigscape cutoff value set in the config file.\n\n    Args:\n        bigscape_dir: Path to the BiG-SCAPE data directory.\n\n    Raises:\n        FileNotFoundError: If the BiG-SCAPE data directory or the clustering file is not found.\n    \"\"\"\n    if not bigscape_dir.exists():\n        raise FileNotFoundError(f\"BiG-SCAPE data directory not found at {bigscape_dir}\")\n\n    clustering_file = bigscape_dir / f\"mix_clustering_c{config.bigscape.cutoff}.tsv\"\n    if not clustering_file.exists():\n        raise FileNotFoundError(f\"BiG-SCAPE clustering file not found: {clustering_file}\")\n</code></pre>"},{"location":"api/bigscape/","title":"BigScape","text":""},{"location":"api/bigscape/#nplinker.genomics.bigscape","title":"bigscape","text":""},{"location":"api/bigscape/#nplinker.genomics.bigscape.BigscapeGCFLoader","title":"BigscapeGCFLoader","text":"<pre><code>BigscapeGCFLoader(cluster_file: str | PathLike)\n</code></pre> <p>Build a loader for BiG-SCAPE GCF cluster file.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <code>str | PathLike</code> <p>Path to the BiG-SCAPE cluster file, the filename has a pattern of \"_clustering_c0.xx.tsv\". required <p>Attributes:</p> Name Type Description <code>cluster_file</code> <p>path to the BiG-SCAPE cluster file.</p> Source code in <code>src/nplinker/genomics/bigscape/bigscape_loader.py</code> <pre><code>def __init__(self, cluster_file: str | PathLike, /) -&gt; None:\n    \"\"\"Build a loader for BiG-SCAPE GCF cluster file.\n\n    Args:\n        cluster_file: Path to the BiG-SCAPE cluster file,\n            the filename has a pattern of \"&lt;class&gt;_clustering_c0.xx.tsv\".\n\n    Attributes:\n        cluster_file: path to the BiG-SCAPE cluster file.\n    \"\"\"\n    self.cluster_file = str(cluster_file)\n    self._gcf_list = self._parse_gcf(self.cluster_file)\n</code></pre>"},{"location":"api/bigscape/#nplinker.genomics.bigscape.BigscapeGCFLoader.get_gcfs","title":"get_gcfs","text":"<pre><code>get_gcfs(keep_mibig_only: bool = False, keep_singleton: bool = False) -&gt; list[GCF]\n</code></pre> <p>Get all GCF objects.</p> <p>Parameters:</p> Name Type Description Default <code>keep_mibig_only</code> <code>bool</code> <p>True to keep GCFs that contain only MIBiG BGCs.</p> <code>False</code> <code>keep_singleton</code> <code>bool</code> <p>True to keep singleton GCFs. A singleton GCF is a GCF that contains only one BGC.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[GCF]</code> <p>list[GCF]: a list of GCF objects.</p> Source code in <code>src/nplinker/genomics/bigscape/bigscape_loader.py</code> <pre><code>def get_gcfs(self, keep_mibig_only: bool = False, keep_singleton: bool = False) -&gt; list[GCF]:\n    \"\"\"Get all GCF objects.\n\n    Args:\n        keep_mibig_only: True to keep GCFs that contain only MIBiG\n            BGCs.\n        keep_singleton: True to keep singleton GCFs. A singleton GCF\n            is a GCF that contains only one BGC.\n\n    Returns:\n        list[GCF]: a list of GCF objects.\n    \"\"\"\n    gcf_list = self._gcf_list\n    if not keep_mibig_only:\n        gcf_list = [gcf for gcf in gcf_list if not gcf.has_mibig_only()]\n    if not keep_singleton:\n        gcf_list = [gcf for gcf in gcf_list if not gcf.is_singleton()]\n    return gcf_list\n</code></pre>"},{"location":"api/bigscape/#nplinker.genomics.bigscape.run_bigscape","title":"run_bigscape","text":"<pre><code>run_bigscape(antismash_path: str | PathLike, output_path: str | PathLike, extra_params: str)\n</code></pre> Source code in <code>src/nplinker/genomics/bigscape/runbigscape.py</code> <pre><code>def run_bigscape(\n    antismash_path: str | PathLike,\n    output_path: str | PathLike,\n    extra_params: str,\n):\n    bigscape_py_path = \"bigscape.py\"\n    logger.info(\n        f'run_bigscape: input=\"{antismash_path}\", output=\"{output_path}\", extra_params={extra_params}\"'\n    )\n\n    try:\n        subprocess.run([bigscape_py_path, \"-h\"], capture_output=True, check=True)\n    except Exception as e:\n        raise Exception(f\"Failed to find/run bigscape.py (path={bigscape_py_path}, err={e})\") from e\n\n    if not os.path.exists(antismash_path):\n        raise Exception(f'antismash_path \"{antismash_path}\" does not exist!')\n\n    # configure the IO-related parameters, including pfam_dir\n    args = [bigscape_py_path, \"-i\", antismash_path, \"-o\", output_path, \"--pfam_dir\", PFAM_PATH]\n\n    # append the user supplied params, if any\n    if len(extra_params) &gt; 0:\n        args.extend(extra_params.split(\" \"))\n\n    logger.info(f\"BiG-SCAPE command: {args}\")\n    result = subprocess.run(args, stdout=sys.stdout, stderr=sys.stderr, check=True)\n    logger.info(f\"BiG-SCAPE completed with return code {result.returncode}\")\n    # use subprocess.CompletedProcess.check_returncode() to test if the BiG-SCAPE\n    # process exited successfully. This throws an exception for non-zero returncodes\n    # which will indicate to the PODPDownloader module that something went wrong.\n    result.check_returncode()\n\n    return True\n</code></pre>"},{"location":"api/genomics/","title":"Data Models","text":""},{"location":"api/genomics/#nplinker.genomics","title":"genomics","text":""},{"location":"api/genomics/#nplinker.genomics.BGC","title":"BGC","text":"<pre><code>BGC(bgc_id: str, /, *product_prediction: str)\n</code></pre> <p>Class to model BGC (biosynthetic gene cluster) data.</p> <p>BGC data include both annotations and sequence data. This class is mainly designed to model the annotations or metadata.</p> <p>The raw BGC data is stored in GenBank format (.gbk). Additional <code>GenBank features</code>_ could be added to the GenBank file to annotate BGCs, e.g. antiSMASH has some self-defined features (like \"region\") in its output GenBank files.</p> <p>The annotations of BGC can be stored in JSON format, which is defined and used by MIBiG.</p> <p>Parameters:</p> Name Type Description Default <code>bgc_id</code> <code>str</code> <p>BGC identifier, e.g. MIBiG accession, GenBank accession.</p> required <code>product_prediction</code> <code>str</code> <p>BGC's (predicted) natural products or product classes.</p> <code>()</code> <p>Attributes:</p> Name Type Description <code>bgc_id</code> <p>BGC identifier, e.g. MIBiG accession, GenBank accession.</p> <code>product_prediction</code> <p>A tuple of (predicted) natural products or product classes of the BGC. For antiSMASH's GenBank data, the feature <code>region /product</code> gives product information. For MIBiG metadata, its biosynthetic class provides such info.</p> <code>mibig_bgc_class</code> <p>A tuple of MIBiG biosynthetic classes to which the BGC belongs. Defaults to None. MIBiG defines 6 major biosynthetic classes for natural products, including \"NRP\", \"Polyketide\", \"RiPP\", \"Terpene\", \"Saccharide\" and \"Alkaloid\". Note that natural products created by all other biosynthetic mechanisms fall under the category \"Other\". More details see the publication: https://doi.org/10.1186/s40793-018-0318-y.</p> <code>description</code> <p>Brief description of the BGC. Defaults to None.</p> <code>smiles</code> <p>A tuple of SMILES formulas of the BGC's products. Defaults to None.</p> <code>antismash_file</code> <p>The path to the antiSMASH GenBank file. Defaults to None.</p> <code>antismash_id</code> <p>Identifier of the antiSMASH BGC, referring to the feature <code>VERSION</code> of GenBank file. Defaults to None.</p> <code>antismash_region</code> <p>AntiSMASH BGC region number, referring to the feature <code>region</code> of GenBank file. Defaults to None.</p> <code>parents</code> <p>The set of GCFs that contain the BGC.</p> <code>strain</code> <p>The strain of the BGC.</p> <p>.. GenBank features:     https://www.insdc.org/submitting-standards/feature-table/</p> Source code in <code>src/nplinker/genomics/bgc.py</code> <pre><code>def __init__(self, bgc_id: str, /, *product_prediction: str):\n    \"\"\"Class to model BGC (biosynthetic gene cluster) data.\n\n    BGC data include both annotations and sequence data. This class is\n    mainly designed to model the annotations or metadata.\n\n    The raw BGC data is stored in GenBank format (.gbk). Additional\n    `GenBank features`_ could be added to the GenBank file to annotate\n    BGCs, e.g. antiSMASH has some self-defined features (like \"region\") in\n    its output GenBank files.\n\n    The annotations of BGC can be stored in JSON format, which is defined\n    and used by MIBiG.\n\n    Args:\n        bgc_id: BGC identifier, e.g. MIBiG accession, GenBank accession.\n        product_prediction: BGC's (predicted) natural products\n            or product classes.\n\n    Attributes:\n        bgc_id: BGC identifier, e.g. MIBiG accession, GenBank accession.\n        product_prediction: A tuple of (predicted) natural\n            products or product classes of the BGC.\n            For antiSMASH's GenBank data, the feature `region /product`\n            gives product information.\n            For MIBiG metadata, its biosynthetic class provides such info.\n        mibig_bgc_class: A tuple of MIBiG biosynthetic\n            classes to which the BGC belongs.\n            Defaults to None.\n            MIBiG defines 6 major biosynthetic classes for natural products,\n            including \"NRP\", \"Polyketide\", \"RiPP\", \"Terpene\", \"Saccharide\"\n            and \"Alkaloid\". Note that natural products created by all other\n            biosynthetic mechanisms fall under the category \"Other\".\n            More details see the publication: https://doi.org/10.1186/s40793-018-0318-y.\n        description: Brief description of the BGC.\n            Defaults to None.\n        smiles: A tuple of SMILES formulas of the BGC's\n            products.\n            Defaults to None.\n        antismash_file: The path to the antiSMASH GenBank file.\n            Defaults to None.\n        antismash_id: Identifier of the antiSMASH BGC, referring\n            to the feature `VERSION` of GenBank file.\n            Defaults to None.\n        antismash_region: AntiSMASH BGC region number, referring\n            to the feature `region` of GenBank file.\n            Defaults to None.\n        parents: The set of GCFs that contain the BGC.\n        strain: The strain of the BGC.\n\n    .. GenBank features:\n        https://www.insdc.org/submitting-standards/feature-table/\n    \"\"\"\n    # BGC metadata\n    self.bgc_id = bgc_id\n    self.product_prediction = product_prediction\n\n    self.mibig_bgc_class: tuple[str] | None = None\n    self.description: str | None = None\n    self.smiles: tuple[str] | None = None\n\n    # antismash related attributes\n    self.antismash_file: str | None = None\n    self.antismash_id: str | None = None  # version in .gbk, id in SeqRecord\n    self.antismash_region: int | None = None  # antismash region number\n\n    # other attributes\n    self.parents: set[GCF] = set()\n    self._strain: Strain | None = None\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.BGC.bigscape_classes","title":"bigscape_classes  <code>property</code>","text":"<pre><code>bigscape_classes: set[str | None]\n</code></pre> <p>Get BiG-SCAPE's BGC classes.</p> <p>BiG-SCAPE's BGC classes are similar to those defined in MiBIG but have more categories (7 classes). More details see: https://doi.org/10.1038%2Fs41589-019-0400-9.</p>"},{"location":"api/genomics/#nplinker.genomics.BGC.aa_predictions","title":"aa_predictions  <code>property</code>","text":"<pre><code>aa_predictions: list\n</code></pre> <p>Amino acids as predicted monomers of product.</p> <p>Returns:</p> Type Description <code>list</code> <p>list of dicts with key as amino acid and value as prediction</p> <code>list</code> <p>probability.</p>"},{"location":"api/genomics/#nplinker.genomics.BGC.add_parent","title":"add_parent","text":"<pre><code>add_parent(gcf: GCF) -&gt; None\n</code></pre> <p>Add a parent GCF to the BGC.</p> <p>Parameters:</p> Name Type Description Default <code>gcf</code> <code>GCF</code> <p>gene cluster family</p> required Source code in <code>src/nplinker/genomics/bgc.py</code> <pre><code>def add_parent(self, gcf: GCF) -&gt; None:\n    \"\"\"Add a parent GCF to the BGC.\n\n    Args:\n        gcf: gene cluster family\n    \"\"\"\n    gcf.add_bgc(self)\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.BGC.detach_parent","title":"detach_parent","text":"<pre><code>detach_parent(gcf: GCF) -&gt; None\n</code></pre> <p>Remove a parent GCF.</p> Source code in <code>src/nplinker/genomics/bgc.py</code> <pre><code>def detach_parent(self, gcf: GCF) -&gt; None:\n    \"\"\"Remove a parent GCF.\"\"\"\n    gcf.detach_bgc(self)\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.BGC.is_mibig","title":"is_mibig","text":"<pre><code>is_mibig() -&gt; bool\n</code></pre> <p>Check if the BGC is MIBiG reference BGC or not.</p> Note <p>This method evaluates MIBiG BGC based on the pattern that MIBiG BGC names start with \"BGC\". It might give false positive result.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if it's MIBiG reference BGC</p> Source code in <code>src/nplinker/genomics/bgc.py</code> <pre><code>def is_mibig(self) -&gt; bool:\n    \"\"\"Check if the BGC is MIBiG reference BGC or not.\n\n    Note:\n        This method evaluates MIBiG BGC based on the pattern that MIBiG\n        BGC names start with \"BGC\". It might give false positive result.\n\n    Returns:\n        True if it's MIBiG reference BGC\n    \"\"\"\n    return self.bgc_id.startswith(\"BGC\")\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.GCF","title":"GCF","text":"<pre><code>GCF(gcf_id: str)\n</code></pre> <p>Class to model gene cluster family (GCF).</p> <p>GCF is a group of similar BGCs and generated by clustering BGCs with tools such as BiG-SCAPE and BiG-SLICE.</p> <p>Parameters:</p> Name Type Description Default <code>gcf_id</code> <code>str</code> <p>id of the GCF object.</p> required <p>Attributes:</p> Name Type Description <code>gcf_id</code> <p>id of the GCF object.</p> <code>bgc_ids</code> <p>a set of BGC ids that belongs to the GCF.</p> <code>bigscape_class</code> <p>BiG-SCAPE's BGC class. BiG-SCAPE's BGC classes are similar to those defined in MiBIG but have more categories (7 classes). More details see: https://doi.org/10.1038%2Fs41589-019-0400-9.</p> Source code in <code>src/nplinker/genomics/gcf.py</code> <pre><code>def __init__(self, gcf_id: str, /) -&gt; None:\n    \"\"\"Class to model gene cluster family (GCF).\n\n    GCF is a group of similar BGCs and generated by clustering BGCs with\n    tools such as BiG-SCAPE and BiG-SLICE.\n\n    Args:\n        gcf_id: id of the GCF object.\n\n    Attributes:\n        gcf_id: id of the GCF object.\n        bgc_ids: a set of BGC ids that belongs to the GCF.\n        bigscape_class: BiG-SCAPE's BGC class.\n            BiG-SCAPE's BGC classes are similar to those defined in MiBIG\n            but have more categories (7 classes). More details see:\n            https://doi.org/10.1038%2Fs41589-019-0400-9.\n    \"\"\"\n    self.gcf_id = gcf_id\n    self.bgc_ids: set[str] = set()\n    self.bigscape_class: str | None = None\n    self._bgcs: set[BGC] = set()\n    self._strains: StrainCollection = StrainCollection()\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.GCF.bgcs","title":"bgcs  <code>property</code>","text":"<pre><code>bgcs: set[BGC]\n</code></pre> <p>Get the BGC objects.</p>"},{"location":"api/genomics/#nplinker.genomics.GCF.strains","title":"strains  <code>property</code>","text":"<pre><code>strains: StrainCollection\n</code></pre> <p>Get the strains in the GCF.</p>"},{"location":"api/genomics/#nplinker.genomics.GCF.add_bgc","title":"add_bgc","text":"<pre><code>add_bgc(bgc: BGC) -&gt; None\n</code></pre> <p>Add a BGC object to the GCF.</p> Source code in <code>src/nplinker/genomics/gcf.py</code> <pre><code>def add_bgc(self, bgc: BGC) -&gt; None:\n    \"\"\"Add a BGC object to the GCF.\"\"\"\n    bgc.parents.add(self)\n    self._bgcs.add(bgc)\n    self.bgc_ids.add(bgc.bgc_id)\n    if bgc.strain is not None:\n        self._strains.add(bgc.strain)\n    else:\n        logger.warning(\"No strain specified for the BGC %s\", bgc.bgc_id)\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.GCF.detach_bgc","title":"detach_bgc","text":"<pre><code>detach_bgc(bgc: BGC) -&gt; None\n</code></pre> <p>Remove a child BGC object.</p> Source code in <code>src/nplinker/genomics/gcf.py</code> <pre><code>def detach_bgc(self, bgc: BGC) -&gt; None:\n    \"\"\"Remove a child BGC object.\"\"\"\n    bgc.parents.remove(self)\n    self._bgcs.remove(bgc)\n    self.bgc_ids.remove(bgc.bgc_id)\n    if bgc.strain is not None:\n        for other_bgc in self._bgcs:\n            if other_bgc.strain == bgc.strain:\n                return\n        self._strains.remove(bgc.strain)\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.GCF.has_strain","title":"has_strain","text":"<pre><code>has_strain(strain: Strain) -&gt; bool\n</code></pre> <p>Check if the given strain exists.</p> <p>Parameters:</p> Name Type Description Default <code>strain</code> <code>Strain</code> <p><code>Strain</code> object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True when the given strain exist.</p> Source code in <code>src/nplinker/genomics/gcf.py</code> <pre><code>def has_strain(self, strain: Strain) -&gt; bool:\n    \"\"\"Check if the given strain exists.\n\n    Args:\n        strain: `Strain` object.\n\n    Returns:\n        True when the given strain exist.\n    \"\"\"\n    return strain in self._strains\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.GCF.has_mibig_only","title":"has_mibig_only","text":"<pre><code>has_mibig_only() -&gt; bool\n</code></pre> <p>Check if the GCF's children are only MIBiG BGCs.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>GCF.bgc_ids</code> are only MIBiG BGC ids.</p> Source code in <code>src/nplinker/genomics/gcf.py</code> <pre><code>def has_mibig_only(self) -&gt; bool:\n    \"\"\"Check if the GCF's children are only MIBiG BGCs.\n\n    Returns:\n        True if `GCF.bgc_ids` are only MIBiG BGC ids.\n    \"\"\"\n    return all(map(lambda id: id.startswith(\"BGC\"), self.bgc_ids))\n</code></pre>"},{"location":"api/genomics/#nplinker.genomics.GCF.is_singleton","title":"is_singleton","text":"<pre><code>is_singleton() -&gt; bool\n</code></pre> <p>Check if the GCF contains only one BGC.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>GCF.bgc_ids</code> contains only one BGC id.</p> Source code in <code>src/nplinker/genomics/gcf.py</code> <pre><code>def is_singleton(self) -&gt; bool:\n    \"\"\"Check if the GCF contains only one BGC.\n\n    Returns:\n        True if `GCF.bgc_ids` contains only one BGC id.\n    \"\"\"\n    return len(self.bgc_ids) == 1\n</code></pre>"},{"location":"api/genomics_abc/","title":"Base Classes","text":""},{"location":"api/genomics_abc/#nplinker.genomics.abc","title":"abc","text":""},{"location":"api/genomics_abc/#nplinker.genomics.abc.BGCLoaderBase","title":"BGCLoaderBase","text":"<pre><code>BGCLoaderBase(data_dir: str)\n</code></pre> <p>             Bases: <code>ABC</code></p> <p>Abstract base class for BGC loader.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to directory that contains BGC metadata files (.json) or full data genbank files (.gbk).</p> required Source code in <code>src/nplinker/genomics/abc.py</code> <pre><code>def __init__(self, data_dir: str):\n    \"\"\"Abstract base class for BGC loader.\n\n    Args:\n        data_dir: Path to directory that contains BGC metadata files\n            (.json) or full data genbank files (.gbk).\n    \"\"\"\n    self.data_dir = data_dir\n</code></pre>"},{"location":"api/genomics_abc/#nplinker.genomics.abc.BGCLoaderBase.get_files","title":"get_files  <code>abstractmethod</code>","text":"<pre><code>get_files() -&gt; dict[str, str]\n</code></pre> <p>Get path to BGC files.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The key is BGC name and value is path to BGC file</p> Source code in <code>src/nplinker/genomics/abc.py</code> <pre><code>@abstractmethod\ndef get_files(self) -&gt; dict[str, str]:\n    \"\"\"Get path to BGC files.\n\n    Returns:\n        The key is BGC name and value is path to BGC file\n    \"\"\"\n</code></pre>"},{"location":"api/genomics_abc/#nplinker.genomics.abc.BGCLoaderBase.get_bgcs","title":"get_bgcs  <code>abstractmethod</code>","text":"<pre><code>get_bgcs() -&gt; Sequence[BGC]\n</code></pre> <p>Get BGC objects.</p> <p>Returns:</p> Type Description <code>Sequence[BGC]</code> <p>A list of BGC objects</p> Source code in <code>src/nplinker/genomics/abc.py</code> <pre><code>@abstractmethod\ndef get_bgcs(self) -&gt; Sequence[BGC]:\n    \"\"\"Get BGC objects.\n\n    Returns:\n        A list of BGC objects\n    \"\"\"\n</code></pre>"},{"location":"api/genomics_abc/#nplinker.genomics.abc.GCFLoaderBase","title":"GCFLoaderBase","text":"<p>             Bases: <code>ABC</code></p>"},{"location":"api/genomics_abc/#nplinker.genomics.abc.GCFLoaderBase.get_gcfs","title":"get_gcfs  <code>abstractmethod</code>","text":"<pre><code>get_gcfs(keep_mibig_only: bool, keep_singleton: bool) -&gt; Sequence[GCF]\n</code></pre> <p>Get GCF objects.</p> <p>Parameters:</p> Name Type Description Default <code>keep_mibig_only</code> <code>bool</code> <p>True to keep GCFs that contain only MIBiG BGCs.</p> required <code>keep_singleton</code> <code>bool</code> <p>True to keep singleton GCFs. A singleton GCF is a GCF that contains only one BGC.</p> required <p>Returns:</p> Type Description <code>Sequence[GCF]</code> <p>A list of GCF objects</p> Source code in <code>src/nplinker/genomics/abc.py</code> <pre><code>@abstractmethod\ndef get_gcfs(self, keep_mibig_only: bool, keep_singleton: bool) -&gt; Sequence[GCF]:\n    \"\"\"Get GCF objects.\n\n    Args:\n        keep_mibig_only: True to keep GCFs that contain only MIBiG\n            BGCs.\n        keep_singleton: True to keep singleton GCFs. A singleton GCF\n            is a GCF that contains only one BGC.\n\n    Returns:\n        A list of GCF objects\n    \"\"\"\n</code></pre>"},{"location":"api/genomics_utils/","title":"Utilities","text":""},{"location":"api/genomics_utils/#nplinker.genomics.utils","title":"utils","text":""},{"location":"api/genomics_utils/#nplinker.genomics.utils.generate_mappings_genome_id_bgc_id","title":"generate_mappings_genome_id_bgc_id","text":"<pre><code>generate_mappings_genome_id_bgc_id(bgc_dir: str | PathLike, output_file: str | PathLike | None = None) -&gt; None\n</code></pre> <p>Generate a file that maps genome id to BGC id.</p> <p>Note that the <code>output_file</code> will be overwritten if it already exists.</p> <p>Parameters:</p> Name Type Description Default <code>bgc_dir</code> <code>str | PathLike</code> <p>The directory has one-layer of subfolders and each subfolder contains BGC files in <code>.gbk</code> format. It assumes that - the subfolder name is the genome id (e.g. refseq), - the BGC file name is the BGC id.</p> required <code>output_file</code> <code>str | PathLike | None</code> <p>The path to the output file. Note that the file will be overwritten if it already exists. Defaults to None, in which case the output file will be placed in the directory <code>bgc_dir</code> with a file name defined in global variable <code>GENOME_BGC_MAPPINGS_FILENAME</code>.</p> <code>None</code> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def generate_mappings_genome_id_bgc_id(\n    bgc_dir: str | PathLike, output_file: str | PathLike | None = None\n) -&gt; None:\n    \"\"\"Generate a file that maps genome id to BGC id.\n\n    Note that the `output_file` will be overwritten if it already exists.\n\n    Args:\n        bgc_dir: The directory has one-layer of subfolders and\n            each subfolder contains BGC files in `.gbk` format.\n            It assumes that\n            - the subfolder name is the genome id (e.g. refseq),\n            - the BGC file name is the BGC id.\n        output_file: The path to the output file. Note\n            that the file will be overwritten if it already exists.\n            Defaults to None, in which case the output file will be placed in\n            the directory `bgc_dir` with a file name defined in global variable\n            `GENOME_BGC_MAPPINGS_FILENAME`.\n    \"\"\"\n    bgc_dir = Path(bgc_dir)\n    genome_bgc_mappings = {}\n\n    for subdir in list_dirs(bgc_dir):\n        genome_id = Path(subdir).name\n        bgc_files = list_files(subdir, suffix=(\".gbk\"), keep_parent=False)\n        bgc_ids = [bgc_id for f in bgc_files if (bgc_id := Path(f).stem) != genome_id]\n        if bgc_ids:\n            genome_bgc_mappings[genome_id] = bgc_ids\n        else:\n            logger.warning(\"No BGC files found in %s\", subdir)\n\n    # sort mappings by genome_id and construct json data\n    genome_bgc_mappings = dict(sorted(genome_bgc_mappings.items()))\n    json_data = [{\"genome_ID\": k, \"BGC_ID\": v} for k, v in genome_bgc_mappings.items()]\n    json_data = {\"mappings\": json_data, \"version\": \"1.0\"}\n\n    # validate json data\n    validate(instance=json_data, schema=GENOME_BGC_MAPPINGS_SCHEMA)\n\n    if output_file is None:\n        output_file = bgc_dir / GENOME_BGC_MAPPINGS_FILENAME\n    with open(output_file, \"w\") as f:\n        json.dump(json_data, f)\n    logger.info(\"Generated genome-BGC mappings file: %s\", output_file)\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.add_strain_to_bgc","title":"add_strain_to_bgc","text":"<pre><code>add_strain_to_bgc(strains: StrainCollection, bgcs: list[BGC]) -&gt; tuple[list[BGC], list[BGC]]\n</code></pre> <p>Assign a Strain object to <code>BGC.strain</code> for input BGCs.</p> <p>BGC id is used to find the corresponding Strain object. It's possible that no Strain object is found for a BGC id.</p> <p>Note that the input list <code>bgcs</code> will be changed in place.</p> <p>Parameters:</p> Name Type Description Default <code>strains</code> <code>StrainCollection</code> <p>A collection of all strain objects.</p> required <code>bgcs</code> <code>list[BGC]</code> <p>A list of BGC objects.</p> required <p>Returns:</p> Type Description <code>tuple[list[BGC], list[BGC]]</code> <p>A tuple of two lists of BGC objects. The first list contains BGC objects that are updated with Strain object; the second list contains BGC objects that are not updated with Strain object because no Strain object is found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Multiple strain objects found for a BGC id.</p> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def add_strain_to_bgc(strains: StrainCollection, bgcs: list[BGC]) -&gt; tuple[list[BGC], list[BGC]]:\n    \"\"\"Assign a Strain object to `BGC.strain` for input BGCs.\n\n    BGC id is used to find the corresponding Strain object. It's possible that\n    no Strain object is found for a BGC id.\n\n    Note that the input list `bgcs` will be changed in place.\n\n    Args:\n        strains: A collection of all strain objects.\n        bgcs: A list of BGC objects.\n\n    Returns:\n        A tuple of two lists of BGC objects. The\n            first list contains BGC objects that are updated with Strain object;\n            the second list contains BGC objects that are not updated with\n            Strain object because no Strain object is found.\n\n    Raises:\n        ValueError: Multiple strain objects found for a BGC id.\n    \"\"\"\n    bgc_with_strain = []\n    bgc_without_strain = []\n    for bgc in bgcs:\n        try:\n            strain_list = strains.lookup(bgc.bgc_id)\n        except ValueError:\n            bgc_without_strain.append(bgc)\n            continue\n        if len(strain_list) &gt; 1:\n            raise ValueError(\n                f\"Multiple strain objects found for BGC id '{bgc.bgc_id}'.\"\n                f\"BGC object accept only one strain.\"\n            )\n        bgc.strain = strain_list[0]\n        bgc_with_strain.append(bgc)\n\n    logger.info(\n        f\"{len(bgc_with_strain)} BGC objects updated with Strain object.\\n\"\n        f\"{len(bgc_without_strain)} BGC objects not updated with Strain object.\"\n    )\n    return bgc_with_strain, bgc_without_strain\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.add_bgc_to_gcf","title":"add_bgc_to_gcf","text":"<pre><code>add_bgc_to_gcf(bgcs: list[BGC], gcfs: list[GCF]) -&gt; tuple[list[GCF], list[GCF], dict[GCF, set[str]]]\n</code></pre> <p>Add BGC objects to GCF object based on GCF's BGC ids.</p> <p>The attribute of <code>GCF.bgc_ids</code> contains the ids of BGC objects. These ids are used to find BGC objects from the input <code>bgcs</code> list. The found BGC objects are added to the <code>bgcs</code> attribute of GCF object. It is possible that some BGC ids are not found in the input <code>bgcs</code> list, and so their BGC objects are missing in the GCF object.</p> <p>This method changes the lists <code>bgcs</code> and <code>gcfs</code> in place.</p> <p>Parameters:</p> Name Type Description Default <code>bgcs</code> <code>list[BGC]</code> <p>A list of BGC objects.</p> required <code>gcfs</code> <code>list[GCF]</code> <p>A list of GCF objects.</p> required <p>Returns:</p> Type Description <code>tuple[list[GCF], list[GCF], dict[GCF, set[str]]]</code> <p>The first list contains GCF objects that are updated with BGC objects; The second list contains GCF objects that are not updated with BGC objects because no BGC objects are found; The dictionary contains GCF objects as keys and a set of ids of missing BGC objects as values.</p> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def add_bgc_to_gcf(\n    bgcs: list[BGC], gcfs: list[GCF]\n) -&gt; tuple[list[GCF], list[GCF], dict[GCF, set[str]]]:\n    \"\"\"Add BGC objects to GCF object based on GCF's BGC ids.\n\n    The attribute of `GCF.bgc_ids` contains the ids of BGC objects. These ids\n    are used to find BGC objects from the input `bgcs` list. The found BGC\n    objects are added to the `bgcs` attribute of GCF object. It is possible that\n    some BGC ids are not found in the input `bgcs` list, and so their BGC\n    objects are missing in the GCF object.\n\n    This method changes the lists `bgcs` and `gcfs` in place.\n\n    Args:\n        bgcs: A list of BGC objects.\n        gcfs: A list of GCF objects.\n\n    Returns:\n        The first list contains GCF objects that are updated with BGC objects;\n            The second list contains GCF objects that are not updated with BGC objects\n            because no BGC objects are found;\n            The dictionary contains GCF objects as keys and a set of ids of missing\n            BGC objects as values.\n    \"\"\"\n    bgc_dict = {bgc.bgc_id: bgc for bgc in bgcs}\n    gcf_with_bgc = []\n    gcf_without_bgc = []\n    gcf_missing_bgc: dict[GCF, set[str]] = {}\n    for gcf in gcfs:\n        for bgc_id in gcf.bgc_ids:\n            try:\n                bgc = bgc_dict[bgc_id]\n            except KeyError:\n                if gcf not in gcf_missing_bgc:\n                    gcf_missing_bgc[gcf] = {bgc_id}\n                else:\n                    gcf_missing_bgc[gcf].add(bgc_id)\n                continue\n            gcf.add_bgc(bgc)\n\n        if gcf.bgcs:\n            gcf_with_bgc.append(gcf)\n        else:\n            gcf_without_bgc.append(gcf)\n\n    logger.info(\n        f\"{len(gcf_with_bgc)} GCF objects updated with BGC objects.\\n\"\n        f\"{len(gcf_without_bgc)} GCF objects not updated with BGC objects.\\n\"\n        f\"{len(gcf_missing_bgc)} GCF objects have missing BGC objects.\"\n    )\n    return gcf_with_bgc, gcf_without_bgc, gcf_missing_bgc\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.get_mibig_from_gcf","title":"get_mibig_from_gcf","text":"<pre><code>get_mibig_from_gcf(gcfs: list[GCF]) -&gt; tuple[list[BGC], StrainCollection]\n</code></pre> <p>Get MIBiG BGCs and strains from GCF objects.</p> <p>Parameters:</p> Name Type Description Default <code>gcfs</code> <code>list[GCF]</code> <p>A list of GCF objects.</p> required <p>Returns:</p> Type Description <code>tuple[list[BGC], StrainCollection]</code> <p>tuple[list[BGC], StrainCollection]: The first is a list of MIBiG BGC objects used in the GCFs; the second is a StrainCollection object that contains all Strain objects used in the GCFs.</p> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def get_mibig_from_gcf(gcfs: list[GCF]) -&gt; tuple[list[BGC], StrainCollection]:\n    \"\"\"Get MIBiG BGCs and strains from GCF objects.\n\n    Args:\n        gcfs: A list of GCF objects.\n\n    Returns:\n        tuple[list[BGC], StrainCollection]: The first is a list of MIBiG BGC\n            objects used in the GCFs; the second is a StrainCollection object\n            that contains all Strain objects used in the GCFs.\n    \"\"\"\n    mibig_bgcs_in_use = []\n    mibig_strains_in_use = StrainCollection()\n    for gcf in gcfs:\n        for bgc in gcf.bgcs:\n            if bgc.is_mibig():\n                mibig_bgcs_in_use.append(bgc)\n                if bgc.strain is not None:\n                    mibig_strains_in_use.add(bgc.strain)\n    return mibig_bgcs_in_use, mibig_strains_in_use\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.extract_mappings_strain_id_original_genome_id","title":"extract_mappings_strain_id_original_genome_id","text":"<pre><code>extract_mappings_strain_id_original_genome_id(podp_project_json_file: str | PathLike) -&gt; dict[str, set[str]]\n</code></pre> <p>Extract mappings \"strain id &lt;-&gt; original genome id\".</p> <p>Parameters:</p> Name Type Description Default <code>podp_project_json_file</code> <code>str | PathLike</code> <p>The path to the PODP project JSON file.</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Key is strain id and value is a set of original genome ids.</p> Notes <p>The <code>podp_project_json_file</code> is the project JSON file downloaded from PODP platform. For example, for project MSV000079284, its json file is https://pairedomicsdata.bioinformatics.nl/api/projects/4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4.</p> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def extract_mappings_strain_id_original_genome_id(\n    podp_project_json_file: str | PathLike\n) -&gt; dict[str, set[str]]:\n    \"\"\"Extract mappings \"strain id &lt;-&gt; original genome id\".\n\n    Args:\n        podp_project_json_file: The path to the PODP project\n            JSON file.\n\n    Returns:\n        Key is strain id and value is a set of original genome ids.\n\n    Notes:\n        The `podp_project_json_file` is the project JSON file downloaded from\n        PODP platform. For example, for project MSV000079284, its json file is\n        https://pairedomicsdata.bioinformatics.nl/api/projects/4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4.\n    \"\"\"\n    mappings_dict = {}\n    with open(podp_project_json_file, \"r\") as f:\n        json_data = json.load(f)\n\n    validate_podp_json(json_data)\n\n    for record in json_data[\"genomes\"]:\n        strain_id = record[\"genome_label\"]\n        genome_id = get_best_available_genome_id(record[\"genome_ID\"])\n        if genome_id is None:\n            logger.warning(\"Failed to extract genome ID from genome with label %s\", strain_id)\n            continue\n        if strain_id in mappings_dict:\n            mappings_dict[strain_id].add(genome_id)\n        else:\n            mappings_dict[strain_id] = {genome_id}\n    return mappings_dict\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.extract_mappings_original_genome_id_resolved_genome_id","title":"extract_mappings_original_genome_id_resolved_genome_id","text":"<pre><code>extract_mappings_original_genome_id_resolved_genome_id(genome_status_json_file: str | PathLike) -&gt; dict[str, str]\n</code></pre> <p>Extract mappings \"original_genome_id &lt;-&gt; resolved_genome_id\".</p> <p>Parameters:</p> Name Type Description Default <code>genome_status_json_file</code> <code>str | PathLike</code> <p>The path to the genome status JSON file.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Key is original genome id and value is resolved genome id.</p> Notes <p>The <code>genome_status_json_file</code> is usually generated by the <code>podp_download_and_extract_antismash_data</code> function with a default file name defined in <code>nplinker.globals.GENOME_STATUS_FILENAME</code>.</p> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def extract_mappings_original_genome_id_resolved_genome_id(\n    genome_status_json_file: str | PathLike\n) -&gt; dict[str, str]:\n    \"\"\"Extract mappings \"original_genome_id &lt;-&gt; resolved_genome_id\".\n\n    Args:\n        genome_status_json_file: The path to the genome status\n            JSON file.\n\n    Returns:\n        Key is original genome id and value is resolved genome id.\n\n    Notes:\n        The `genome_status_json_file` is usually generated by the\n        `podp_download_and_extract_antismash_data` function with\n        a default file name defined in `nplinker.globals.GENOME_STATUS_FILENAME`.\n    \"\"\"\n    gs_mappings_dict = GenomeStatus.read_json(genome_status_json_file)\n    return {gs.original_id: gs.resolved_refseq_id for gs in gs_mappings_dict.values()}\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.extract_mappings_resolved_genome_id_bgc_id","title":"extract_mappings_resolved_genome_id_bgc_id","text":"<pre><code>extract_mappings_resolved_genome_id_bgc_id(genome_bgc_mappings_file: str | PathLike) -&gt; dict[str, set[str]]\n</code></pre> <p>Extract mappings \"resolved_genome_id &lt;-&gt; bgc_id\".</p> <p>Parameters:</p> Name Type Description Default <code>genome_bgc_mappings_file</code> <code>str | PathLike</code> <p>The path to the genome BGC mappings JSON file.</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Key is resolved genome id and value is a set of BGC ids.</p> Notes <p>The <code>genome_bgc_mappings_file</code> is usually generated by the <code>generate_mappings_genome_id_bgc_id</code> function with a default file name defined in <code>nplinker.globals.GENOME_BGC_MAPPINGS_FILENAME</code>.</p> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def extract_mappings_resolved_genome_id_bgc_id(\n    genome_bgc_mappings_file: str | PathLike\n) -&gt; dict[str, set[str]]:\n    \"\"\"Extract mappings \"resolved_genome_id &lt;-&gt; bgc_id\".\n\n    Args:\n        genome_bgc_mappings_file: The path to the genome BGC\n            mappings JSON file.\n\n    Returns:\n        Key is resolved genome id and value is a set of BGC ids.\n\n    Notes:\n        The `genome_bgc_mappings_file` is usually generated by the\n        `generate_mappings_genome_id_bgc_id` function with a default file name\n        defined in `nplinker.globals.GENOME_BGC_MAPPINGS_FILENAME`.\n    \"\"\"\n    with open(genome_bgc_mappings_file, \"r\") as f:\n        json_data = json.load(f)\n\n    # validate the JSON data\n    validate(json_data, GENOME_BGC_MAPPINGS_SCHEMA)\n\n    return {mapping[\"genome_ID\"]: set(mapping[\"BGC_ID\"]) for mapping in json_data[\"mappings\"]}\n</code></pre>"},{"location":"api/genomics_utils/#nplinker.genomics.utils.get_mappings_strain_id_bgc_id","title":"get_mappings_strain_id_bgc_id","text":"<pre><code>get_mappings_strain_id_bgc_id(mappings_strain_id_original_genome_id: dict[str, set[str]], mappings_original_genome_id_resolved_genome_id: dict[str, str], mappings_resolved_genome_id_bgc_id: dict[str, set[str]]) -&gt; dict[str, set[str]]\n</code></pre> <p>Get mappings \"strain_id &lt;-&gt; bgc_id\".</p> <p>Parameters:</p> Name Type Description Default <code>mappings_strain_id_original_genome_id</code> <code>dict[str, set[str]]</code> <p>Mappings \"strain_id &lt;-&gt; original_genome_id\".</p> required <code>mappings_original_genome_id_resolved_genome_id</code> <code>dict[str, str]</code> <p>Mappings \"original_genome_id &lt;-&gt; resolved_genome_id\".</p> required <code>mappings_resolved_genome_id_bgc_id</code> <code>dict[str, set[str]]</code> <p>Mappings \"resolved_genome_id &lt;-&gt; bgc_id\".</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Key is strain id and value is a set of BGC ids.</p> See Also <ul> <li><code>extract_mappings_strain_id_original_genome_id</code>: Extract mappings     \"strain_id &lt;-&gt; original_genome_id\".</li> <li><code>extract_mappings_original_genome_id_resolved_genome_id</code>: Extract mappings     \"original_genome_id &lt;-&gt; resolved_genome_id\".</li> <li><code>extract_mappings_resolved_genome_id_bgc_id</code>: Extract mappings     \"resolved_genome_id &lt;-&gt; bgc_id\".</li> </ul> Source code in <code>src/nplinker/genomics/utils.py</code> <pre><code>def get_mappings_strain_id_bgc_id(\n    mappings_strain_id_original_genome_id: dict[str, set[str]],\n    mappings_original_genome_id_resolved_genome_id: dict[str, str],\n    mappings_resolved_genome_id_bgc_id: dict[str, set[str]],\n) -&gt; dict[str, set[str]]:\n    \"\"\"Get mappings \"strain_id &lt;-&gt; bgc_id\".\n\n    Args:\n        mappings_strain_id_original_genome_id: Mappings\n            \"strain_id &lt;-&gt; original_genome_id\".\n        mappings_original_genome_id_resolved_genome_id: Mappings\n            \"original_genome_id &lt;-&gt; resolved_genome_id\".\n        mappings_resolved_genome_id_bgc_id: Mappings\n            \"resolved_genome_id &lt;-&gt; bgc_id\".\n\n    Returns:\n        Key is strain id and value is a set of BGC ids.\n\n    See Also:\n        - `extract_mappings_strain_id_original_genome_id`: Extract mappings\n            \"strain_id &lt;-&gt; original_genome_id\".\n        - `extract_mappings_original_genome_id_resolved_genome_id`: Extract mappings\n            \"original_genome_id &lt;-&gt; resolved_genome_id\".\n        - `extract_mappings_resolved_genome_id_bgc_id`: Extract mappings\n            \"resolved_genome_id &lt;-&gt; bgc_id\".\n    \"\"\"\n    mappings_dict = {}\n    for strain_id, original_genome_ids in mappings_strain_id_original_genome_id.items():\n        bgc_ids = set()\n        for original_genome_id in original_genome_ids:\n            resolved_genome_id = mappings_original_genome_id_resolved_genome_id[original_genome_id]\n            if (bgc_id := mappings_resolved_genome_id_bgc_id.get(resolved_genome_id)) is not None:\n                bgc_ids.update(bgc_id)\n        if bgc_ids:\n            mappings_dict[strain_id] = bgc_ids\n    return mappings_dict\n</code></pre>"},{"location":"api/gnps/","title":"GNPS","text":""},{"location":"api/gnps/#nplinker.metabolomics.gnps","title":"gnps","text":""},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSFormat","title":"GNPSFormat","text":"<p>             Bases: <code>Enum</code></p> <p>Enum class for GNPS format (workflow).</p> <p>The GNPS format refers to the GNPS workflow. The name of the enum is a simple short name for the workflow, and the value of the enum is the actual name of the workflow in the GNPS website.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSDownloader","title":"GNPSDownloader","text":"<pre><code>GNPSDownloader(task_id: str, download_root: str | PathLike)\n</code></pre> <p>Download GNPS zip archive for the given task id.</p> <p>Note that only GNPS workflows listed in the GNPSFormat enum are supported.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>GNPS task id, identifying the data to be downloaded.</p> required <code>download_root</code> <code>str | PathLike</code> <p>Path where to store the downloaded archive.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given task id does not correspond to a supported GNPS workflow.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; GNPSDownloader(\"c22f44b14a3d450eb836d607cb9521bb\", \"~/downloads\")\n</code></pre> Source code in <code>src/nplinker/metabolomics/gnps/gnps_downloader.py</code> <pre><code>def __init__(self, task_id: str, download_root: str | PathLike):\n    \"\"\"Download GNPS zip archive for the given task id.\n\n    Note that only GNPS workflows listed in the GNPSFormat enum are supported.\n\n    Args:\n        task_id: GNPS task id, identifying the data to be downloaded.\n        download_root: Path where to store the downloaded archive.\n\n    Raises:\n        ValueError: If the given task id does not correspond to a supported\n            GNPS workflow.\n\n    Examples:\n        &gt;&gt;&gt; GNPSDownloader(\"c22f44b14a3d450eb836d607cb9521bb\", \"~/downloads\")\n    \"\"\"\n    gnps_format = gnps_format_from_task_id(task_id)\n    if gnps_format == GNPSFormat.Unknown:\n        raise ValueError(\n            f\"Unknown workflow type for GNPS task '{task_id}'.\"\n            f\"Supported GNPS workflows are described in the GNPSFormat enum, \"\n            f\"including such as 'METABOLOMICS-SNETS', 'METABOLOMICS-SNETS-V2' \"\n            f\"and 'FEATURE-BASED-MOLECULAR-NETWORKING'.\"\n        )\n\n    self._task_id = task_id\n    self._download_root: Path = Path(download_root)\n    self._gnps_format = gnps_format\n    self._file_name = gnps_format.value + \"-\" + self._task_id + \".zip\"\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSDownloader.gnps_format","title":"gnps_format  <code>property</code>","text":"<pre><code>gnps_format: GNPSFormat\n</code></pre> <p>Get the GNPS workflow type.</p> <p>Returns:</p> Type Description <code>GNPSFormat</code> <p>GNPS workflow type.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSDownloader.download","title":"download","text":"<pre><code>download() -&gt; 'Self'\n</code></pre> <p>Execute the downloading process.</p> <p>Note: GNPS data is downloaded using the POST method (empty payload is OK).</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_downloader.py</code> <pre><code>def download(self) -&gt; \"Self\":\n    \"\"\"Execute the downloading process.\n\n    Note: GNPS data is downloaded using the POST method (empty payload is OK).\n    \"\"\"\n    download_url(\n        self.get_url(), self._download_root, filename=self._file_name, http_method=\"POST\"\n    )\n    return self\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSDownloader.get_download_file","title":"get_download_file","text":"<pre><code>get_download_file() -&gt; str\n</code></pre> <p>Get the path to the zip file.</p> <p>Returns:</p> Type Description <code>str</code> <p>Download path as string</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_downloader.py</code> <pre><code>def get_download_file(self) -&gt; str:\n    \"\"\"Get the path to the zip file.\n\n    Returns:\n        Download path as string\n    \"\"\"\n    return str(Path(self._download_root) / self._file_name)\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSDownloader.get_task_id","title":"get_task_id","text":"<pre><code>get_task_id() -&gt; str\n</code></pre> <p>Get the GNPS task id.</p> <p>Returns:</p> Type Description <code>str</code> <p>Task id as string.</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_downloader.py</code> <pre><code>def get_task_id(self) -&gt; str:\n    \"\"\"Get the GNPS task id.\n\n    Returns:\n        Task id as string.\n    \"\"\"\n    return self._task_id\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSDownloader.get_url","title":"get_url","text":"<pre><code>get_url() -&gt; str\n</code></pre> <p>Get the full URL linking to GNPS data to be dowloaded.</p> <p>Returns:</p> Type Description <code>str</code> <p>URL pointing to the GNPS data to be downloaded.</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_downloader.py</code> <pre><code>def get_url(self) -&gt; str:\n    \"\"\"Get the full URL linking to GNPS data to be dowloaded.\n\n    Returns:\n        URL pointing to the GNPS data to be downloaded.\n    \"\"\"\n    if self.gnps_format == GNPSFormat.FBMN:\n        return GNPSDownloader.GNPS_DATA_DOWNLOAD_URL_FBMN.format(self._task_id)\n    return GNPSDownloader.GNPS_DATA_DOWNLOAD_URL.format(self._task_id)\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSExtractor","title":"GNPSExtractor","text":"<pre><code>GNPSExtractor(file: str | PathLike, extract_dir: str | PathLike)\n</code></pre> <p>Class to extract files from a GNPS molecular networking archive(.zip).</p> <p>Four files are extracted and renamed to the following names:</p> <ul> <li>file_mappings(.tsv/.csv)</li> <li>spectra.mgf</li> <li>molecular_families.tsv</li> <li>annotations.tsv</li> </ul> <p>The files to be extracted are selected based on the GNPS workflow type, as desribed below (in the order of the files above):</p> <ol> <li>METABOLOMICS-SNETS<ul> <li>clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.tsv</li> <li>METABOLOMICS-SNETS*.mgf</li> <li>networkedges_selfloop/*.pairsinfo</li> <li>result_specnets_DB/*.tsv</li> </ul> </li> <li>METABOLOMICS-SNETS-V2<ul> <li>clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.clustersummary</li> <li>METABOLOMICS-SNETS-V2*.mgf</li> <li>networkedges_selfloop/*.selfloop</li> <li>result_specnets_DB/.tsv</li> </ul> </li> <li>FEATURE-BASED-MOLECULAR-NETWORKING<ul> <li>quantification_table/.csv</li> <li>spectra/*.mgf</li> <li>networkedges_selfloop/*.selfloop</li> <li>DB_result/*.tsv</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>The path to the GNPS zip file.</p> required <code>extract_dir</code> <code>str | PathLike</code> <p>path to the directory where to extract the files to.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given file is an invalid GNPS archive.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gnps_extractor = GNPSExtractor(\"path/to/gnps_archive.zip\", \"path/to/extract_dir\")\n&gt;&gt;&gt; gnps_extractor.gnps_format\n&lt;GNPSFormat.SNETS: 'METABOLOMICS-SNETS'&gt;\n&gt;&gt;&gt; gnps_extractor.extract_dir\n'path/to/extract_dir'\n</code></pre> Source code in <code>src/nplinker/metabolomics/gnps/gnps_extractor.py</code> <pre><code>def __init__(self, file: str | PathLike, extract_dir: str | PathLike):\n    \"\"\"Class to extract files from a GNPS molecular networking archive(.zip).\n\n    Four files are extracted and renamed to the following names:\n\n    - file_mappings(.tsv/.csv)\n    - spectra.mgf\n    - molecular_families.tsv\n    - annotations.tsv\n\n    The files to be extracted are selected based on the GNPS workflow type,\n    as desribed below (in the order of the files above):\n\n    1. METABOLOMICS-SNETS\n        - clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.tsv\n        - METABOLOMICS-SNETS*.mgf\n        - networkedges_selfloop/*.pairsinfo\n        - result_specnets_DB/*.tsv\n    2. METABOLOMICS-SNETS-V2\n        - clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.clustersummary\n        - METABOLOMICS-SNETS-V2*.mgf\n        - networkedges_selfloop/*.selfloop\n        - result_specnets_DB/.tsv\n    3. FEATURE-BASED-MOLECULAR-NETWORKING\n        - quantification_table*/*.csv\n        - spectra/*.mgf\n        - networkedges_selfloop/*.selfloop\n        - DB_result/*.tsv\n\n    Args:\n        file: The path to the GNPS zip file.\n        extract_dir: path to the directory where to extract the files to.\n\n    Raises:\n        ValueError: If the given file is an invalid GNPS archive.\n\n    Examples:\n        &gt;&gt;&gt; gnps_extractor = GNPSExtractor(\"path/to/gnps_archive.zip\", \"path/to/extract_dir\")\n        &gt;&gt;&gt; gnps_extractor.gnps_format\n        &lt;GNPSFormat.SNETS: 'METABOLOMICS-SNETS'&gt;\n        &gt;&gt;&gt; gnps_extractor.extract_dir\n        'path/to/extract_dir'\n    \"\"\"\n    gnps_format = gnps_format_from_archive(file)\n    if gnps_format == GNPSFormat.Unknown:\n        raise ValueError(\n            f\"Unknown workflow type for GNPS archive '{file}'.\"\n            f\"Supported GNPS workflows are described in the GNPSFormat enum, \"\n            f\"including such as 'METABOLOMICS-SNETS', 'METABOLOMICS-SNETS-V2' \"\n            f\"and 'FEATURE-BASED-MOLECULAR-NETWORKING'.\"\n        )\n\n    self._file = Path(file)\n    self._extract_path = Path(extract_dir)\n    self._gnps_format = gnps_format\n    # the order of filenames matters\n    self._target_files = [\n        \"file_mappings\",\n        \"spectra.mgf\",\n        \"molecular_families.tsv\",\n        \"annotations.tsv\",\n    ]\n\n    self._extract()\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSExtractor.gnps_format","title":"gnps_format  <code>property</code>","text":"<pre><code>gnps_format: GNPSFormat\n</code></pre> <p>Get the GNPS workflow type.</p> <p>Returns:</p> Type Description <code>GNPSFormat</code> <p>GNPS workflow type.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSExtractor.extract_dir","title":"extract_dir  <code>property</code>","text":"<pre><code>extract_dir: str\n</code></pre> <p>Get the path where to extract the files to.</p> <p>Returns:</p> Type Description <code>str</code> <p>Path where to extract files as string.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSSpectrumLoader","title":"GNPSSpectrumLoader","text":"<pre><code>GNPSSpectrumLoader(file: str | PathLike)\n</code></pre> <p>             Bases: <code>SpectrumLoaderBase</code></p> <p>Class to load mass spectra from the given GNPS MGF file.</p> <p>The file mappings file is from GNPS output archive, as described below for each GNPS workflow type:</p> <ol> <li>METABOLOMICS-SNETS<ul> <li>METABOLOMICS-SNETS*.mgf</li> </ul> </li> <li>METABOLOMICS-SNETS-V2<ul> <li>METABOLOMICS-SNETS-V2*.mgf</li> </ul> </li> <li>FEATURE-BASED-MOLECULAR-NETWORKING<ul> <li>spectra/*.mgf</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>path to the MGF file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises ValueError if the file is not valid.</p> Example <p>loader = GNPSSpectrumLoader(\"gnps_spectra.mgf\") print(loader.spectra[0])</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_spectrum_loader.py</code> <pre><code>def __init__(self, file: str | PathLike):\n    \"\"\"Class to load mass spectra from the given GNPS MGF file.\n\n    The file mappings file is from GNPS output archive, as described below\n    for each GNPS workflow type:\n\n    1. METABOLOMICS-SNETS\n        - METABOLOMICS-SNETS*.mgf\n    2. METABOLOMICS-SNETS-V2\n        - METABOLOMICS-SNETS-V2*.mgf\n    3. FEATURE-BASED-MOLECULAR-NETWORKING\n        - spectra/*.mgf\n\n    Args:\n        file: path to the MGF file.\n\n    Raises:\n        ValueError: Raises ValueError if the file is not valid.\n\n    Example:\n        &gt;&gt;&gt; loader = GNPSSpectrumLoader(\"gnps_spectra.mgf\")\n        &gt;&gt;&gt; print(loader.spectra[0])\n    \"\"\"\n    self._file = str(file)\n    self._spectra: list[Spectrum] = []\n\n    self._validate()\n    self._load()\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSSpectrumLoader.spectra","title":"spectra  <code>property</code>","text":"<pre><code>spectra: list[Spectrum]\n</code></pre> <p>Get the list of Spectrum objects.</p> <p>Returns:</p> Type Description <code>list[Spectrum]</code> <p>list[Spectrum]: the loaded spectra as a list of <code>Spectrum</code> objects.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSMolecularFamilyLoader","title":"GNPSMolecularFamilyLoader","text":"<pre><code>GNPSMolecularFamilyLoader(file: str | PathLike)\n</code></pre> <p>             Bases: <code>MolecularFamilyLoaderBase</code></p> <p>Class to load molecular families from GNPS output file.</p> <p>The molecular family file is from GNPS output archive, as described below for each GNPS workflow type: 1. METABOLOMICS-SNETS     - networkedges_selfloop/.pairsinfo 2. METABOLOMICS-SNETS-V2     - networkedges_selfloop/.selfloop 3. FEATURE-BASED-MOLECULAR-NETWORKING     - networkedges_selfloop/*.selfloop</p> <p>The \"ComponentIndex\" column in the GNPS molecular family's file is treated as family id. But for molecular families that have only one member (i.e. spectrum), named singleton molecular families, their files have the same value of \"-1\" in the \"ComponentIndex\" column. To make the family id unique,the spectrum id plus a prefix \"singleton-\" is used as the family id of singleton molecular families.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to the GNPS molecular family file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises ValueError if the file is not valid.</p> Example <p>loader = GNPSMolecularFamilyLoader(\"gnps_molecular_families.tsv\") print(loader.families) [, , ...] print(loader.families[0].spectra_ids) Source code in <code>src/nplinker/metabolomics/gnps/gnps_molecular_family_loader.py</code> <pre><code>def __init__(self, file: str | PathLike):\n    \"\"\"Class to load molecular families from GNPS output file.\n\n    The molecular family file is from GNPS output archive, as described below\n    for each GNPS workflow type:\n    1. METABOLOMICS-SNETS\n        - networkedges_selfloop/*.pairsinfo\n    2. METABOLOMICS-SNETS-V2\n        - networkedges_selfloop/*.selfloop\n    3. FEATURE-BASED-MOLECULAR-NETWORKING\n        - networkedges_selfloop/*.selfloop\n\n    The \"ComponentIndex\" column in the GNPS molecular family's file is treated\n    as family id. But for molecular families that have only one member (i.e. spectrum),\n    named singleton molecular families, their files have the same value of\n    \"-1\" in the \"ComponentIndex\" column. To make the family id unique,the\n    spectrum id plus a prefix \"singleton-\" is used as the family id of\n    singleton molecular families.\n\n    Args:\n        file: Path to the GNPS molecular family file.\n\n    Raises:\n        ValueError: Raises ValueError if the file is not valid.\n\n    Example:\n        &gt;&gt;&gt; loader = GNPSMolecularFamilyLoader(\"gnps_molecular_families.tsv\")\n        &gt;&gt;&gt; print(loader.families)\n        [&lt;MolecularFamily 1&gt;, &lt;MolecularFamily 2&gt;, ...]\n        &gt;&gt;&gt; print(loader.families[0].spectra_ids)\n        {'1', '3', '7', ...}\n    \"\"\"\n    self._mfs: list[MolecularFamily] = []\n    self._file = file\n\n    self._validate()\n    self._load()\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSMolecularFamilyLoader.get_mfs","title":"get_mfs","text":"<pre><code>get_mfs(keep_singleton: bool = False) -&gt; list[MolecularFamily]\n</code></pre> <p>Get MolecularFamily objects.</p> <p>Parameters:</p> Name Type Description Default <code>keep_singleton</code> <code>bool</code> <p>True to keep singleton molecular families. A singleton molecular family is a molecular family that contains only one spectrum.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[MolecularFamily]</code> <p>list[MolecularFamily]: A list of MolecularFamily objects with their spectra ids.</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_molecular_family_loader.py</code> <pre><code>def get_mfs(self, keep_singleton: bool = False) -&gt; list[MolecularFamily]:\n    \"\"\"Get MolecularFamily objects.\n\n    Args:\n        keep_singleton: True to keep singleton molecular families. A\n            singleton molecular family is a molecular family that contains\n            only one spectrum.\n\n    Returns:\n        list[MolecularFamily]: A list of MolecularFamily objects with their\n            spectra ids.\n    \"\"\"\n    mfs = self._mfs\n    if not keep_singleton:\n        mfs = [mf for mf in mfs if not mf.is_singleton()]\n    return mfs\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSAnnotationLoader","title":"GNPSAnnotationLoader","text":"<pre><code>GNPSAnnotationLoader(file: str | PathLike)\n</code></pre> <p>             Bases: <code>AnnotationLoaderBase</code></p> <p>Load annotations from GNPS output file.</p> <p>The annotation file is a .tsv file from GNPS output archive, as described below for each GNPS workflow type: 1. METABOLOMICS-SNETS     - result_specnets_DB/.tsv 2. METABOLOMICS-SNETS-V2     - result_specnets_DB/.tsv 3. FEATURE-BASED-MOLECULAR-NETWORKING     - DB_result/.tsv</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>The GNPS annotation file.</p> required Example <p>loader = GNPSAnnotationLoader(\"gnps_annotations.tsv\") print(loader.annotations[\"100\"]) {'#Scan#': '100', 'Adduct': 'M+H', 'CAS_Number': 'N/A', 'Charge': '1', 'Compound_Name': 'MLS002153841-01!Iobenguane sulfate', 'Compound_Source': 'NIH Pharmacologically Active Library', 'Data_Collector': 'VP/LMS', 'ExactMass': '274.992', 'INCHI': 'N/A', 'INCHI_AUX': 'N/A', 'Instrument': 'qTof', 'IonMode': 'Positive', 'Ion_Source': 'LC-ESI', 'LibMZ': '276.003', 'LibraryName': 'lib-00014.mgf', 'LibraryQualityString': 'Gold', 'Library_Class': '1', 'MQScore': '0.704152', 'MZErrorPPM': '405416', 'MassDiff': '111.896', 'Organism': 'GNPS-NIH-SMALLMOLECULEPHARMACOLOGICALLYACTIVE', 'PI': 'Dorrestein', 'Precursor_MZ': '276.003', 'Pubmed_ID': 'N/A', 'RT_Query': '795.979', 'SharedPeaks': '7', 'Smiles': 'NC(=N)NCc1cccc(I)c1.OS(=O)(=O)O', 'SpecCharge': '1', 'SpecMZ': '164.107', 'SpectrumFile': 'spectra/specs_ms.pklbin', 'SpectrumID': 'CCMSLIB00000086167', 'TIC_Query': '986.997', 'UpdateWorkflowName': 'UPDATE-SINGLE-ANNOTATED-GOLD', 'tags': ' ', 'png_url': 'https://metabolomics-usi.gnps2.org/png/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167', 'json_url': 'https://metabolomics-usi.gnps2.org/json/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167', 'svg_url': 'https://metabolomics-usi.gnps2.org/svg/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167', 'spectrum_url': 'https://metabolomics-usi.gnps2.org/spectrum/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167'}</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_annotation_loader.py</code> <pre><code>def __init__(self, file: str | PathLike):\n    \"\"\"Load annotations from GNPS output file.\n\n    The annotation file is a .tsv file from GNPS output archive, as described\n    below for each GNPS workflow type:\n    1. METABOLOMICS-SNETS\n        - result_specnets_DB/*.tsv\n    2. METABOLOMICS-SNETS-V2\n        - result_specnets_DB/.tsv\n    3. FEATURE-BASED-MOLECULAR-NETWORKING\n        - DB_result/*.tsv\n\n    Args:\n        file: The GNPS annotation file.\n\n    Example:\n        &gt;&gt;&gt; loader = GNPSAnnotationLoader(\"gnps_annotations.tsv\")\n        &gt;&gt;&gt; print(loader.annotations[\"100\"])\n        {'#Scan#': '100',\n        'Adduct': 'M+H',\n        'CAS_Number': 'N/A',\n        'Charge': '1',\n        'Compound_Name': 'MLS002153841-01!Iobenguane sulfate',\n        'Compound_Source': 'NIH Pharmacologically Active Library',\n        'Data_Collector': 'VP/LMS',\n        'ExactMass': '274.992',\n        'INCHI': 'N/A',\n        'INCHI_AUX': 'N/A',\n        'Instrument': 'qTof',\n        'IonMode': 'Positive',\n        'Ion_Source': 'LC-ESI',\n        'LibMZ': '276.003',\n        'LibraryName': 'lib-00014.mgf',\n        'LibraryQualityString': 'Gold',\n        'Library_Class': '1',\n        'MQScore': '0.704152',\n        'MZErrorPPM': '405416',\n        'MassDiff': '111.896',\n        'Organism': 'GNPS-NIH-SMALLMOLECULEPHARMACOLOGICALLYACTIVE',\n        'PI': 'Dorrestein',\n        'Precursor_MZ': '276.003',\n        'Pubmed_ID': 'N/A',\n        'RT_Query': '795.979',\n        'SharedPeaks': '7',\n        'Smiles': 'NC(=N)NCc1cccc(I)c1.OS(=O)(=O)O',\n        'SpecCharge': '1',\n        'SpecMZ': '164.107',\n        'SpectrumFile': 'spectra/specs_ms.pklbin',\n        'SpectrumID': 'CCMSLIB00000086167',\n        'TIC_Query': '986.997',\n        'UpdateWorkflowName': 'UPDATE-SINGLE-ANNOTATED-GOLD',\n        'tags': ' ',\n        'png_url': 'https://metabolomics-usi.gnps2.org/png/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167',\n        'json_url': 'https://metabolomics-usi.gnps2.org/json/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167',\n        'svg_url': 'https://metabolomics-usi.gnps2.org/svg/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167',\n        'spectrum_url': 'https://metabolomics-usi.gnps2.org/spectrum/?usi1=mzspec:GNPS:GNPS-LIBRARY:accession:CCMSLIB00000086167'}\n    \"\"\"\n    self._file = Path(file)\n    self._annotations: dict[str, dict] = {}\n\n    self._validate()\n    self._load()\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSAnnotationLoader.annotations","title":"annotations  <code>property</code>","text":"<pre><code>annotations: dict[str, dict]\n</code></pre> <p>Get annotations.</p> <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: Keys are spectrum ids (\"#Scan#\" in annotation file) and values are the annotations dict for each spectrum.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSFileMappingLoader","title":"GNPSFileMappingLoader","text":"<pre><code>GNPSFileMappingLoader(file: str | PathLike)\n</code></pre> <p>             Bases: <code>FileMappingLoaderBase</code></p> <p>Class to load file mappings from GNPS output file.</p> <p>File mappings refers to the mapping from spectrum id to files in which this spectrum occurs.</p> <p>The file mappings file is from GNPS output archive, as described below for each GNPS workflow type:</p> <ol> <li>METABOLOMICS-SNETS<ul> <li>clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.tsv</li> </ul> </li> <li>METABOLOMICS-SNETS-V2<ul> <li>clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.clustersummary</li> </ul> </li> <li>FEATURE-BASED-MOLECULAR-NETWORKING<ul> <li>quantification_table/.csv</li> </ul> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to the GNPS file mappings file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises ValueError if the file is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loader = GNPSFileMappingLoader(\"gnps_file_mappings.tsv\")\n&gt;&gt;&gt; print(loader.mappings[\"1\"])\n['26c.mzXML']\n&gt;&gt;&gt; print(loader.mapping_reversed[\"26c.mzXML\"])\n{'1', '3', '7', ...}\n</code></pre> Source code in <code>src/nplinker/metabolomics/gnps/gnps_file_mapping_loader.py</code> <pre><code>def __init__(self, file: str | PathLike):\n    \"\"\"Class to load file mappings from GNPS output file.\n\n    File mappings refers to the mapping from spectrum id to files in which\n    this spectrum occurs.\n\n    The file mappings file is from GNPS output archive, as described below\n    for each GNPS workflow type:\n\n    1. METABOLOMICS-SNETS\n        - clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.tsv\n    2. METABOLOMICS-SNETS-V2\n        - clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.clustersummary\n    3. FEATURE-BASED-MOLECULAR-NETWORKING\n        - quantification_table*/*.csv\n\n    Args:\n        file: Path to the GNPS file mappings file.\n\n    Raises:\n        ValueError: Raises ValueError if the file is not valid.\n\n    Examples:\n        &gt;&gt;&gt; loader = GNPSFileMappingLoader(\"gnps_file_mappings.tsv\")\n        &gt;&gt;&gt; print(loader.mappings[\"1\"])\n        ['26c.mzXML']\n        &gt;&gt;&gt; print(loader.mapping_reversed[\"26c.mzXML\"])\n        {'1', '3', '7', ...}\n    \"\"\"\n    self._gnps_format = gnps_format_from_file_mapping(file)\n    if self._gnps_format is GNPSFormat.Unknown:\n        raise ValueError(\"Unknown workflow type for GNPS file mappings file \")\n\n    self._file = Path(file)\n    self._mapping: dict[str, list[str]] = {}\n\n    self._validate()\n    self._load()\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSFileMappingLoader.mappings","title":"mappings  <code>property</code>","text":"<pre><code>mappings: dict[str, list[str]]\n</code></pre> <p>Return mapping from spectrum id to files in which this spectrum occurs.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: Mapping from spectrum id to names of all files in which this spectrum occurs.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.GNPSFileMappingLoader.mapping_reversed","title":"mapping_reversed  <code>property</code>","text":"<pre><code>mapping_reversed: dict[str, set[str]]\n</code></pre> <p>Return mapping from file name to all spectra that occur in this file.</p> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>dict[str, set[str]]: Mapping from file name to all spectra ids that occur in this file.</p>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.gnps_format_from_archive","title":"gnps_format_from_archive","text":"<pre><code>gnps_format_from_archive(zip_file: str | PathLike) -&gt; GNPSFormat\n</code></pre> <p>Detect GNPS format from a downloaded GNPS zip archive.</p> <p>The detection is based on the filename of the zip file and the names of the files contained in the zip file.</p> <p>Parameters:</p> Name Type Description Default <code>zip_file</code> <code>str | PathLike</code> <p>Path to the downloaded GNPS zip file.</p> required <p>Returns:</p> Type Description <code>GNPSFormat</code> <p>The format identified in the GNPS zip file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gnps_format_from_archive(\"downloads/ProteoSAFe-METABOLOMICS-SNETS-c22f44b1-download_clustered_spectra.zip\") == GNPSFormat.SNETS\n&gt;&gt;&gt; gnps_format_from_archive(\"downloads/ProteoSAFe-METABOLOMICS-SNETS-V2-189e8bf1-download_clustered_spectra.zip\") == GNPSFormat.SNETSV2\n&gt;&gt;&gt; gnps_format_from_archive(\"downloads/ProteoSAFe-FEATURE-BASED-MOLECULAR-NETWORKING-672d0a53-download_cytoscape_data.zip\") == GNPSFormat.FBMN\n</code></pre> Source code in <code>src/nplinker/metabolomics/gnps/gnps_format.py</code> <pre><code>def gnps_format_from_archive(zip_file: str | PathLike) -&gt; GNPSFormat:\n    \"\"\"Detect GNPS format from a downloaded GNPS zip archive.\n\n    The detection is based on the filename of the zip file and the names of the\n    files contained in the zip file.\n\n    Args:\n        zip_file: Path to the downloaded GNPS zip file.\n\n    Returns:\n        The format identified in the GNPS zip file.\n\n    Examples:\n        &gt;&gt;&gt; gnps_format_from_archive(\"downloads/ProteoSAFe-METABOLOMICS-SNETS-c22f44b1-download_clustered_spectra.zip\") == GNPSFormat.SNETS\n        &gt;&gt;&gt; gnps_format_from_archive(\"downloads/ProteoSAFe-METABOLOMICS-SNETS-V2-189e8bf1-download_clustered_spectra.zip\") == GNPSFormat.SNETSV2\n        &gt;&gt;&gt; gnps_format_from_archive(\"downloads/ProteoSAFe-FEATURE-BASED-MOLECULAR-NETWORKING-672d0a53-download_cytoscape_data.zip\") == GNPSFormat.FBMN\n    \"\"\"\n    file = Path(zip_file)\n    # Guess the format from the filename of the zip file\n    if GNPSFormat.FBMN.value in file.name:\n        return GNPSFormat.FBMN\n    # the order of the if statements matters for the following two\n    if GNPSFormat.SNETSV2.value in file.name:\n        return GNPSFormat.SNETSV2\n    if GNPSFormat.SNETS.value in file.name:\n        return GNPSFormat.SNETS\n\n    # Guess the format from the names of the files in the zip file\n    with zipfile.ZipFile(file) as archive:\n        filenames = archive.namelist()\n    if any(GNPSFormat.FBMN.value in x for x in filenames):\n        return GNPSFormat.FBMN\n    # the order of the if statements matters for the following two\n    if any(GNPSFormat.SNETSV2.value in x for x in filenames):\n        return GNPSFormat.SNETSV2\n    if any(GNPSFormat.SNETS.value in x for x in filenames):\n        return GNPSFormat.SNETS\n\n    return GNPSFormat.Unknown\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.gnps_format_from_file_mapping","title":"gnps_format_from_file_mapping","text":"<pre><code>gnps_format_from_file_mapping(file: str | PathLike) -&gt; GNPSFormat\n</code></pre> <p>Detect GNPS format from the given file mapping file.</p> <p>The GNSP file mapping file is located in different folders depending on the GNPS workflow. Here are the locations in corresponding GNPS zip archives:</p> <ul> <li>METABOLOMICS-SNETS workflow: the .tsv file under folder \"clusterinfosummarygroup_attributes_withIDs_withcomponentID\"</li> <li>METABOLOMICS-SNETS-V2 workflow: the .clustersummary file (tsv) under folder \"clusterinfosummarygroup_attributes_withIDs_withcomponentID\"</li> <li>FEATURE-BASED-MOLECULAR-NETWORKING workflow: the .csv file under folder \"quantification_table\"</li> </ul> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to the file to peek the format for.</p> required <p>Returns:</p> Type Description <code>GNPSFormat</code> <p>GNPS format identified in the file.</p> Source code in <code>src/nplinker/metabolomics/gnps/gnps_format.py</code> <pre><code>def gnps_format_from_file_mapping(file: str | PathLike) -&gt; GNPSFormat:\n    \"\"\"Detect GNPS format from the given file mapping file.\n\n    The GNSP file mapping file is located in different folders depending on the\n    GNPS workflow. Here are the locations in corresponding GNPS zip archives:\n\n    - METABOLOMICS-SNETS workflow: the .tsv file under folder \"clusterinfosummarygroup_attributes_withIDs_withcomponentID\"\n    - METABOLOMICS-SNETS-V2 workflow: the .clustersummary file (tsv) under folder \"clusterinfosummarygroup_attributes_withIDs_withcomponentID\"\n    - FEATURE-BASED-MOLECULAR-NETWORKING workflow: the .csv file under folder \"quantification_table\"\n\n    Args:\n        file: Path to the file to peek the format for.\n\n    Returns:\n        GNPS format identified in the file.\n    \"\"\"\n    headers = get_headers(file)\n    if \"AllFiles\" in headers:\n        return GNPSFormat.SNETS\n    if \"UniqueFileSources\" in headers:\n        return GNPSFormat.SNETSV2\n    if \"row ID\" in headers:\n        return GNPSFormat.FBMN\n    return GNPSFormat.Unknown\n</code></pre>"},{"location":"api/gnps/#nplinker.metabolomics.gnps.gnps_format_from_task_id","title":"gnps_format_from_task_id","text":"<pre><code>gnps_format_from_task_id(task_id: str) -&gt; GNPSFormat\n</code></pre> <p>Detect GNPS format for the given task id.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>str</code> <p>GNPS task id.</p> required <p>Returns:</p> Type Description <code>GNPSFormat</code> <p>The format identified in the GNPS task.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gnps_format_from_task_id(\"c22f44b14a3d450eb836d607cb9521bb\") == GNPSFormat.SNETS\n&gt;&gt;&gt; gnps_format_from_task_id(\"189e8bf16af145758b0a900f1c44ff4a\") == GNPSFormat.SNETSV2\n&gt;&gt;&gt; gnps_format_from_task_id(\"92036537c21b44c29e509291e53f6382\") == GNPSFormat.FBMN\n&gt;&gt;&gt; gnps_format_from_task_id(\"0ad6535e34d449788f297e712f43068a\") == GNPSFormat.Unknown\n</code></pre> Source code in <code>src/nplinker/metabolomics/gnps/gnps_format.py</code> <pre><code>def gnps_format_from_task_id(task_id: str) -&gt; GNPSFormat:\n    \"\"\"Detect GNPS format for the given task id.\n\n    Args:\n        task_id: GNPS task id.\n\n    Returns:\n        The format identified in the GNPS task.\n\n    Examples:\n        &gt;&gt;&gt; gnps_format_from_task_id(\"c22f44b14a3d450eb836d607cb9521bb\") == GNPSFormat.SNETS\n        &gt;&gt;&gt; gnps_format_from_task_id(\"189e8bf16af145758b0a900f1c44ff4a\") == GNPSFormat.SNETSV2\n        &gt;&gt;&gt; gnps_format_from_task_id(\"92036537c21b44c29e509291e53f6382\") == GNPSFormat.FBMN\n        &gt;&gt;&gt; gnps_format_from_task_id(\"0ad6535e34d449788f297e712f43068a\") == GNPSFormat.Unknown\n    \"\"\"\n    task_html = httpx.get(GNPS_TASK_URL.format(task_id))\n    soup = BeautifulSoup(task_html.text, features=\"html.parser\")\n    tags = soup.find_all(\"th\")\n    workflow_tag: Tag = list(filter(lambda x: x.contents == [\"Workflow\"], tags))[0]\n    workflow_format_tag: Tag = workflow_tag.parent.contents[3]\n    workflow_format = workflow_format_tag.contents[0].strip()\n\n    if workflow_format == GNPSFormat.FBMN.value:\n        return GNPSFormat.FBMN\n    if workflow_format == GNPSFormat.SNETSV2.value:\n        return GNPSFormat.SNETSV2\n    if workflow_format == GNPSFormat.SNETS.value:\n        return GNPSFormat.SNETS\n    return GNPSFormat.Unknown\n</code></pre>"},{"location":"api/loader/","title":"Dataset Loader","text":""},{"location":"api/loader/#nplinker.loader","title":"loader","text":""},{"location":"api/loader/#nplinker.loader.DatasetLoader","title":"DatasetLoader","text":"<pre><code>DatasetLoader()\n</code></pre> Source code in <code>src/nplinker/loader.py</code> <pre><code>def __init__(self):\n    # set public attributes\n    self.bgcs, self.gcfs, self.spectra, self.molfams = [], [], [], []\n    self.mibig_bgcs = []\n    self.mibig_strains_in_use = StrainCollection()\n    self.product_types = []\n    self.strains = StrainCollection()\n\n    self.class_matches = None\n    self.chem_classes = None\n</code></pre>"},{"location":"api/metabolomics/","title":"Data Models","text":""},{"location":"api/metabolomics/#nplinker.metabolomics","title":"metabolomics","text":""},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily","title":"MolecularFamily","text":"<pre><code>MolecularFamily(family_id: str)\n</code></pre> <p>Class to model molecular family.</p> <p>Parameters:</p> Name Type Description Default <code>family_id</code> <code>str</code> <p>Unique id for the molecular family.</p> required <p>Attributes:</p> Name Type Description <code>family_id</code> <p>Unique id for the molecular family.</p> <code>spectra_ids</code> <p>Set of spectrum ids in the molecular family.</p> Source code in <code>src/nplinker/metabolomics/molecular_family.py</code> <pre><code>def __init__(self, family_id: str):\n    \"\"\"Class to model molecular family.\n\n    Args:\n        family_id: Unique id for the molecular family.\n\n    Attributes:\n        family_id: Unique id for the molecular family.\n        spectra_ids: Set of spectrum ids in the molecular family.\n    \"\"\"\n    self.family_id: str = family_id\n    self.spectra_ids: set[str] = set()\n    self._spectra: set[Spectrum] = set()\n    self._strains: StrainCollection = StrainCollection()\n</code></pre>"},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily.spectra","title":"spectra  <code>property</code>","text":"<pre><code>spectra: set[Spectrum]\n</code></pre> <p>Get Spectrum objects in the molecular family.</p>"},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily.strains","title":"strains  <code>property</code>","text":"<pre><code>strains: StrainCollection\n</code></pre> <p>Get strains in the molecular family.</p>"},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily.add_spectrum","title":"add_spectrum","text":"<pre><code>add_spectrum(spectrum: Spectrum) -&gt; None\n</code></pre> <p>Add a Spectrum object to the molecular family.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum</code> <code>Spectrum</code> <p><code>Spectrum</code> object to add to the molecular family.</p> required Source code in <code>src/nplinker/metabolomics/molecular_family.py</code> <pre><code>def add_spectrum(self, spectrum: Spectrum) -&gt; None:\n    \"\"\"Add a Spectrum object to the molecular family.\n\n    Args:\n        spectrum: `Spectrum` object to add to the molecular family.\n    \"\"\"\n    self._spectra.add(spectrum)\n    self.spectra_ids.add(spectrum.spectrum_id)\n    self._strains = self._strains + spectrum.strains\n    # add the molecular family to the spectrum\n    spectrum.family = self\n</code></pre>"},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily.detach_spectrum","title":"detach_spectrum","text":"<pre><code>detach_spectrum(spectrum: Spectrum) -&gt; None\n</code></pre> <p>Remove a Spectrum object from the molecular family.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum</code> <code>Spectrum</code> <p><code>Spectrum</code> object to remove from the molecular family.</p> required Source code in <code>src/nplinker/metabolomics/molecular_family.py</code> <pre><code>def detach_spectrum(self, spectrum: Spectrum) -&gt; None:\n    \"\"\"Remove a Spectrum object from the molecular family.\n\n    Args:\n        spectrum: `Spectrum` object to remove from the molecular family.\n    \"\"\"\n    self._spectra.remove(spectrum)\n    self.spectra_ids.remove(spectrum.spectrum_id)\n    self._strains = self._update_strains()\n    # remove the molecular family from the spectrum\n    spectrum.family = None\n</code></pre>"},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily.has_strain","title":"has_strain","text":"<pre><code>has_strain(strain: Strain) -&gt; bool\n</code></pre> <p>Check if the given strain exists.</p> <p>Parameters:</p> Name Type Description Default <code>strain</code> <code>Strain</code> <p><code>Strain</code> object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True when the given strain exists.</p> Source code in <code>src/nplinker/metabolomics/molecular_family.py</code> <pre><code>def has_strain(self, strain: Strain) -&gt; bool:\n    \"\"\"Check if the given strain exists.\n\n    Args:\n        strain: `Strain` object.\n\n    Returns:\n        True when the given strain exists.\n    \"\"\"\n    return strain in self._strains\n</code></pre>"},{"location":"api/metabolomics/#nplinker.metabolomics.MolecularFamily.is_singleton","title":"is_singleton","text":"<pre><code>is_singleton() -&gt; bool\n</code></pre> <p>Check if the molecular family contains only one spectrum.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True when <code>MolecularFamily.spectra_ids</code> contains only one spectrum id.</p> Source code in <code>src/nplinker/metabolomics/molecular_family.py</code> <pre><code>def is_singleton(self) -&gt; bool:\n    \"\"\"Check if the molecular family contains only one spectrum.\n\n    Returns:\n        True when `MolecularFamily.spectra_ids` contains only one spectrum id.\n    \"\"\"\n    return len(self.spectra_ids) == 1\n</code></pre>"},{"location":"api/metabolomics/#nplinker.metabolomics.Spectrum","title":"Spectrum","text":"<pre><code>Spectrum(spectrum_id: str, mz: list[float], intensity: list[float], precursor_mz: float, rt: float = 0, metadata: dict | None = None)\n</code></pre> <p>Class to model MS/MS Spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum_id</code> <code>str</code> <p>the spectrum ID.</p> required <code>mz</code> <code>list[float]</code> <p>the list of m/z values.</p> required <code>intensity</code> <code>list[float]</code> <p>the list of intensity values.</p> required <code>precursor_mz</code> <code>float</code> <p>the precursor m/z.</p> required <code>rt</code> <code>float</code> <p>the retention time in seconds. Defaults to 0.</p> <code>0</code> <code>metadata</code> <code>dict | None</code> <p>the metadata of the spectrum, i.e. the header infomation in the MGF file.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>spectrum_id</code> <p>the spectrum ID.</p> <code>mz</code> <p>the list of m/z values.</p> <code>intensity</code> <p>the list of intensity values.</p> <code>precursor_mz</code> <p>the m/z value of the precursor.</p> <code>rt</code> <p>the retention time in seconds.</p> <code>metadata</code> <p>the metadata of the spectrum, i.e. the header infomation in the MGF file.</p> <code>gnps_annotations</code> <p>the GNPS annotations of the spectrum.</p> <code>gnps_id</code> <p>the GNPS ID of the spectrum.</p> <code>strains</code> <p>the strains that this spectrum belongs to.</p> <code>family</code> <p>the molecular family that this spectrum belongs to.</p> <code>peaks</code> <p>2D array of peaks, each row is a peak of (m/z, intensity) values.</p> Source code in <code>src/nplinker/metabolomics/spectrum.py</code> <pre><code>def __init__(\n    self,\n    spectrum_id: str,\n    mz: list[float],\n    intensity: list[float],\n    precursor_mz: float,\n    rt: float = 0,\n    metadata: dict | None = None,\n) -&gt; None:\n    \"\"\"Class to model MS/MS Spectrum.\n\n    Args:\n        spectrum_id: the spectrum ID.\n        mz: the list of m/z values.\n        intensity: the list of intensity values.\n        precursor_mz: the precursor m/z.\n        rt: the retention time in seconds. Defaults to 0.\n        metadata: the metadata of the spectrum, i.e. the header infomation\n            in the MGF file.\n\n    Attributes:\n        spectrum_id: the spectrum ID.\n        mz: the list of m/z values.\n        intensity: the list of intensity values.\n        precursor_mz: the m/z value of the precursor.\n        rt: the retention time in seconds.\n        metadata: the metadata of the spectrum, i.e. the header infomation in the MGF\n            file.\n        gnps_annotations: the GNPS annotations of the spectrum.\n        gnps_id: the GNPS ID of the spectrum.\n        strains: the strains that this spectrum belongs to.\n        family: the molecular family that this spectrum belongs to.\n        peaks: 2D array of peaks, each row is a peak of (m/z, intensity) values.\n    \"\"\"\n    self.spectrum_id = spectrum_id\n    self.mz = mz\n    self.intensity = intensity\n    self.precursor_mz = precursor_mz\n    self.rt = rt\n    self.metadata = metadata or {}\n\n    self.gnps_annotations: dict = {}\n    self.gnps_id: str | None = None\n    self.strains: StrainCollection = StrainCollection()\n    self.family: MolecularFamily | None = None\n</code></pre>"},{"location":"api/metabolomics/#nplinker.metabolomics.Spectrum.peaks","title":"peaks  <code>cached</code> <code>property</code>","text":"<pre><code>peaks: ndarray\n</code></pre> <p>Get the peaks, a 2D array with each row containing the values of (m/z, intensity).</p>"},{"location":"api/metabolomics/#nplinker.metabolomics.Spectrum.has_strain","title":"has_strain","text":"<pre><code>has_strain(strain: Strain) -&gt; bool\n</code></pre> <p>Check if the given strain exists in the spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>strain</code> <code>Strain</code> <p><code>Strain</code> object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True when the given strain exist in the spectrum.</p> Source code in <code>src/nplinker/metabolomics/spectrum.py</code> <pre><code>def has_strain(self, strain: Strain) -&gt; bool:\n    \"\"\"Check if the given strain exists in the spectrum.\n\n    Args:\n        strain: `Strain` object.\n\n    Returns:\n        True when the given strain exist in the spectrum.\n    \"\"\"\n    return strain in self.strains\n</code></pre>"},{"location":"api/metabolomics_abc/","title":"Base Classes","text":""},{"location":"api/metabolomics_abc/#nplinker.metabolomics.abc","title":"abc","text":""},{"location":"api/metabolomics_abc/#nplinker.metabolomics.abc.MolecularFamilyLoaderBase","title":"MolecularFamilyLoaderBase","text":"<p>             Bases: <code>ABC</code></p>"},{"location":"api/metabolomics_abc/#nplinker.metabolomics.abc.MolecularFamilyLoaderBase.get_mfs","title":"get_mfs  <code>abstractmethod</code>","text":"<pre><code>get_mfs(keep_singleton: bool) -&gt; Sequence[MolecularFamily]\n</code></pre> <p>Get MolecularFamily objects.</p> <p>Parameters:</p> Name Type Description Default <code>keep_singleton</code> <code>bool</code> <p>True to keep singleton molecular families. A singleton molecular family is a molecular family that contains only one spectrum.</p> required <p>Returns:</p> Type Description <code>Sequence[MolecularFamily]</code> <p>Sequence[MolecularFamily]: a list of MolecularFamily objects.</p> Source code in <code>src/nplinker/metabolomics/abc.py</code> <pre><code>@abstractmethod\ndef get_mfs(self, keep_singleton: bool) -&gt; Sequence[\"MolecularFamily\"]:\n    \"\"\"Get MolecularFamily objects.\n\n    Args:\n        keep_singleton: True to keep singleton molecular families. A\n            singleton molecular family is a molecular family that contains\n            only one spectrum.\n\n    Returns:\n        Sequence[MolecularFamily]: a list of MolecularFamily objects.\n    \"\"\"\n</code></pre>"},{"location":"api/metabolomics_utils/","title":"Utilities","text":""},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils","title":"utils","text":""},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils.add_annotation_to_spectrum","title":"add_annotation_to_spectrum","text":"<pre><code>add_annotation_to_spectrum(annotations: dict[str, dict], spectra: list[Spectrum]) -&gt; None\n</code></pre> <p>Add GNPS annotations to the <code>Spectrum.gnps_annotaions</code> attribute for input spectra.</p> <p>It is possible that some spectra don't have annotations. Note that the input <code>spectra</code> list is changed in place.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict[str, dict]</code> <p>A dictionary of GNPS annotations, where the keys are spectrum ids and the values are GNPS annotations.</p> required <code>spectra</code> <code>list[Spectrum]</code> <p>A list of Spectrum objects.</p> required Source code in <code>src/nplinker/metabolomics/utils.py</code> <pre><code>def add_annotation_to_spectrum(annotations: dict[str, dict], spectra: list[Spectrum]) -&gt; None:\n    \"\"\"Add GNPS annotations to the `Spectrum.gnps_annotaions` attribute for input spectra.\n\n    It is possible that some spectra don't have annotations.\n    Note that the input `spectra` list is changed in place.\n\n    Args:\n        annotations: A dictionary of GNPS annotations, where the keys are\n            spectrum ids and the values are GNPS annotations.\n        spectra: A list of Spectrum objects.\n    \"\"\"\n    for spec in spectra:\n        if spec.spectrum_id in annotations:\n            spec.gnps_annotations = annotations[spec.spectrum_id]\n</code></pre>"},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils.add_strains_to_spectrum","title":"add_strains_to_spectrum","text":"<pre><code>add_strains_to_spectrum(strains: StrainCollection, spectra: list[Spectrum]) -&gt; tuple[list[Spectrum], list[Spectrum]]\n</code></pre> <p>Add <code>Strain</code> objects to the <code>Spectrum.strains</code> attribute for input spectra.</p> <p>Note that the input <code>spectra</code> list is changed in place.</p> <p>Parameters:</p> Name Type Description Default <code>strains</code> <code>StrainCollection</code> <p>A collection of strain objects.</p> required <code>spectra</code> <code>list[Spectrum]</code> <p>A list of Spectrum objects.</p> required <p>Returns:</p> Type Description <code>tuple[list[Spectrum], list[Spectrum]]</code> <p>A tuple of two lists of Spectrum objects. The first list contains Spectrum objects that are updated with Strain objects; the second list contains Spectrum objects that are not updated with Strain objects becuase no Strain objects are found.</p> Source code in <code>src/nplinker/metabolomics/utils.py</code> <pre><code>def add_strains_to_spectrum(\n    strains: StrainCollection, spectra: list[Spectrum]\n) -&gt; tuple[list[Spectrum], list[Spectrum]]:\n    \"\"\"Add `Strain` objects to the `Spectrum.strains` attribute for input spectra.\n\n    Note that the input `spectra` list is changed in place.\n\n    Args:\n        strains: A collection of strain objects.\n        spectra: A list of Spectrum objects.\n\n    Returns:\n        A tuple of two lists of Spectrum\n            objects. The first list contains Spectrum objects that are updated\n            with Strain objects; the second list contains Spectrum objects that\n            are not updated with Strain objects becuase no Strain objects are found.\n    \"\"\"\n    spectra_with_strains = []\n    spectra_without_strains = []\n    for spec in spectra:\n        try:\n            strain_list = strains.lookup(spec.spectrum_id)\n        except ValueError:\n            spectra_without_strains.append(spec)\n            continue\n\n        for strain in strain_list:\n            spec.strains.add(strain)\n        spectra_with_strains.append(spec)\n\n    logger.info(\n        f\"{len(spectra_with_strains)} Spectrum objects updated with Strain objects.\\n\"\n        f\"{len(spectra_without_strains)} Spectrum objects not updated with Strain objects.\"\n    )\n\n    return spectra_with_strains, spectra_without_strains\n</code></pre>"},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils.add_spectrum_to_mf","title":"add_spectrum_to_mf","text":"<pre><code>add_spectrum_to_mf(spectra: list[Spectrum], mfs: list[MolecularFamily]) -&gt; tuple[list[MolecularFamily], list[MolecularFamily], dict[MolecularFamily, set[str]]]\n</code></pre> <p>Add Spectrum objects to MolecularFamily objects.</p> <p>The attribute of <code>spectra_ids</code> of MolecularFamily object contains the ids of Spectrum objects. These ids are used to find Spectrum objects from the input <code>spectra</code> list. The found Spectrum objects are added to the <code>spectra</code> attribute of MolecularFamily object. It is possible that some spectrum ids are not found in the input <code>spectra</code> list, and so their Spectrum objects are missing in the MolecularFamily object.</p> <p>Note that the input <code>mfs</code> list is changed in place.</p> <p>Parameters:</p> Name Type Description Default <code>spectra</code> <code>list[Spectrum]</code> <p>A list of Spectrum objects.</p> required <code>mfs</code> <code>list[MolecularFamily]</code> <p>A list of MolecularFamily objects.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list[MolecularFamily], list[MolecularFamily], dict[MolecularFamily, set[str]]]</code> <p>The first list contains MolecularFamily objects that are updated with Spectrum objects. The second list contains MolecularFamily objects that are not updated with Spectrum objects (all Spectrum objects are missing). The dictionary contains MolecularFamily objects as keys and a set of ids of missing Spectrum objects as values.</p> Source code in <code>src/nplinker/metabolomics/utils.py</code> <pre><code>def add_spectrum_to_mf(\n    spectra: list[Spectrum], mfs: list[MolecularFamily]\n) -&gt; tuple[list[MolecularFamily], list[MolecularFamily], dict[MolecularFamily, set[str]]]:\n    \"\"\"Add Spectrum objects to MolecularFamily objects.\n\n    The attribute of `spectra_ids` of MolecularFamily object contains the ids of Spectrum objects.\n    These ids are used to find Spectrum objects from the input `spectra` list. The found Spectrum\n    objects are added to the `spectra` attribute of MolecularFamily object. It is possible that\n    some spectrum ids are not found in the input `spectra` list, and so their Spectrum objects are\n    missing in the MolecularFamily object.\n\n    Note that the input `mfs` list is changed in place.\n\n    Args:\n        spectra: A list of Spectrum objects.\n        mfs: A list of MolecularFamily objects.\n\n    Returns:\n        tuple:\n            The first list contains MolecularFamily objects that are updated with Spectrum objects.\n            The second list contains MolecularFamily objects that are not updated with Spectrum\n            objects (all Spectrum objects are missing).\n            The dictionary contains MolecularFamily objects as keys and a set of ids of missing\n            Spectrum objects as values.\n    \"\"\"\n    spec_dict = {spec.spectrum_id: spec for spec in spectra}\n    mf_with_spec = []\n    mf_without_spec = []\n    mf_missing_spec: dict[MolecularFamily, set[str]] = {}\n    for mf in mfs:\n        for spec_id in mf.spectra_ids:\n            try:\n                spec = spec_dict[spec_id]\n            except KeyError:\n                if mf not in mf_missing_spec:\n                    mf_missing_spec[mf] = {spec_id}\n                else:\n                    mf_missing_spec[mf].add(spec_id)\n                continue\n            mf.add_spectrum(spec)\n\n        if mf.spectra:\n            mf_with_spec.append(mf)\n        else:\n            mf_without_spec.append(mf)\n\n    logger.info(\n        f\"{len(mf_with_spec)} MolecularFamily objects updated with Spectrum objects.\\n\"\n        f\"{len(mf_without_spec)} MolecularFamily objects not updated with Spectrum objects.\\n\"\n        f\"{len(mf_missing_spec)} MolecularFamily objects have missing Spectrum objects.\"\n    )\n    return mf_with_spec, mf_without_spec, mf_missing_spec\n</code></pre>"},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils.extract_mappings_strain_id_ms_filename","title":"extract_mappings_strain_id_ms_filename","text":"<pre><code>extract_mappings_strain_id_ms_filename(podp_project_json_file: str | PathLike) -&gt; dict[str, set[str]]\n</code></pre> <p>Extract mappings \"strain_id &lt;-&gt; MS_filename\".</p> <p>Parameters:</p> Name Type Description Default <code>podp_project_json_file</code> <code>str | PathLike</code> <p>The path to the PODP project JSON file.</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Key is strain id and value is a set of MS filenames.</p> Notes <p>The <code>podp_project_json_file</code> is the project JSON file downloaded from PODP platform. For example, for project MSV000079284, its json file is https://pairedomicsdata.bioinformatics.nl/api/projects/4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4.</p> Source code in <code>src/nplinker/metabolomics/utils.py</code> <pre><code>def extract_mappings_strain_id_ms_filename(\n    podp_project_json_file: str | PathLike\n) -&gt; dict[str, set[str]]:\n    \"\"\"Extract mappings \"strain_id &lt;-&gt; MS_filename\".\n\n    Args:\n        podp_project_json_file: The path to the PODP project\n            JSON file.\n\n    Returns:\n        Key is strain id and value is a set of MS filenames.\n\n    Notes:\n        The `podp_project_json_file` is the project JSON file downloaded from\n        PODP platform. For example, for project MSV000079284, its json file is\n        https://pairedomicsdata.bioinformatics.nl/api/projects/4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4.\n    \"\"\"\n    mappings_dict = {}\n    with open(podp_project_json_file, \"r\") as f:\n        json_data = json.load(f)\n\n    validate_podp_json(json_data)\n\n    # Extract mappings strain id &lt;-&gt; metabolomics filename\n    for record in json_data[\"genome_metabolome_links\"]:\n        strain_id = record[\"genome_label\"]\n        # get the actual filename of the mzXML URL\n        filename = Path(record[\"metabolomics_file\"]).name\n        if strain_id in mappings_dict:\n            mappings_dict[strain_id].add(filename)\n        else:\n            mappings_dict[strain_id] = {filename}\n    return mappings_dict\n</code></pre>"},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils.extract_mappings_ms_filename_spectrum_id","title":"extract_mappings_ms_filename_spectrum_id","text":"<pre><code>extract_mappings_ms_filename_spectrum_id(gnps_file_mappings_file: str | PathLike) -&gt; dict[str, set[str]]\n</code></pre> <p>Extract mappings \"MS_filename &lt;-&gt; spectrum_id\".</p> <p>Parameters:</p> Name Type Description Default <code>gnps_file_mappings_file</code> <code>str | PathLike</code> <p>The path to the GNPS file mappings file (csv or tsv).</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Key is MS filename and value is a set of spectrum ids.</p> Notes <p>The <code>gnps_file_mappings_file</code> is generated by GNPS molecular networking. It's downloaded from GNPS website to a file with a default name defined in <code>GNPS_FILE_MAPPINGS_FILENAME</code>.</p> See Also <p>GNPSFileMappingLoader: A class to load GNPS file mappings file.</p> Source code in <code>src/nplinker/metabolomics/utils.py</code> <pre><code>def extract_mappings_ms_filename_spectrum_id(\n    gnps_file_mappings_file: str | PathLike\n) -&gt; dict[str, set[str]]:\n    \"\"\"Extract mappings \"MS_filename &lt;-&gt; spectrum_id\".\n\n    Args:\n        gnps_file_mappings_file: The path to the GNPS file mappings file (csv or\n            tsv).\n\n    Returns:\n        Key is MS filename and value is a set of spectrum ids.\n\n    Notes:\n        The `gnps_file_mappings_file` is generated by GNPS molecular networking. It's downloaded\n        from GNPS website to a file with a default name defined in `GNPS_FILE_MAPPINGS_FILENAME`.\n\n    See Also:\n        GNPSFileMappingLoader: A class to load GNPS file mappings file.\n    \"\"\"\n    loader = GNPSFileMappingLoader(gnps_file_mappings_file)\n    return loader.mapping_reversed\n</code></pre>"},{"location":"api/metabolomics_utils/#nplinker.metabolomics.utils.get_mappings_strain_id_spectrum_id","title":"get_mappings_strain_id_spectrum_id","text":"<pre><code>get_mappings_strain_id_spectrum_id(mappings_strain_id_ms_filename: dict[str, set[str]], mappings_ms_filename_spectrum_id: dict[str, set[str]]) -&gt; dict[str, set[str]]\n</code></pre> <p>Get mappings \"strain_id &lt;-&gt; spectrum_id\".</p> <p>Parameters:</p> Name Type Description Default <code>mappings_strain_id_ms_filename</code> <code>dict[str, set[str]]</code> <p>Mappings \"strain_id &lt;-&gt; MS_filename\".</p> required <code>mappings_ms_filename_spectrum_id</code> <code>dict[str, set[str]]</code> <p>Mappings \"MS_filename &lt;-&gt; spectrum_id\".</p> required <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Key is strain id and value is a set of spectrum ids.</p> See Also <p><code>extract_mappings_strain_id_ms_filename</code>: Extract mappings     \"strain_id &lt;-&gt; MS_filename\". <code>extract_mappings_ms_filename_spectrum_id</code>: Extract mappings     \"MS_filename &lt;-&gt; spectrum_id\".</p> Source code in <code>src/nplinker/metabolomics/utils.py</code> <pre><code>def get_mappings_strain_id_spectrum_id(\n    mappings_strain_id_ms_filename: dict[str, set[str]],\n    mappings_ms_filename_spectrum_id: dict[str, set[str]],\n) -&gt; dict[str, set[str]]:\n    \"\"\"Get mappings \"strain_id &lt;-&gt; spectrum_id\".\n\n    Args:\n        mappings_strain_id_ms_filename: Mappings\n            \"strain_id &lt;-&gt; MS_filename\".\n        mappings_ms_filename_spectrum_id: Mappings\n            \"MS_filename &lt;-&gt; spectrum_id\".\n\n    Returns:\n        Key is strain id and value is a set of spectrum ids.\n\n\n    See Also:\n        `extract_mappings_strain_id_ms_filename`: Extract mappings\n            \"strain_id &lt;-&gt; MS_filename\".\n        `extract_mappings_ms_filename_spectrum_id`: Extract mappings\n            \"MS_filename &lt;-&gt; spectrum_id\".\n    \"\"\"\n    mappings_dict = {}\n    for strain_id, ms_filenames in mappings_strain_id_ms_filename.items():\n        spectrum_ids = set()\n        for ms_filename in ms_filenames:\n            if (sid := mappings_ms_filename_spectrum_id.get(ms_filename)) is not None:\n                spectrum_ids.update(sid)\n        if spectrum_ids:\n            mappings_dict[strain_id] = spectrum_ids\n    return mappings_dict\n</code></pre>"},{"location":"api/mibig/","title":"MiBIG","text":""},{"location":"api/mibig/#nplinker.genomics.mibig","title":"mibig","text":""},{"location":"api/mibig/#nplinker.genomics.mibig.MibigLoader","title":"MibigLoader","text":"<pre><code>MibigLoader(data_dir: str)\n</code></pre> <p>Parse MIBiG metadata files and return BGC objects.</p> <p>MIBiG metadata file (json) contains annotations/metadata information for each BGC. See https://mibig.secondarymetabolites.org/download.</p> <p>The MiBIG accession is used as BGC id and strain name. The loaded BGC objects have Strain object as their strain attribute (i.e. <code>BGC.strain</code>).</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Path to the directory of MIBiG metadata json files</p> required Source code in <code>src/nplinker/genomics/mibig/mibig_loader.py</code> <pre><code>def __init__(self, data_dir: str):\n    \"\"\"Parse MIBiG metadata files and return BGC objects.\n\n    MIBiG metadata file (json) contains annotations/metadata information\n    for each BGC. See https://mibig.secondarymetabolites.org/download.\n\n    The MiBIG accession is used as BGC id and strain name. The loaded BGC\n    objects have Strain object as their strain attribute (i.e. `BGC.strain`).\n\n    Args:\n        data_dir: Path to the directory of MIBiG metadata json files\n    \"\"\"\n    self.data_dir = data_dir\n    self._file_dict = self.parse_data_dir(self.data_dir)\n    self._metadata_dict = self._parse_metadatas()\n    self._bgcs = self._parse_bgcs()\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigLoader.get_files","title":"get_files","text":"<pre><code>get_files() -&gt; dict[str, str]\n</code></pre> <p>Get the path of all MIBiG metadata json files.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The key is metadata file name (BGC accession), and the value is path to the metadata</p> <code>dict[str, str]</code> <p>json file</p> Source code in <code>src/nplinker/genomics/mibig/mibig_loader.py</code> <pre><code>def get_files(self) -&gt; dict[str, str]:\n    \"\"\"Get the path of all MIBiG metadata json files.\n\n    Returns:\n        The key is metadata file name (BGC accession), and the value is path to the metadata\n        json file\n    \"\"\"\n    return self._file_dict\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigLoader.parse_data_dir","title":"parse_data_dir  <code>staticmethod</code>","text":"<pre><code>parse_data_dir(data_dir: str) -&gt; dict[str, str]\n</code></pre> <p>Parse metadata directory and return paths to all metadata json files.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>path to the directory of MIBiG metadata json files</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>The key is metadata file name (BGC accession), and the value is path to the metadata</p> <code>dict[str, str]</code> <p>json file</p> Source code in <code>src/nplinker/genomics/mibig/mibig_loader.py</code> <pre><code>@staticmethod\ndef parse_data_dir(data_dir: str) -&gt; dict[str, str]:\n    \"\"\"Parse metadata directory and return paths to all metadata json files.\n\n    Args:\n        data_dir: path to the directory of MIBiG metadata json files\n\n    Returns:\n        The key is metadata file name (BGC accession), and the value is path to the metadata\n        json file\n    \"\"\"\n    file_dict = {}\n    json_files = list_files(data_dir, prefix=\"BGC\", suffix=\".json\")\n    for file in json_files:\n        fname = os.path.splitext(os.path.basename(file))[0]\n        file_dict[fname] = file\n    return file_dict\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigLoader.get_metadatas","title":"get_metadatas","text":"<pre><code>get_metadatas() -&gt; dict[str, MibigMetadata]\n</code></pre> <p>Get MibigMetadata objects.</p> <p>Returns:</p> Type Description <code>dict[str, MibigMetadata]</code> <p>The key is BGC accession (file name) and the value is MibigMetadata object</p> Source code in <code>src/nplinker/genomics/mibig/mibig_loader.py</code> <pre><code>def get_metadatas(self) -&gt; dict[str, MibigMetadata]:\n    \"\"\"Get MibigMetadata objects.\n\n    Returns:\n        The key is BGC accession (file name) and the value is MibigMetadata object\n    \"\"\"\n    return self._metadata_dict\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigLoader.get_bgcs","title":"get_bgcs","text":"<pre><code>get_bgcs() -&gt; list[BGC]\n</code></pre> <p>Get BGC objects.</p> <p>The BGC objects use MiBIG accession as id and have Strain object as their strain attribute (i.e. <code>BGC.strain</code>), where the name of the Strain object is also MiBIG accession.</p> <p>Returns:</p> Type Description <code>list[BGC]</code> <p>A list of BGC objects</p> Source code in <code>src/nplinker/genomics/mibig/mibig_loader.py</code> <pre><code>def get_bgcs(self) -&gt; list[BGC]:\n    \"\"\"Get BGC objects.\n\n    The BGC objects use MiBIG accession as id and have Strain object as\n    their strain attribute (i.e. `BGC.strain`), where the name of the Strain\n    object is also MiBIG accession.\n\n    Returns:\n        A list of BGC objects\n    \"\"\"\n    return self._bgcs\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigMetadata","title":"MibigMetadata","text":"<pre><code>MibigMetadata(file: str)\n</code></pre> <p>Class to model the BGC metadata/annotations defined in MIBiG.</p> <p>MIBiG is a specification of BGC metadata and use JSON schema to represent BGC metadata. More details see: https://mibig.secondarymetabolites.org/download.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the json file of MIBiG BGC metadata</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; metadata = MibigMetadata(\"/data/BGC0000001.json\")\n</code></pre> Source code in <code>src/nplinker/genomics/mibig/mibig_metadata.py</code> <pre><code>def __init__(self, file: str) -&gt; None:\n    \"\"\"Class to model the BGC metadata/annotations defined in MIBiG.\n\n    MIBiG is a specification of BGC metadata and use JSON schema to\n    represent BGC metadata. More details see:\n    https://mibig.secondarymetabolites.org/download.\n\n    Args:\n        file: Path to the json file of MIBiG BGC metadata\n\n    Examples:\n        &gt;&gt;&gt; metadata = MibigMetadata(\"/data/BGC0000001.json\")\n    \"\"\"\n    self.file = file\n    with open(self.file, \"rb\") as f:\n        self.metadata = json.load(f)\n\n    self._mibig_accession: str\n    self._biosyn_class: tuple[str]\n    self._parse_metadata()\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigMetadata.mibig_accession","title":"mibig_accession  <code>property</code>","text":"<pre><code>mibig_accession: str\n</code></pre> <p>Get the value of metadata item 'mibig_accession'.</p>"},{"location":"api/mibig/#nplinker.genomics.mibig.MibigMetadata.biosyn_class","title":"biosyn_class  <code>property</code>","text":"<pre><code>biosyn_class: tuple[str]\n</code></pre> <p>Get the value of metadata item 'biosyn_class'.</p> <p>The 'biosyn_class' is biosynthetic class(es), namely the type of natural product or secondary metabolite.</p> <p>MIBiG defines 6 major biosynthetic classes, including \"NRP\", \"Polyketide\", \"RiPP\", \"Terpene\", \"Saccharide\" and \"Alkaloid\". Note that natural products created by all other biosynthetic mechanisms fall under the category \"Other\". More details see the publication: https://doi.org/10.1186/s40793-018-0318-y.</p>"},{"location":"api/mibig/#nplinker.genomics.mibig.download_and_extract_mibig_metadata","title":"download_and_extract_mibig_metadata","text":"<pre><code>download_and_extract_mibig_metadata(download_root: str | PathLike, extract_path: str | PathLike, version: str = '3.1')\n</code></pre> <p>Download and extract MIBiG metadata json files.</p> <p>Note that it does not matter whether the metadata json files are in nested folders or not in the archive, all json files will be extracted to the same location, i.e. <code>extract_path</code>. The nested folders will be removed if they exist. So the <code>extract_path</code> will have only json files.</p> <p>Parameters:</p> Name Type Description Default <code>download_root</code> <code>str | PathLike</code> <p>Path to the directory in which to place the downloaded archive.</p> required <code>extract_path</code> <code>str | PathLike</code> <p>Path to an empty directory where the json files will be extracted. The directory must be empty if it exists. If it doesn't exist, the directory will be created.</p> required <code>version</code> <code>str</code> <p>description. Defaults to \"3.1\".</p> <code>'3.1'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; download_and_extract_mibig_metadata(\"/data/download\", \"/data/mibig_metadata\")\n</code></pre> Source code in <code>src/nplinker/genomics/mibig/mibig_downloader.py</code> <pre><code>def download_and_extract_mibig_metadata(\n    download_root: str | os.PathLike,\n    extract_path: str | os.PathLike,\n    version: str = \"3.1\",\n):\n    \"\"\"Download and extract MIBiG metadata json files.\n\n    Note that it does not matter whether the metadata json files are in nested folders or not in the archive,\n    all json files will be extracted to the same location, i.e. `extract_path`. The nested\n    folders will be removed if they exist. So the `extract_path` will have only json files.\n\n    Args:\n        download_root: Path to the directory in which to place the downloaded archive.\n        extract_path: Path to an empty directory where the json files will be extracted.\n            The directory must be empty if it exists. If it doesn't exist, the directory will be created.\n        version: _description_. Defaults to \"3.1\".\n\n    Examples:\n        &gt;&gt;&gt; download_and_extract_mibig_metadata(\"/data/download\", \"/data/mibig_metadata\")\n    \"\"\"\n    download_root = Path(download_root)\n    extract_path = Path(extract_path)\n\n    if download_root == extract_path:\n        raise ValueError(\"Identical path of download directory and extract directory\")\n\n    # check if extract_path is empty\n    if not extract_path.exists():\n        extract_path.mkdir(parents=True)\n    else:\n        if len(list(extract_path.iterdir())) != 0:\n            raise ValueError(f'Nonempty directory: \"{extract_path}\"')\n\n    # download and extract\n    md5 = _MD5_MIBIG_METADATA[version]\n    download_and_extract_archive(\n        url=MIBIG_METADATA_URL.format(version=version),\n        download_root=download_root,\n        extract_root=extract_path,\n        md5=md5,\n    )\n\n    # After extracting mibig archive, it's either one dir or many json files,\n    # if it's a dir, then move all json files from it to extract_path\n    subdirs = list_dirs(extract_path)\n    if len(subdirs) &gt; 1:\n        raise ValueError(f\"Expected one extracted directory, got {len(subdirs)}\")\n\n    if len(subdirs) == 1:\n        subdir_path = subdirs[0]\n        for fname in list_files(subdir_path, prefix=\"BGC\", suffix=\".json\", keep_parent=False):\n            shutil.move(os.path.join(subdir_path, fname), os.path.join(extract_path, fname))\n        # delete subdir\n        if subdir_path != extract_path:\n            shutil.rmtree(subdir_path)\n</code></pre>"},{"location":"api/mibig/#nplinker.genomics.mibig.parse_bgc_metadata_json","title":"parse_bgc_metadata_json","text":"<pre><code>parse_bgc_metadata_json(file: str) -&gt; BGC\n</code></pre> <p>Parse MIBiG metadata file and return BGC object.</p> <p>Note that the MiBIG accession is used as the BGC id and strain name. The BGC object has Strain object as its strain attribute.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to the MIBiG metadata json file</p> required <p>Returns:</p> Type Description <code>BGC</code> <p>BGC object</p> Source code in <code>src/nplinker/genomics/mibig/mibig_loader.py</code> <pre><code>def parse_bgc_metadata_json(file: str) -&gt; BGC:\n    \"\"\"Parse MIBiG metadata file and return BGC object.\n\n    Note that the MiBIG accession is used as the BGC id and strain name. The BGC\n    object has Strain object as its strain attribute.\n\n    Args:\n        file: Path to the MIBiG metadata json file\n\n    Returns:\n        BGC object\n    \"\"\"\n    metadata = MibigMetadata(file)\n    mibig_bgc = BGC(metadata.mibig_accession, *metadata.biosyn_class)\n    mibig_bgc.mibig_bgc_class = metadata.biosyn_class\n    mibig_bgc.strain = Strain(metadata.mibig_accession)\n    return mibig_bgc\n</code></pre>"},{"location":"api/nplinker/","title":"NPLinker","text":""},{"location":"api/nplinker/#nplinker.nplinker","title":"nplinker","text":""},{"location":"api/nplinker/#nplinker.nplinker.NPLinker","title":"NPLinker","text":"<pre><code>NPLinker()\n</code></pre> <p>Initialise an NPLinker instance.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialise an NPLinker instance.\"\"\"\n    # configure logging based on the supplied config params\n    LogConfig.setLogLevelStr(config.log.level)\n    logfile = config.get(\"log.file\")\n    if logfile:\n        logfile_dest = logging.FileHandler(logfile)\n        # if we want to log to stdout plus logfile, add the new destination\n        if config.get(\"log.to_stdout\"):  # default to True\n            LogConfig.addLogDestination(logfile_dest)\n        else:\n            # otherwise overwrite the default stdout destination\n            LogConfig.setLogDestination(logfile_dest)\n\n    self._loader = DatasetLoader()\n\n    self._spectra = []\n    self._bgcs = []\n    self._gcfs = []\n    self._strains = None\n    self._metadata = {}\n    self._molfams = []\n    self._mibig_bgcs = []\n    self._chem_classes = None\n    self._class_matches = None\n\n    self._bgc_lookup = {}\n    self._gcf_lookup = {}\n    self._spec_lookup = {}\n    self._mf_lookup = {}\n\n    self._scoring_methods = {}\n    config_methods = config.get(\"scoring_methods\", [])\n    for name, method in NPLinker.SCORING_METHODS.items():\n        if len(config_methods) == 0 or name in config_methods:\n            self._scoring_methods[name] = method\n            logger.debug(f\"Enabled scoring method: {name}\")\n\n    self._scoring_methods_setup_complete = {\n        name: False for name in self._scoring_methods.keys()\n    }\n\n    self._datalinks = None\n\n    self._repro_data = {}\n    repro_file = config.get(\"repro_file\")\n    if repro_file:\n        self.save_repro_data(repro_file)\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.root_dir","title":"root_dir  <code>property</code>","text":"<pre><code>root_dir: str\n</code></pre> <p>Returns path to the current dataset root directory.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the dataset root directory currently in use</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.data_dir","title":"data_dir  <code>property</code>","text":"<pre><code>data_dir\n</code></pre> <p>Returns path to nplinker/data directory (files packaged with the app itself).</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.bigscape_cutoff","title":"bigscape_cutoff  <code>property</code>","text":"<pre><code>bigscape_cutoff\n</code></pre> <p>Returns the current BiGSCAPE clustering cutoff value.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.strains","title":"strains  <code>property</code>","text":"<pre><code>strains\n</code></pre> <p>Returns a list of all the strains in the dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.bgcs","title":"bgcs  <code>property</code>","text":"<pre><code>bgcs\n</code></pre> <p>Returns a list of all the BGCs in the dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.gcfs","title":"gcfs  <code>property</code>","text":"<pre><code>gcfs\n</code></pre> <p>Returns a list of all the GCFs in the dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.spectra","title":"spectra  <code>property</code>","text":"<pre><code>spectra\n</code></pre> <p>Returns a list of all the Spectra in the dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.molfams","title":"molfams  <code>property</code>","text":"<pre><code>molfams\n</code></pre> <p>Returns a list of all the MolecularFamilies in the dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.mibig_bgcs","title":"mibig_bgcs  <code>property</code>","text":"<pre><code>mibig_bgcs\n</code></pre> <p>Get a list of all the MIBiG BGCs in the dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.product_types","title":"product_types  <code>property</code>","text":"<pre><code>product_types\n</code></pre> <p>Returns a list of the available BiGSCAPE product types in current dataset.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.repro_data","title":"repro_data  <code>property</code>","text":"<pre><code>repro_data\n</code></pre> <p>Returns the dict containing reproducibility data.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.scoring_methods","title":"scoring_methods  <code>property</code>","text":"<pre><code>scoring_methods\n</code></pre> <p>Returns a list of available scoring method names.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.chem_classes","title":"chem_classes  <code>property</code>","text":"<pre><code>chem_classes\n</code></pre> <p>Returns loaded ChemClassPredictions with the class predictions.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.class_matches","title":"class_matches  <code>property</code>","text":"<pre><code>class_matches\n</code></pre> <p>ClassMatches with the matched classes and scoring tables from MIBiG.</p>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.load_data","title":"load_data","text":"<pre><code>load_data()\n</code></pre> <p>Loads the basic components of a dataset.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def load_data(self):\n    \"\"\"Loads the basic components of a dataset.\"\"\"\n    arranger = DatasetArranger()\n    arranger.arrange()\n    self._loader.load()\n\n    self._spectra = self._loader.spectra\n    self._molfams = self._loader.molfams\n    self._bgcs = self._loader.bgcs\n    self._gcfs = self._loader.gcfs\n    self._mibig_bgcs = self._loader.mibig_bgcs\n    self._strains = self._loader.strains\n    self._product_types = self._loader.product_types\n    self._chem_classes = self._loader.chem_classes\n    self._class_matches = self._loader.class_matches\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.get_links","title":"get_links","text":"<pre><code>get_links(input_objects: list, scoring_methods: list, and_mode: bool = True) -&gt; LinkCollection\n</code></pre> <p>Find links for a set of input objects (BGCs/GCFs/Spectra/MolFams).</p> <p>The input objects can be any mix of the following NPLinker types:</p> <pre><code>- BGC\n- GCF\n- Spectrum\n- MolecularFamily\n</code></pre> <p>TODO longer description here</p> <p>Parameters:</p> Name Type Description Default <code>input_objects</code> <code>list</code> <p>objects to be passed to the scoring method(s). This may be either a flat list of a uniform type (one of the 4 types above), or a list of such lists</p> required <code>scoring_methods</code> <code>list</code> <p>a list of one or more scoring methods to use</p> required <code>and_mode</code> <code>bool</code> <p>determines how results from multiple methods are combined. This is ignored if a single method is supplied. If multiple methods are used and <code>and_mode</code> is True, the results will only contain links found by ALL methods. If False, results will contain links found by ANY method.</p> <code>True</code> <p>Returns:</p> Type Description <code>LinkCollection</code> <p>An instance of <code>nplinker.scoring.methods.LinkCollection</code></p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def get_links(\n    self, input_objects: list, scoring_methods: list, and_mode: bool = True\n) -&gt; LinkCollection:\n    \"\"\"Find links for a set of input objects (BGCs/GCFs/Spectra/MolFams).\n\n    The input objects can be any mix of the following NPLinker types:\n\n        - BGC\n        - GCF\n        - Spectrum\n        - MolecularFamily\n\n    TODO longer description here\n\n    Args:\n        input_objects: objects to be passed to the scoring method(s).\n            This may be either a flat list of a uniform type (one of the 4\n            types above), or a list of such lists\n        scoring_methods: a list of one or more scoring methods to use\n        and_mode: determines how results from multiple methods are combined.\n            This is ignored if a single method is supplied. If multiple methods\n            are used and ``and_mode`` is True, the results will only contain\n            links found by ALL methods. If False, results will contain links\n            found by ANY method.\n\n    Returns:\n        An instance of ``nplinker.scoring.methods.LinkCollection``\n    \"\"\"\n    if isinstance(input_objects, list) and len(input_objects) == 0:\n        raise Exception(\"input_objects length must be &gt; 0\")\n\n    if isinstance(scoring_methods, list) and len(scoring_methods) == 0:\n        raise Exception(\"scoring_methods length must be &gt; 0\")\n\n    # for convenience convert a single scoring object into a single entry\n    # list\n    if not isinstance(scoring_methods, list):\n        scoring_methods = [scoring_methods]\n\n    # check if input_objects is a list of lists. if so there should be one\n    # entry for each supplied method for it to be a valid parameter\n    if isinstance(input_objects[0], list):\n        if len(input_objects) != len(scoring_methods):\n            raise Exception(\n                \"Number of input_objects lists must match number of scoring_methods (found: {}, expected: {})\".format(\n                    len(input_objects), len(scoring_methods)\n                )\n            )\n\n    # TODO check scoring_methods only contains ScoringMethod-derived\n    # instances\n\n    # want everything to be in lists of lists\n    if not isinstance(input_objects, list) or (\n        isinstance(input_objects, list) and not isinstance(input_objects[0], list)\n    ):\n        input_objects = [input_objects]\n\n    logger.debug(\n        \"get_links: {} object sets, {} methods\".format(len(input_objects), len(scoring_methods))\n    )\n\n    # copy the object set if required to make up the numbers\n    if len(input_objects) != len(scoring_methods):\n        if len(scoring_methods) &lt; len(input_objects):\n            raise Exception(\"Number of scoring methods must be &gt;= number of input object sets\")\n        elif (len(scoring_methods) &gt; len(input_objects)) and len(input_objects) != 1:\n            raise Exception(\n                \"Mismatch between number of scoring methods and input objects ({} vs {})\".format(\n                    len(scoring_methods), len(input_objects)\n                )\n            )\n        elif len(scoring_methods) &gt; len(input_objects):\n            # this is a special case for convenience: pass in 1 set of objects and multiple methods,\n            # result is that set is used for all methods\n            logger.debug(\"Duplicating input object set\")\n            while len(input_objects) &lt; len(scoring_methods):\n                input_objects.append(input_objects[0])\n                logger.debug(\"Duplicating input object set\")\n\n    link_collection = LinkCollection(and_mode)\n\n    for i, method in enumerate(scoring_methods):\n        # do any one-off initialisation required by this method\n        if not self._scoring_methods_setup_complete[method.name]:\n            logger.debug(f\"Doing one-time setup for {method.name}\")\n            self._scoring_methods[method.name].setup(self)\n            self._scoring_methods_setup_complete[method.name] = True\n\n        # should construct a dict of {object_with_link: &lt;link_data&gt;}\n        # entries\n        objects_for_method = input_objects[i]\n        logger.debug(\n            \"Calling scoring method {} on {} objects\".format(\n                method.name, len(objects_for_method)\n            )\n        )\n        link_collection = method.get_links(*objects_for_method, link_collection=link_collection)\n\n    if not self._datalinks:\n        logger.debug(\"Creating internal datalinks object\")\n        self._datalinks = self.scoring_method(MetcalfScoring.NAME).datalinks\n        logger.debug(\"Created internal datalinks object\")\n\n    if len(link_collection) == 0:\n        logger.debug(\"No links found or remaining after merging all method results!\")\n\n    # populate shared strain info\n    logger.debug(\"Calculating shared strain information...\")\n    # TODO more efficient version?\n    for source, link_data in link_collection.links.items():\n        if isinstance(source, BGC):\n            logger.debug(\"Cannot determine shared strains for BGC input!\")\n            break\n\n        targets = list(filter(lambda x: not isinstance(x, BGC), link_data.keys()))\n        if len(targets) &gt; 0:\n            if isinstance(source, GCF):\n                shared_strains = self._datalinks.get_common_strains(targets, [source], True)\n                for target, link in link_data.items():\n                    if (target, source) in shared_strains:\n                        link.shared_strains = shared_strains[(target, source)]\n            else:\n                shared_strains = self._datalinks.get_common_strains([source], targets, True)\n                for target, link in link_data.items():\n                    if (source, target) in shared_strains:\n                        link.shared_strains = shared_strains[(source, target)]\n\n    logger.debug(\"Finished calculating shared strain information\")\n\n    logger.debug(\"Final size of link collection is {}\".format(len(link_collection)))\n    return link_collection\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.get_common_strains","title":"get_common_strains","text":"<pre><code>get_common_strains(met: Sequence[Spectrum] | Sequence[MolecularFamily], gcfs: Sequence[GCF], filter_no_shared: bool = True) -&gt; dict[tuple[Spectrum | MolecularFamily, GCF], list[Strain]]\n</code></pre> <p>Get common strains between given spectra/molecular families and GCFs.</p> <p>Parameters:</p> Name Type Description Default <code>met</code> <code>Sequence[Spectrum] | Sequence[MolecularFamily]</code> <p>A list of Spectrum or MolecularFamily objects.</p> required <code>gcfs</code> <code>Sequence[GCF]</code> <p>A list of GCF objects.</p> required <code>filter_no_shared</code> <code>bool</code> <p>If True, the pairs of spectrum/mf and GCF without common strains will be removed from the returned dict;</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[tuple[Spectrum | MolecularFamily, GCF], list[Strain]]</code> <p>A dict where the keys are tuples of (Spectrum/MolecularFamily, GCF)</p> <code>dict[tuple[Spectrum | MolecularFamily, GCF], list[Strain]]</code> <p>and values are a list of shared Strain objects.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def get_common_strains(\n    self,\n    met: Sequence[Spectrum] | Sequence[MolecularFamily],\n    gcfs: Sequence[GCF],\n    filter_no_shared: bool = True,\n) -&gt; dict[tuple[Spectrum | MolecularFamily, GCF], list[Strain]]:\n    \"\"\"Get common strains between given spectra/molecular families and GCFs.\n\n    Args:\n        met:\n            A list of Spectrum or MolecularFamily objects.\n        gcfs: A list of GCF objects.\n        filter_no_shared: If True, the pairs of spectrum/mf and GCF\n            without common strains will be removed from the returned dict;\n\n    Returns:\n        A dict where the keys are tuples of (Spectrum/MolecularFamily, GCF)\n        and values are a list of shared Strain objects.\n    \"\"\"\n    if not self._datalinks:\n        self._datalinks = self.scoring_method(MetcalfScoring.NAME).datalinks\n    common_strains = self._datalinks.get_common_strains(met, gcfs, filter_no_shared)\n    return common_strains\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.has_bgc","title":"has_bgc","text":"<pre><code>has_bgc(bgc_id)\n</code></pre> <p>Returns True if BGC <code>bgc_id</code> exists in the dataset.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def has_bgc(self, bgc_id):\n    \"\"\"Returns True if BGC ``bgc_id`` exists in the dataset.\"\"\"\n    return bgc_id in self._bgc_lookup\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.lookup_bgc","title":"lookup_bgc","text":"<pre><code>lookup_bgc(bgc_id)\n</code></pre> <p>If BGC <code>bgc_id</code> exists, return it. Otherwise return None.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def lookup_bgc(self, bgc_id):\n    \"\"\"If BGC ``bgc_id`` exists, return it. Otherwise return None.\"\"\"\n    return self._bgc_lookup.get(bgc_id, None)\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.lookup_gcf","title":"lookup_gcf","text":"<pre><code>lookup_gcf(gcf_id)\n</code></pre> <p>If GCF <code>gcf_id</code> exists, return it. Otherwise return None.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def lookup_gcf(self, gcf_id):\n    \"\"\"If GCF ``gcf_id`` exists, return it. Otherwise return None.\"\"\"\n    return self._gcf_lookup.get(gcf_id, None)\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.lookup_spectrum","title":"lookup_spectrum","text":"<pre><code>lookup_spectrum(spectrum_id)\n</code></pre> <p>If Spectrum <code>name</code> exists, return it. Otherwise return None.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def lookup_spectrum(self, spectrum_id):\n    \"\"\"If Spectrum ``name`` exists, return it. Otherwise return None.\"\"\"\n    return self._spec_lookup.get(spectrum_id, None)\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.lookup_mf","title":"lookup_mf","text":"<pre><code>lookup_mf(mf_id)\n</code></pre> <p>If MolecularFamily <code>family_id</code> exists, return it. Otherwise return None.</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def lookup_mf(self, mf_id):\n    \"\"\"If MolecularFamily `family_id` exists, return it. Otherwise return None.\"\"\"\n    return self._mf_lookup.get(mf_id, None)\n</code></pre>"},{"location":"api/nplinker/#nplinker.nplinker.NPLinker.scoring_method","title":"scoring_method","text":"<pre><code>scoring_method(name: str) -&gt; ScoringMethod | None\n</code></pre> <p>Return an instance of a scoring method.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the method (see :func:<code>scoring_methods</code>)</p> required <p>Returns:</p> Type Description <code>ScoringMethod | None</code> <p>An instance of the named scoring method class, or None if the name is invalid</p> Source code in <code>src/nplinker/nplinker.py</code> <pre><code>def scoring_method(self, name: str) -&gt; ScoringMethod | None:\n    \"\"\"Return an instance of a scoring method.\n\n    Args:\n        name: the name of the method (see :func:`scoring_methods`)\n\n    Returns:\n        An instance of the named scoring method class, or None if the name is invalid\n    \"\"\"\n    if name not in self._scoring_methods_setup_complete:\n        return None\n\n    if not self._scoring_methods_setup_complete[name]:\n        self._scoring_methods[name].setup(self)\n        self._scoring_methods_setup_complete[name] = True\n\n    return self._scoring_methods.get(name, None)(self)\n</code></pre>"},{"location":"api/schema/","title":"Schemas","text":""},{"location":"api/schema/#nplinker.schemas","title":"schemas","text":""},{"location":"api/schema/#nplinker.schemas.validate_podp_json","title":"validate_podp_json","text":"<pre><code>validate_podp_json(json_data: dict) -&gt; None\n</code></pre> <p>Validate a dictionary of JSON data against the PODP JSON schema.</p> <p>All validation error messages are collected and raised as a single ValueError.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>dict</code> <p>The JSON data to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the JSON data does not match the schema.</p> Source code in <code>src/nplinker/schemas/utils.py</code> <pre><code>def validate_podp_json(json_data: dict) -&gt; None:\n    \"\"\"Validate a dictionary of JSON data against the PODP JSON schema.\n\n    All validation error messages are collected and raised as a single\n    ValueError.\n\n    Parameters:\n        json_data: The JSON data to validate.\n\n    Raises:\n        ValueError: If the JSON data does not match the schema.\n    \"\"\"\n    validator = Draft7Validator(PODP_ADAPTED_SCHEMA)\n    errors = sorted(validator.iter_errors(json_data), key=lambda e: e.path)\n    if errors:\n        error_messages = [f\"{e.json_path}: {e.message}\" for e in errors]\n        raise ValueError(\n            \"Not match PODP adapted schema, here are the detailed error:\\n  - \"\n            + \"\\n  - \".join(error_messages)\n        )\n</code></pre>"},{"location":"api/scoring/","title":"Scoring","text":""},{"location":"api/scoring/#nplinker.scoring","title":"scoring","text":""},{"location":"api/scoring/#nplinker.scoring.ScoringMethod","title":"ScoringMethod","text":"<pre><code>ScoringMethod(npl)\n</code></pre> <p>Base class of scoring methods.</p> Source code in <code>src/nplinker/scoring/methods.py</code> <pre><code>def __init__(self, npl):\n    self.npl = npl\n    self.name = self.__class__.NAME\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.ScoringMethod.setup","title":"setup  <code>staticmethod</code>","text":"<pre><code>setup(npl)\n</code></pre> <p>Perform any one-off initialisation required (will only be called once).</p> Source code in <code>src/nplinker/scoring/methods.py</code> <pre><code>@staticmethod\ndef setup(npl):\n    \"\"\"Perform any one-off initialisation required (will only be called once).\"\"\"\n    pass\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.ScoringMethod.get_links","title":"get_links","text":"<pre><code>get_links(*objects, link_collection: LinkCollection) -&gt; LinkCollection\n</code></pre> <p>Given a set of objects, return link information.</p> Source code in <code>src/nplinker/scoring/methods.py</code> <pre><code>def get_links(self, *objects, link_collection: LinkCollection) -&gt; LinkCollection:\n    \"\"\"Given a set of objects, return link information.\"\"\"\n    return link_collection\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.ScoringMethod.format_data","title":"format_data","text":"<pre><code>format_data(data)\n</code></pre> <p>Given whatever output data the method produces, return a readable string version.</p> Source code in <code>src/nplinker/scoring/methods.py</code> <pre><code>def format_data(self, data):\n    \"\"\"Given whatever output data the method produces, return a readable string version.\"\"\"\n    return \"\"\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.ScoringMethod.sort","title":"sort","text":"<pre><code>sort(objects, reverse=True)\n</code></pre> <p>Given a list of objects, return them sorted by link score.</p> Source code in <code>src/nplinker/scoring/methods.py</code> <pre><code>def sort(self, objects, reverse=True):\n    \"\"\"Given a list of objects, return them sorted by link score.\"\"\"\n    return objects\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.MetcalfScoring","title":"MetcalfScoring","text":"<pre><code>MetcalfScoring(npl: NPLinker)\n</code></pre> <p>             Bases: <code>ScoringMethod</code></p> <p>Metcalf scoring method.</p> <p>Attributes:</p> Name Type Description <code>DATALINKS</code> <p>The DataLinks object to use for scoring.</p> <code>LINKFINDER</code> <p>The LinkFinder object to use for scoring.</p> <code>NAME</code> <p>The name of the scoring method. This is set to 'metcalf'.</p> <p>Create a MetcalfScoring object.</p> <p>Parameters:</p> Name Type Description Default <code>npl</code> <code>NPLinker</code> <p>The NPLinker object to use for scoring.</p> required <p>Attributes:</p> Name Type Description <code>cutoff</code> <p>The cutoff value to use for scoring. Scores below this value will be discarded. Defaults to 1.0.</p> <code>standardised</code> <p>Whether to use standardised scores. Defaults to True.</p> <code>name</code> <p>The name of the scoring method. It's set to a fixed value 'metcalf'.</p> Source code in <code>src/nplinker/scoring/metcalf_scoring.py</code> <pre><code>def __init__(self, npl: NPLinker) -&gt; None:\n    \"\"\"Create a MetcalfScoring object.\n\n    Args:\n        npl: The NPLinker object to use for scoring.\n\n    Attributes:\n        cutoff: The cutoff value to use for scoring. Scores below\n            this value will be discarded. Defaults to 1.0.\n        standardised: Whether to use standardised scores. Defaults\n            to True.\n        name: The name of the scoring method. It's set to a fixed value\n            'metcalf'.\n    \"\"\"\n    super().__init__(npl)\n    self.cutoff = 1.0\n    self.standardised = True\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.MetcalfScoring.setup","title":"setup  <code>staticmethod</code>","text":"<pre><code>setup(npl: NPLinker)\n</code></pre> <p>Setup the MetcalfScoring object.</p> <p>DataLinks and LinkFinder objects are created and cached for later use.</p> Source code in <code>src/nplinker/scoring/metcalf_scoring.py</code> <pre><code>@staticmethod\ndef setup(npl: NPLinker):\n    \"\"\"Setup the MetcalfScoring object.\n\n    DataLinks and LinkFinder objects are created and cached for later use.\n    \"\"\"\n    logger.info(\n        \"MetcalfScoring.setup (bgcs={}, gcfs={}, spectra={}, molfams={}, strains={})\".format(\n            len(npl.bgcs), len(npl.gcfs), len(npl.spectra), len(npl.molfams), len(npl.strains)\n        )\n    )\n\n    cache_dir = os.path.join(npl.root_dir, \"metcalf\")\n    cache_file = os.path.join(cache_dir, \"metcalf_scores.pckl\")\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # the metcalf preprocessing can take a long time for large datasets, so it's\n    # better to cache as the data won't change unless the number of objects does\n    dataset_counts = [\n        len(npl.bgcs),\n        len(npl.gcfs),\n        len(npl.spectra),\n        len(npl.molfams),\n        len(npl.strains),\n    ]\n    datalinks, linkfinder = None, None\n    if os.path.exists(cache_file):\n        logger.debug(\"MetcalfScoring.setup loading cached data\")\n        cache_data = load_pickled_data(npl, cache_file)\n        cache_ok = True\n        if cache_data is not None:\n            (counts, datalinks, linkfinder) = cache_data\n            # need to invalidate this if dataset appears to have changed\n            for i in range(len(counts)):\n                if counts[i] != dataset_counts[i]:\n                    logger.info(\"MetcalfScoring.setup invalidating cached data!\")\n                    cache_ok = False\n                    break\n\n        if cache_ok:\n            MetcalfScoring.DATALINKS = datalinks\n            MetcalfScoring.LINKFINDER = linkfinder\n\n    if MetcalfScoring.DATALINKS is None:\n        logger.info(\"MetcalfScoring.setup preprocessing dataset (this may take some time)\")\n        MetcalfScoring.DATALINKS = DataLinks(npl.gcfs, npl.spectra, npl.molfams, npl.strains)\n        MetcalfScoring.LINKFINDER = LinkFinder()\n        MetcalfScoring.LINKFINDER.calc_score(MetcalfScoring.DATALINKS, link_type=LINK_TYPES[0])\n        MetcalfScoring.LINKFINDER.calc_score(MetcalfScoring.DATALINKS, link_type=LINK_TYPES[1])\n        logger.debug(\"MetcalfScoring.setup caching results\")\n        save_pickled_data(\n            (dataset_counts, MetcalfScoring.DATALINKS, MetcalfScoring.LINKFINDER), cache_file\n        )\n\n    logger.info(\"MetcalfScoring.setup completed\")\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.MetcalfScoring.get_links","title":"get_links","text":"<pre><code>get_links(*objects: GCF | Spectrum | MolecularFamily, link_collection: LinkCollection) -&gt; LinkCollection\n</code></pre> <p>Get links for the given objects and add them to the given LinkCollection.</p> <p>The given objects are treated as input or source objects, which must be GCF, Spectrum or MolecularFamily objects.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>GCF | Spectrum | MolecularFamily</code> <p>The objects to get links for. Must be GCF, Spectrum or MolecularFamily objects.</p> <code>()</code> <code>link_collection</code> <code>LinkCollection</code> <p>The LinkCollection object to add the links to.</p> required <p>Returns:</p> Type Description <code>LinkCollection</code> <p>The LinkCollection object with the new links added.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input objects are empty.</p> <code>TypeError</code> <p>If the input objects are not of the correct type.</p> <code>ValueError</code> <p>If LinkFinder instance has not been created (MetcalfScoring object has not been setup).</p> Source code in <code>src/nplinker/scoring/metcalf_scoring.py</code> <pre><code>def get_links(\n    self, *objects: GCF | Spectrum | MolecularFamily, link_collection: LinkCollection\n) -&gt; LinkCollection:\n    \"\"\"Get links for the given objects and add them to the given LinkCollection.\n\n    The given objects are treated as input or source objects, which must\n    be GCF, Spectrum or MolecularFamily objects.\n\n    Args:\n        objects: The objects to get links for. Must be GCF, Spectrum\n            or MolecularFamily objects.\n        link_collection: The LinkCollection object to add the links to.\n\n    Returns:\n        The LinkCollection object with the new links added.\n\n    Raises:\n        ValueError: If the input objects are empty.\n        TypeError: If the input objects are not of the correct type.\n        ValueError: If LinkFinder instance has not been created\n            (MetcalfScoring object has not been setup).\n    \"\"\"\n    if len(objects) == 0:\n        raise ValueError(\"Empty input objects.\")\n\n    if isinstance_all(*objects, objtype=GCF):\n        obj_type = \"gcf\"\n    elif isinstance_all(*objects, objtype=Spectrum):\n        obj_type = \"spec\"\n    elif isinstance_all(*objects, objtype=MolecularFamily):\n        obj_type = \"mf\"\n    else:\n        types = [type(i) for i in objects]\n        raise TypeError(\n            f\"Invalid type {set(types)}. Input objects must be GCF, Spectrum or MolecularFamily objects.\"\n        )\n\n    if self.LINKFINDER is None:\n        raise ValueError(\n            (\"LinkFinder object not found. Have you called `MetcalfScoring.setup(npl)`?\")\n        )\n\n    logger.debug(f\"MetcalfScoring: standardised = {self.standardised}\")\n    if not self.standardised:\n        scores_list = self.LINKFINDER.get_links(*objects, score_cutoff=self.cutoff)\n    # TODO CG: verify the logics of standardised score and add unit tests\n    else:\n        # use negative infinity as the score cutoff to ensure we get all links\n        # the self.cutoff will be applied later in the postprocessing step\n        scores_list = self.LINKFINDER.get_links(*objects, score_cutoff=np.NINF)\n        if obj_type == \"gcf\":\n            scores_list = self._calc_standardised_score_gen(self.LINKFINDER, scores_list)\n        else:\n            scores_list = self._calc_standardised_score_met(self.LINKFINDER, scores_list)\n\n    link_scores: dict[\n        GCF | Spectrum | MolecularFamily, dict[GCF | Spectrum | MolecularFamily, ObjectLink]\n    ] = {}\n    if obj_type == \"gcf\":\n        logger.debug(\n            f\"MetcalfScoring: input_type=GCF, result_type=Spec/MolFam, \"\n            f\"#inputs={len(objects)}.\"\n        )\n        for scores in scores_list:\n            # when no links found\n            if scores.shape[1] == 0:\n                logger.debug(f'MetcalfScoring: found no \"{scores.name}\" links')\n            else:\n                # when links found\n                for col_index in range(scores.shape[1]):\n                    gcf = self.npl.lookup_gcf(scores.loc[\"source\", col_index])\n                    if scores.name == LINK_TYPES[0]:\n                        met = self.npl.lookup_spectrum(scores.loc[\"target\", col_index])\n                    else:\n                        met = self.npl.lookup_mf(scores.loc[\"target\", col_index])\n                    if gcf not in link_scores:\n                        link_scores[gcf] = {}\n                    # TODO CG: use id instead of object for gcf, met and self?\n                    link_scores[gcf][met] = ObjectLink(\n                        gcf, met, self, scores.loc[\"score\", col_index]\n                    )\n                logger.debug(f\"MetcalfScoring: found {len(link_scores)} {scores.name} links.\")\n    else:\n        logger.debug(\n            f\"MetcalfScoring: input_type=Spec/MolFam, result_type=GCF, \"\n            f\"#inputs={len(objects)}.\"\n        )\n        scores = scores_list[0]\n        # when no links found\n        if scores.shape[1] == 0:\n            logger.debug(f'MetcalfScoring: found no links \"{scores.name}\" for input objects')\n        else:\n            for col_index in range(scores.shape[1]):\n                gcf = self.npl.lookup_gcf(scores.loc[\"target\", col_index])\n                if scores.name == LINK_TYPES[0]:\n                    met = self.npl.lookup_spectrum(scores.loc[\"source\", col_index])\n                else:\n                    met = self.npl.lookup_mf(scores.loc[\"source\", col_index])\n                if met not in link_scores:\n                    link_scores[met] = {}\n                link_scores[met][gcf] = ObjectLink(\n                    met, gcf, self, scores.loc[\"score\", col_index]\n                )\n            logger.debug(f\"MetcalfScoring: found {len(link_scores)} {scores.name} links.\")\n\n    link_collection._add_links_from_method(self, link_scores)\n    logger.debug(\"MetcalfScoring: completed\")\n    return link_collection\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.LinkCollection","title":"LinkCollection","text":"<pre><code>LinkCollection(and_mode=True)\n</code></pre> <p>Class which stores the results of running one or more scoring methods.</p> <p>It provides access to the set of objects which were found to have links, the set of objects linked to each of those objects, and the information produced by the scoring method(s) about each link.</p> <p>There are also some useful utility methods to filter the original results.</p> Source code in <code>src/nplinker/scoring/link_collection.py</code> <pre><code>def __init__(self, and_mode=True):\n    self._methods = set()\n    self._link_data = {}\n    self._targets = {}\n    self._and_mode = and_mode\n</code></pre>"},{"location":"api/scoring/#nplinker.scoring.ObjectLink","title":"ObjectLink","text":"<pre><code>ObjectLink(source, target, method, data=None, shared_strains=[])\n</code></pre> <p>Class which stores information about a single link between two objects.</p> <p>There will be at most one instance of an ObjectLink for a given pair of objects (source, target) after running 1 or more scoring methods. Some methods, e.g. Metcalf, will always produce a single output per link. However other methods like Rosetta may find multiple \"hits\" for a given pair. In either case the data for a given method is associated with the ObjectLink so it can be retrieved afterwards.</p> The information stored is basically <ul> <li>the \"source\" of the link (original object provided as part of the input)</li> <li>the \"target\" of the link (linked object, as determined by the method(s) used)</li> <li>a (possibly empty) list of Strain objects shared between source and target</li> <li>the output of the scoring method(s) used for this link (e.g. a metcalf score)</li> </ul> Source code in <code>src/nplinker/scoring/object_link.py</code> <pre><code>def __init__(self, source, target, method, data=None, shared_strains=[]):\n    self.source = source\n    self.target = target\n    self.shared_strains = shared_strains\n    self._method_data = {method: data}\n</code></pre>"},{"location":"api/strain/","title":"Data Models","text":""},{"location":"api/strain/#nplinker.strain","title":"strain","text":""},{"location":"api/strain/#nplinker.strain.Strain","title":"Strain","text":"<pre><code>Strain(primary_id: str)\n</code></pre> <p>To model the mapping between strain id and its aliases.</p> <p>It's recommended to use NCBI taxonomy strain id or name as the primary id.</p> <p>Parameters:</p> Name Type Description Default <code>primary_id</code> <code>str</code> <p>the representative id of the strain.</p> required Source code in <code>src/nplinker/strain/strain.py</code> <pre><code>def __init__(self, primary_id: str) -&gt; None:\n    \"\"\"To model the mapping between strain id and its aliases.\n\n    It's recommended to use NCBI taxonomy strain id or name as the primary\n    id.\n\n    Args:\n        primary_id: the representative id of the strain.\n    \"\"\"\n    self.id: str = primary_id\n    self._aliases: set[str] = set()\n</code></pre>"},{"location":"api/strain/#nplinker.strain.Strain.names","title":"names  <code>property</code>","text":"<pre><code>names: set[str]\n</code></pre> <p>Get the set of strain names including id and aliases.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: A set of names associated with the strain.</p>"},{"location":"api/strain/#nplinker.strain.Strain.aliases","title":"aliases  <code>property</code>","text":"<pre><code>aliases: set[str]\n</code></pre> <p>Get the set of known aliases.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: A set of aliases associated with the strain.</p>"},{"location":"api/strain/#nplinker.strain.Strain.add_alias","title":"add_alias","text":"<pre><code>add_alias(alias: str) -&gt; None\n</code></pre> <p>Add an alias to the list of known aliases.</p> <p>Parameters:</p> Name Type Description Default <code>alias</code> <code>str</code> <p>The alias to add to the list of known aliases.</p> required Source code in <code>src/nplinker/strain/strain.py</code> <pre><code>def add_alias(self, alias: str) -&gt; None:\n    \"\"\"Add an alias to the list of known aliases.\n\n    Args:\n        alias: The alias to add to the list of known aliases.\n    \"\"\"\n    if not isinstance(alias, str):\n        raise TypeError(f\"Expected str, got {type(alias)}\")\n    if len(alias) == 0:\n        logger.warning(\"Refusing to add an empty-string alias to strain {%s}\", self)\n    else:\n        self._aliases.add(alias)\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection","title":"StrainCollection","text":"<pre><code>StrainCollection()\n</code></pre> <p>A collection of Strain objects.</p> Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def __init__(self):\n    \"\"\"A collection of Strain objects.\"\"\"\n    # the order of strains is needed for scoring part, so use a list\n    self._strains: list[Strain] = []\n    self._strain_dict_name: dict[str, list[Strain]] = {}\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.add","title":"add","text":"<pre><code>add(strain: Strain) -&gt; None\n</code></pre> <p>Add strain to the collection.</p> <p>If the strain already exists, merge the aliases.</p> <p>Parameters:</p> Name Type Description Default <code>strain</code> <code>Strain</code> <p>The strain to add.</p> required Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def add(self, strain: Strain) -&gt; None:\n    \"\"\"Add strain to the collection.\n\n    If the strain already exists, merge the aliases.\n\n    Args:\n        strain: The strain to add.\n    \"\"\"\n    if strain in self._strains:\n        # only one strain object per id\n        strain_ref = self._strain_dict_name[strain.id][0]\n        new_aliases = [alias for alias in strain.aliases if alias not in strain_ref.aliases]\n        for alias in new_aliases:\n            strain_ref.add_alias(alias)\n            if alias not in self._strain_dict_name:\n                self._strain_dict_name[alias] = [strain_ref]\n            else:\n                self._strain_dict_name[alias].append(strain_ref)\n    else:\n        self._strains.append(strain)\n        for name in strain.names:\n            if name not in self._strain_dict_name:\n                self._strain_dict_name[name] = [strain]\n            else:\n                self._strain_dict_name[name].append(strain)\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.remove","title":"remove","text":"<pre><code>remove(strain: Strain)\n</code></pre> <p>Remove a strain from the collection.</p> <p>It removes the given strain object from the collection by strain id. If the strain id is not found, raise ValueError.</p> <p>Parameters:</p> Name Type Description Default <code>strain</code> <code>Strain</code> <p>The strain to remove.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the strain is not found in the collection.</p> Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def remove(self, strain: Strain):\n    \"\"\"Remove a strain from the collection.\n\n    It removes the given strain object from the collection by strain id.\n    If the strain id is not found, raise ValueError.\n\n    Args:\n        strain: The strain to remove.\n\n    Raises:\n        ValueError: If the strain is not found in the collection.\n    \"\"\"\n    if strain in self._strains:\n        self._strains.remove(strain)\n        # only one strain object per id\n        strain_ref = self._strain_dict_name[strain.id][0]\n        for name in strain_ref.names:\n            if name in self._strain_dict_name:\n                new_strain_list = [s for s in self._strain_dict_name[name] if s.id != strain.id]\n                if not new_strain_list:\n                    del self._strain_dict_name[name]\n                else:\n                    self._strain_dict_name[name] = new_strain_list\n    else:\n        raise ValueError(f\"Strain {strain} not found in strain collection.\")\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.filter","title":"filter","text":"<pre><code>filter(strain_set: set[Strain])\n</code></pre> <p>Remove all strains that are not in strain_set from the strain collection.</p> <p>Parameters:</p> Name Type Description Default <code>strain_set</code> <code>set[Strain]</code> <p>Set of strains to keep.</p> required Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def filter(self, strain_set: set[Strain]):\n    \"\"\"Remove all strains that are not in strain_set from the strain collection.\n\n    Args:\n        strain_set: Set of strains to keep.\n    \"\"\"\n    # note that we need to copy the list of strains, as we are modifying it\n    for strain in self._strains.copy():\n        if strain not in strain_set:\n            self.remove(strain)\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.has_name","title":"has_name","text":"<pre><code>has_name(name: str) -&gt; bool\n</code></pre> <p>Check if the strain collection contains the given strain name (id or alias).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Strain name (id or alias) to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the strain name is in the collection, False otherwise.</p> Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def has_name(self, name: str) -&gt; bool:\n    \"\"\"Check if the strain collection contains the given strain name (id or alias).\n\n    Args:\n        name: Strain name (id or alias) to check.\n\n    Returns:\n        True if the strain name is in the collection, False otherwise.\n    \"\"\"\n    return name in self._strain_dict_name\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.lookup","title":"lookup","text":"<pre><code>lookup(name: str) -&gt; list[Strain]\n</code></pre> <p>Lookup a strain by name (id or alias).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Strain name (id or alias) to lookup.</p> required <p>Returns:</p> Type Description <code>list[Strain]</code> <p>List of Strain objects with the given name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the strain name is not found.</p> Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def lookup(self, name: str) -&gt; list[Strain]:\n    \"\"\"Lookup a strain by name (id or alias).\n\n    Args:\n        name: Strain name (id or alias) to lookup.\n\n    Returns:\n        List of Strain objects with the given name.\n\n    Raises:\n        ValueError: If the strain name is not found.\n    \"\"\"\n    if name in self._strain_dict_name:\n        return self._strain_dict_name[name]\n    raise ValueError(f\"Strain {name} not found in the strain collection.\")\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.read_json","title":"read_json  <code>staticmethod</code>","text":"<pre><code>read_json(file: str | PathLike) -&gt; 'StrainCollection'\n</code></pre> <p>Read a strain mappings JSON file and return a StrainCollection object.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to the strain mappings JSON file.</p> required <p>Returns:</p> Type Description <code>'StrainCollection'</code> <p>StrainCollection object.</p> Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>@staticmethod\ndef read_json(file: str | PathLike) -&gt; \"StrainCollection\":\n    \"\"\"Read a strain mappings JSON file and return a StrainCollection object.\n\n    Args:\n        file: Path to the strain mappings JSON file.\n\n    Returns:\n        StrainCollection object.\n    \"\"\"\n    with open(file, \"r\") as f:\n        json_data = json.load(f)\n\n    # validate json data\n    validate(instance=json_data, schema=STRAIN_MAPPINGS_SCHEMA)\n\n    strain_collection = StrainCollection()\n    for data in json_data[\"strain_mappings\"]:\n        strain = Strain(data[\"strain_id\"])\n        for alias in data[\"strain_alias\"]:\n            strain.add_alias(alias)\n        strain_collection.add(strain)\n    return strain_collection\n</code></pre>"},{"location":"api/strain/#nplinker.strain.StrainCollection.to_json","title":"to_json","text":"<pre><code>to_json(file: str | PathLike | None = None) -&gt; str | None\n</code></pre> <p>Convert the StrainCollection object to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike | None</code> <p>Path to output JSON file. If None, return the JSON string instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>If <code>file</code> is None, return the JSON string. Otherwise, write the JSON string to the given</p> <code>str | None</code> <p>file.</p> Source code in <code>src/nplinker/strain/strain_collection.py</code> <pre><code>def to_json(self, file: str | PathLike | None = None) -&gt; str | None:\n    \"\"\"Convert the StrainCollection object to a JSON string.\n\n    Args:\n        file: Path to output JSON file. If None,\n            return the JSON string instead.\n\n    Returns:\n        If `file` is None, return the JSON string. Otherwise, write the JSON string to the given\n        file.\n    \"\"\"\n    data_list = [\n        {\"strain_id\": strain.id, \"strain_alias\": list(strain.aliases)} for strain in self\n    ]\n    json_data = {\"strain_mappings\": data_list, \"version\": \"1.0\"}\n\n    # validate json data\n    validate(instance=json_data, schema=STRAIN_MAPPINGS_SCHEMA)\n\n    if file is not None:\n        with open(file, \"w\") as f:\n            json.dump(json_data, f)\n        return None\n    return json.dumps(json_data)\n</code></pre>"},{"location":"api/strain_utils/","title":"Utilities","text":""},{"location":"api/strain_utils/#nplinker.strain.utils","title":"utils","text":""},{"location":"api/strain_utils/#nplinker.strain.utils.load_user_strains","title":"load_user_strains","text":"<pre><code>load_user_strains(json_file: str | PathLike) -&gt; set[Strain]\n</code></pre> <p>Load user specified strains from a JSON file.</p> <p>The JSON file must follow the schema defined in <code>schemas/user_strains.json</code>.</p> An example content of the JSON file <pre><code>{\"strain_ids\": [\"strain1\", \"strain2\"]}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>json_file</code> <code>str | PathLike</code> <p>Path to the JSON file containing user specified strains.</p> required <p>Returns:</p> Type Description <code>set[Strain]</code> <p>set[Strain]: A set of user specified strains.</p> Source code in <code>src/nplinker/strain/utils.py</code> <pre><code>def load_user_strains(json_file: str | PathLike) -&gt; set[Strain]:\n    \"\"\"Load user specified strains from a JSON file.\n\n    The JSON file must follow the schema defined in `schemas/user_strains.json`.\n\n    An example content of the JSON file:\n        ```\n        {\"strain_ids\": [\"strain1\", \"strain2\"]}\n        ```\n\n    Args:\n        json_file: Path to the JSON file containing user specified strains.\n\n    Returns:\n        set[Strain]: A set of user specified strains.\n    \"\"\"\n    with open(json_file, \"r\") as f:\n        json_data = json.load(f)\n\n    # validate json data\n    validate(instance=json_data, schema=USER_STRAINS_SCHEMA)\n\n    strains = set()\n    for strain_id in json_data[\"strain_ids\"]:\n        strains.add(Strain(strain_id))\n\n    return strains\n</code></pre>"},{"location":"api/strain_utils/#nplinker.strain.utils.podp_generate_strain_mappings","title":"podp_generate_strain_mappings","text":"<pre><code>podp_generate_strain_mappings(podp_project_json_file: str | PathLike, genome_status_json_file: str | PathLike, genome_bgc_mappings_file: str | PathLike, gnps_file_mappings_file: str | PathLike, output_json_file: str | PathLike) -&gt; StrainCollection\n</code></pre> <p>Generate strain mappings JSON file for PODP pipeline.</p> <p>To get the strain mappings, we need to combine the following mappings:</p> <ul> <li>strain_id &lt;-&gt; original_genome_id &lt;-&gt; resolved_genome_id &lt;-&gt; bgc_id</li> <li>strain_id &lt;-&gt; MS_filename &lt;-&gt; spectrum_id</li> </ul> <p>These mappings are extracted from the following files:</p> <ul> <li>\"strain_id &lt;-&gt; original_genome_id\" is extracted from <code>podp_project_json_file</code>.</li> <li>\"original_genome_id &lt;-&gt; resolved_genome_id\" is extracted from <code>genome_status_json_file</code>.</li> <li>\"resolved_genome_id &lt;-&gt; bgc_id\" is extracted from <code>genome_bgc_mappings_file</code>.</li> <li>\"strain_id &lt;-&gt; MS_filename\" is extracted from <code>podp_project_json_file</code>.</li> <li>\"MS_filename &lt;-&gt; spectrum_id\" is extracted from <code>gnps_file_mappings_file</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>podp_project_json_file</code> <code>str | PathLike</code> <p>The path to the PODP project JSON file.</p> required <code>genome_status_json_file</code> <code>str | PathLike</code> <p>The path to the genome status JSON file.</p> required <code>genome_bgc_mappings_file</code> <code>str | PathLike</code> <p>The path to the genome BGC mappings JSON file.</p> required <code>gnps_file_mappings_file</code> <code>str | PathLike</code> <p>The path to the GNPS file mappings file (csv or tsv).</p> required <code>output_json_file</code> <code>str | PathLike</code> <p>The path to the output JSON file.</p> required <p>Returns:</p> Type Description <code>StrainCollection</code> <p>The strain mappings stored in a StrainCollection object.</p> See Also <ul> <li><code>extract_mappings_strain_id_original_genome_id</code>: Extract mappings     \"strain_id &lt;-&gt; original_genome_id\".</li> <li><code>extract_mappings_original_genome_id_resolved_genome_id</code>: Extract mappings     \"original_genome_id &lt;-&gt; resolved_genome_id\".</li> <li><code>extract_mappings_resolved_genome_id_bgc_id</code>: Extract mappings     \"resolved_genome_id &lt;-&gt; bgc_id\".</li> <li><code>get_mappings_strain_id_bgc_id</code>: Get mappings \"strain_id &lt;-&gt; bgc_id\".</li> <li><code>extract_mappings_strain_id_ms_filename</code>: Extract mappings     \"strain_id &lt;-&gt; MS_filename\".</li> <li><code>extract_mappings_ms_filename_spectrum_id</code>: Extract mappings     \"MS_filename &lt;-&gt; spectrum_id\".</li> <li><code>get_mappings_strain_id_spectrum_id</code>: Get mappings \"strain_id &lt;-&gt; spectrum_id\".</li> </ul> Source code in <code>src/nplinker/strain/utils.py</code> <pre><code>def podp_generate_strain_mappings(\n    podp_project_json_file: str | PathLike,\n    genome_status_json_file: str | PathLike,\n    genome_bgc_mappings_file: str | PathLike,\n    gnps_file_mappings_file: str | PathLike,\n    output_json_file: str | PathLike,\n) -&gt; StrainCollection:\n    \"\"\"Generate strain mappings JSON file for PODP pipeline.\n\n    To get the strain mappings, we need to combine the following mappings:\n\n    - strain_id &lt;-&gt; original_genome_id &lt;-&gt; resolved_genome_id &lt;-&gt; bgc_id\n    - strain_id &lt;-&gt; MS_filename &lt;-&gt; spectrum_id\n\n    These mappings are extracted from the following files:\n\n    - \"strain_id &lt;-&gt; original_genome_id\" is extracted from `podp_project_json_file`.\n    - \"original_genome_id &lt;-&gt; resolved_genome_id\" is extracted from `genome_status_json_file`.\n    - \"resolved_genome_id &lt;-&gt; bgc_id\" is extracted from `genome_bgc_mappings_file`.\n    - \"strain_id &lt;-&gt; MS_filename\" is extracted from `podp_project_json_file`.\n    - \"MS_filename &lt;-&gt; spectrum_id\" is extracted from `gnps_file_mappings_file`.\n\n    Args:\n        podp_project_json_file: The path to the PODP project\n            JSON file.\n        genome_status_json_file: The path to the genome status\n            JSON file.\n        genome_bgc_mappings_file: The path to the genome BGC\n            mappings JSON file.\n        gnps_file_mappings_file: The path to the GNPS file\n            mappings file (csv or tsv).\n        output_json_file: The path to the output JSON file.\n\n    Returns:\n        The strain mappings stored in a StrainCollection object.\n\n    See Also:\n        - `extract_mappings_strain_id_original_genome_id`: Extract mappings\n            \"strain_id &lt;-&gt; original_genome_id\".\n        - `extract_mappings_original_genome_id_resolved_genome_id`: Extract mappings\n            \"original_genome_id &lt;-&gt; resolved_genome_id\".\n        - `extract_mappings_resolved_genome_id_bgc_id`: Extract mappings\n            \"resolved_genome_id &lt;-&gt; bgc_id\".\n        - `get_mappings_strain_id_bgc_id`: Get mappings \"strain_id &lt;-&gt; bgc_id\".\n        - `extract_mappings_strain_id_ms_filename`: Extract mappings\n            \"strain_id &lt;-&gt; MS_filename\".\n        - `extract_mappings_ms_filename_spectrum_id`: Extract mappings\n            \"MS_filename &lt;-&gt; spectrum_id\".\n        - `get_mappings_strain_id_spectrum_id`: Get mappings \"strain_id &lt;-&gt; spectrum_id\".\n    \"\"\"\n    # Get mappings strain_id &lt;-&gt; original_geonme_id &lt;-&gt; resolved_genome_id &lt;-&gt; bgc_id\n    mappings_strain_id_bgc_id = get_mappings_strain_id_bgc_id(\n        extract_mappings_strain_id_original_genome_id(podp_project_json_file),\n        extract_mappings_original_genome_id_resolved_genome_id(genome_status_json_file),\n        extract_mappings_resolved_genome_id_bgc_id(genome_bgc_mappings_file),\n    )\n\n    # Get mappings strain_id &lt;-&gt; MS_filename &lt;-&gt; spectrum_id\n    mappings_strain_id_spectrum_id = get_mappings_strain_id_spectrum_id(\n        extract_mappings_strain_id_ms_filename(podp_project_json_file),\n        extract_mappings_ms_filename_spectrum_id(gnps_file_mappings_file),\n    )\n\n    # Get mappings strain_id &lt;-&gt; bgc_id / spectrum_id\n    mappings = mappings_strain_id_bgc_id.copy()\n    for strain_id, spectrum_ids in mappings_strain_id_spectrum_id.items():\n        if strain_id in mappings:\n            mappings[strain_id].update(spectrum_ids)\n        else:\n            mappings[strain_id] = spectrum_ids.copy()\n\n    # Create StrainCollection\n    sc = StrainCollection()\n    for strain_id, bgc_ids in mappings.items():\n        if not sc.has_name(strain_id):\n            strain = Strain(strain_id)\n            for bgc_id in bgc_ids:\n                strain.add_alias(bgc_id)\n            sc.add(strain)\n        else:\n            # strain_list has only one element\n            strain_list = sc.lookup(strain_id)\n            for bgc_id in bgc_ids:\n                strain_list[0].add_alias(bgc_id)\n\n    # Write strain mappings JSON file\n    sc.to_json(output_json_file)\n    logger.info(\"Generated strain mappings JSON file: %s\", output_json_file)\n\n    return sc\n</code></pre>"},{"location":"api/utils/","title":"General Utilities","text":""},{"location":"api/utils/#nplinker.utils","title":"utils","text":""},{"location":"api/utils/#nplinker.utils.download_and_extract_archive","title":"download_and_extract_archive","text":"<pre><code>download_and_extract_archive(url: str, download_root: str | PathLike, extract_root: str | Path | None = None, filename: str | None = None, md5: str | None = None, remove_finished: bool = False) -&gt; None\n</code></pre> <p>Download a file from url and extract it.</p> <p>This method is a wrapper of <code>download_url</code> and <code>extract_archive</code> methods.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to download file from</p> required <code>download_root</code> <code>str | PathLike</code> <p>Path to the directory to place downloaded file in. If it doesn't exist, it will be created.</p> required <code>extract_root</code> <code>str | Path | None</code> <p>Path to the directory the file will be extracted to. The given directory will be created if not exist. If omitted, the <code>download_root</code> is used.</p> <code>None</code> <code>filename</code> <code>str | None</code> <p>Name to save the downloaded file under. If None, use the basename of the URL</p> <code>None</code> <code>md5</code> <code>str | None</code> <p>MD5 checksum of the download. If None, do not check</p> <code>None</code> <code>remove_finished</code> <code>bool</code> <p>If <code>True</code>, remove the downloaded file  after the extraction. Defaults to False.</p> <code>False</code> Source code in <code>src/nplinker/utils.py</code> <pre><code>def download_and_extract_archive(\n    url: str,\n    download_root: str | PathLike,\n    extract_root: str | Path | None = None,\n    filename: str | None = None,\n    md5: str | None = None,\n    remove_finished: bool = False,\n) -&gt; None:\n    \"\"\"Download a file from url and extract it.\n\n       This method is a wrapper of `download_url` and `extract_archive` methods.\n\n    Args:\n        url: URL to download file from\n        download_root: Path to the directory to place downloaded\n            file in. If it doesn't exist, it will be created.\n        extract_root: Path to the directory the file\n            will be extracted to. The given directory will be created if not exist.\n            If omitted, the `download_root` is used.\n        filename: Name to save the downloaded file under.\n            If None, use the basename of the URL\n        md5: MD5 checksum of the download. If None, do not check\n        remove_finished: If `True`, remove the downloaded file\n             after the extraction. Defaults to False.\n    \"\"\"\n    download_root = Path(download_root)\n    if extract_root is None:\n        extract_root = download_root\n    else:\n        extract_root = Path(extract_root)\n    if not filename:\n        filename = Path(url).name\n\n    download_url(url, download_root, filename, md5)\n\n    archive = download_root / filename\n    print(f\"Extracting {archive} to {extract_root}\")\n    extract_archive(archive, extract_root, remove_finished=remove_finished)\n</code></pre>"},{"location":"api/utils/#nplinker.utils.download_url","title":"download_url","text":"<pre><code>download_url(url: str, root: str | PathLike, filename: str | None = None, md5: str | None = None, http_method: str = 'GET', allow_http_redirect: bool = True) -&gt; None\n</code></pre> <p>Download a file from a url and place it in root.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to download file from</p> required <code>root</code> <code>str | PathLike</code> <p>Directory to place downloaded file in. If it doesn't exist, it will be created.</p> required <code>filename</code> <code>str | None</code> <p>Name to save the file under. If None, use the basename of the URL.</p> <code>None</code> <code>md5</code> <code>str | None</code> <p>MD5 checksum of the download. If None, do not check.</p> <code>None</code> <code>http_method</code> <code>str</code> <p>HTTP request method, e.g. \"GET\", \"POST\". Defaults to \"GET\".</p> <code>'GET'</code> <code>allow_http_redirect</code> <code>bool</code> <p>If true, enable following redirects for all HTTP (\"http:\") methods.</p> <code>True</code> Source code in <code>src/nplinker/utils.py</code> <pre><code>def download_url(\n    url: str,\n    root: str | PathLike,\n    filename: str | None = None,\n    md5: str | None = None,\n    http_method: str = \"GET\",\n    allow_http_redirect: bool = True,\n) -&gt; None:\n    \"\"\"Download a file from a url and place it in root.\n\n    Args:\n        url: URL to download file from\n        root: Directory to place downloaded file in. If it doesn't exist, it will be created.\n        filename: Name to save the file under. If None, use the\n            basename of the URL.\n        md5: MD5 checksum of the download. If None, do not check.\n        http_method: HTTP request method, e.g. \"GET\", \"POST\".\n            Defaults to \"GET\".\n        allow_http_redirect: If true, enable following redirects for all HTTP (\"http:\") methods.\n    \"\"\"\n    root = transform_to_full_path(root)\n    # create the download directory if not exist\n    root.mkdir(exist_ok=True)\n    if not filename:\n        filename = Path(url).name\n    fpath = root / filename\n\n    # check if file is already present locally\n    if fpath.is_file() and md5 is not None and check_md5(fpath, md5):\n        print(\"Using downloaded and verified file: \" + str(fpath))\n        return\n\n    # download the file\n    with open(fpath, \"wb\") as fh:\n        with httpx.stream(http_method, url, follow_redirects=allow_http_redirect) as response:\n            if not response.is_success:\n                fpath.unlink(missing_ok=True)\n                raise RuntimeError(\n                    f\"Failed to download url {url} with status code {response.status_code}\"\n                )\n            total = int(response.headers.get(\"Content-Length\", 0))\n            with tqdm(total=total, unit_scale=True, unit_divisor=1024, unit=\"B\") as progress:\n                num_bytes_downloaded = response.num_bytes_downloaded\n                for chunk in response.iter_bytes():\n                    fh.write(chunk)\n                    progress.update(response.num_bytes_downloaded - num_bytes_downloaded)\n                    num_bytes_downloaded = response.num_bytes_downloaded\n\n    # check integrity of downloaded file\n    if md5 is not None and not check_md5(fpath, md5):\n        raise RuntimeError(\"MD5 validation failed.\")\n</code></pre>"},{"location":"api/utils/#nplinker.utils.extract_archive","title":"extract_archive","text":"<pre><code>extract_archive(from_path: str | PathLike, extract_root: str | PathLike | None = None, members: list | None = None, remove_finished: bool = False) -&gt; str\n</code></pre> <p>Extract an archive.</p> <p>The archive type and a possible compression is automatically detected from the file name. If the file is compressed but not an archive the call is dispatched to :func:<code>decompress</code>.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str | PathLike</code> <p>Path to the file to be extracted.</p> required <code>extract_root</code> <code>str | PathLike | None</code> <p>Path to the directory the file will be extracted to. The given directory will be created if not exist. If omitted, the directory of the archive file is used.</p> <code>None</code> <code>members</code> <code>list | None</code> <p>Optional selection of members to extract. If not specified, all members are extracted. Memers must be a subset of the list returned by - <code>zipfile.ZipFile.namelist()</code> or a list of strings for zip file - <code>tarfile.TarFile.getmembers()</code> for tar file</p> <code>None</code> <code>remove_finished</code> <code>bool</code> <p>If <code>True</code>, remove the file after the extraction.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the directory the file was extracted to.</p> Source code in <code>src/nplinker/utils.py</code> <pre><code>def extract_archive(\n    from_path: str | PathLike,\n    extract_root: str | PathLike | None = None,\n    members: list | None = None,\n    remove_finished: bool = False,\n) -&gt; str:\n    \"\"\"Extract an archive.\n\n    The archive type and a possible compression is automatically detected from\n    the file name. If the file is compressed but not an archive the call is\n    dispatched to :func:`decompress`.\n\n    Args:\n        from_path: Path to the file to be extracted.\n        extract_root: Path to the directory the file will be extracted to.\n            The given directory will be created if not exist.\n            If omitted, the directory of the archive file is used.\n        members: Optional selection of members to extract. If not specified,\n            all members are extracted.\n            Memers must be a subset of the list returned by\n            - `zipfile.ZipFile.namelist()` or a list of strings for zip file\n            - `tarfile.TarFile.getmembers()` for tar file\n        remove_finished: If `True`, remove the file after the extraction.\n\n    Returns:\n        Path to the directory the file was extracted to.\n    \"\"\"\n    from_path = Path(from_path)\n\n    if extract_root is None:\n        extract_root = from_path.parent\n    else:\n        extract_root = Path(extract_root)\n\n    # create the extract directory if not exist\n    extract_root.mkdir(exist_ok=True)\n\n    suffix, archive_type, compression = _detect_file_type(from_path)\n    if not archive_type:\n        return _decompress(\n            from_path,\n            extract_root / from_path.name.replace(suffix, \"\"),\n            remove_finished=remove_finished,\n        )\n\n    extractor = _ARCHIVE_EXTRACTORS[archive_type]\n\n    extractor(str(from_path), str(extract_root), members, compression)\n    if remove_finished:\n        from_path.unlink()\n\n    return str(extract_root)\n</code></pre>"},{"location":"api/utils/#nplinker.utils.find_delimiter","title":"find_delimiter","text":"<pre><code>find_delimiter(file: str | PathLike) -&gt; str\n</code></pre> <p>Detect the delimiter for the given tabular file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to tabular file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Detected delimiter character.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; delim = find_delimiter(\"~/table.csv\")\n</code></pre> Source code in <code>src/nplinker/utils.py</code> <pre><code>def find_delimiter(file: str | PathLike) -&gt; str:\n    \"\"\"Detect the delimiter for the given tabular file.\n\n    Args:\n        file: Path to tabular file.\n\n    Returns:\n        Detected delimiter character.\n\n    Examples:\n        &gt;&gt;&gt; delim = find_delimiter(\"~/table.csv\")\n    \"\"\"\n    sniffer = csv.Sniffer()\n    with open(file, mode=\"rt\", encoding=\"utf-8\") as fp:\n        delimiter = sniffer.sniff(fp.read(5000)).delimiter\n    return delimiter\n</code></pre>"},{"location":"api/utils/#nplinker.utils.get_headers","title":"get_headers","text":"<pre><code>get_headers(file: str | PathLike) -&gt; list[str]\n</code></pre> <p>Read headers from the given tabular file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to the file to read the header from.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of column names from the header.</p> Source code in <code>src/nplinker/utils.py</code> <pre><code>def get_headers(file: str | PathLike) -&gt; list[str]:\n    \"\"\"Read headers from the given tabular file.\n\n    Args:\n        file: Path to the file to read the header from.\n\n    Returns:\n        list[str]: list of column names from the header.\n    \"\"\"\n    with open(file) as f:\n        headers = f.readline().strip()\n        dl = find_delimiter(file)\n        return headers.split(dl)\n</code></pre>"},{"location":"api/utils/#nplinker.utils.is_file_format","title":"is_file_format","text":"<pre><code>is_file_format(file: str | PathLike, format: str = 'tsv') -&gt; bool\n</code></pre> <p>Check if the file is in the given format.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | PathLike</code> <p>Path to the file to check.</p> required <code>format</code> <code>str</code> <p>The format to check for, either \"tsv\" or \"csv\".</p> <code>'tsv'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file is in the given format, False otherwise.</p> Source code in <code>src/nplinker/utils.py</code> <pre><code>def is_file_format(file: str | PathLike, format: str = \"tsv\") -&gt; bool:\n    \"\"\"Check if the file is in the given format.\n\n    Args:\n        file: Path to the file to check.\n        format: The format to check for, either \"tsv\" or \"csv\".\n\n    Returns:\n        True if the file is in the given format, False otherwise.\n    \"\"\"\n    try:\n        with open(file, \"rt\") as f:\n            if format == \"tsv\":\n                reader = csv.reader(f, delimiter=\"\\t\")\n            elif format == \"csv\":\n                reader = csv.reader(f, delimiter=\",\")\n            else:\n                raise ValueError(f\"Unknown format '{format}'.\")\n            for _ in reader:\n                pass\n        return True\n    except csv.Error:\n        return False\n</code></pre>"},{"location":"api/utils/#nplinker.utils.list_dirs","title":"list_dirs","text":"<pre><code>list_dirs(root: str | PathLike, keep_parent: bool = True) -&gt; list[str]\n</code></pre> <p>List all directories at a given root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str | PathLike</code> <p>Path to directory whose folders need to be listed</p> required <code>keep_parent</code> <code>bool</code> <p>If true, prepends the path to each result, otherwise only returns the name of the directories found</p> <code>True</code> Source code in <code>src/nplinker/utils.py</code> <pre><code>def list_dirs(root: str | PathLike, keep_parent: bool = True) -&gt; list[str]:\n    \"\"\"List all directories at a given root.\n\n    Args:\n        root: Path to directory whose folders need to be listed\n        keep_parent: If true, prepends the path to each result, otherwise\n            only returns the name of the directories found\n    \"\"\"\n    root = transform_to_full_path(root)\n    directories = [str(p) for p in root.iterdir() if p.is_dir()]\n    if not keep_parent:\n        directories = [os.path.basename(d) for d in directories]\n    return directories\n</code></pre>"},{"location":"api/utils/#nplinker.utils.list_files","title":"list_files","text":"<pre><code>list_files(root: str | PathLike, prefix: str | tuple[str, ...] = '', suffix: str | tuple[str, ...] = '', keep_parent: bool = True) -&gt; list[str]\n</code></pre> <p>List all files at a given root.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str | PathLike</code> <p>Path to directory whose files need to be listed</p> required <code>prefix</code> <code>str | tuple[str, ...]</code> <p>Prefix of the file names to match, Defaults to empty string '\"\"'.</p> <code>''</code> <code>suffix</code> <code>str | tuple[str, ...]</code> <p>Suffix of the files to match, e.g. \".png\" or (\".jpg\", \".png\"). Defaults to empty string '\"\"'.</p> <code>''</code> <code>keep_parent</code> <code>bool</code> <p>If true, prepends the parent path to each result, otherwise only returns the name of the files found. Defaults to False.</p> <code>True</code> Source code in <code>src/nplinker/utils.py</code> <pre><code>def list_files(\n    root: str | PathLike,\n    prefix: str | tuple[str, ...] = \"\",\n    suffix: str | tuple[str, ...] = \"\",\n    keep_parent: bool = True,\n) -&gt; list[str]:\n    \"\"\"List all files at a given root.\n\n    Args:\n        root: Path to directory whose files need to be listed\n        prefix: Prefix of the file names to match,\n            Defaults to empty string '\"\"'.\n        suffix: Suffix of the files to match, e.g. \".png\" or\n            (\".jpg\", \".png\").\n            Defaults to empty string '\"\"'.\n        keep_parent: If true, prepends the parent path to each\n            result, otherwise only returns the name of the files found.\n            Defaults to False.\n    \"\"\"\n    root = Path(root)\n    files = [\n        str(p)\n        for p in root.iterdir()\n        if p.is_file() and p.name.startswith(prefix) and p.name.endswith(suffix)\n    ]\n\n    if not keep_parent:\n        files = [os.path.basename(f) for f in files]\n\n    return files\n</code></pre>"},{"location":"api/utils/#nplinker.utils.transform_to_full_path","title":"transform_to_full_path","text":"<pre><code>transform_to_full_path(p: str | PathLike) -&gt; Path\n</code></pre> <p>Transform a path to a full path.</p> <p>The path is expanded (i.e. the <code>~</code> will be replaced with actual path) and converted to an absolute path (i.e. <code>.</code> or <code>..</code> will be replaced with actual path).</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>str | PathLike</code> <p>The path to transform.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The transformed full path.</p> Source code in <code>src/nplinker/utils.py</code> <pre><code>def transform_to_full_path(p: str | PathLike) -&gt; Path:\n    \"\"\"Transform a path to a full path.\n\n    The path is expanded (i.e. the `~` will be replaced with actual path) and converted to an\n    absolute path (i.e. `.` or `..` will be replaced with actual path).\n\n    Args:\n        p: The path to transform.\n\n    Returns:\n        The transformed full path.\n    \"\"\"\n    # Multiple calls to `Path` are used to ensure static typing compatibility.\n    p = Path(p).expanduser()\n    p = Path(p).resolve()\n    return Path(p)\n</code></pre>"},{"location":"concepts/bigscape/","title":"BigScape","text":"<p>NPLinker can run BigScape automatically if the <code>bigscape</code> directory does not exist in the working directory.</p> <p>To run BigScape, NPLinker requires the following BigScape parameters:</p> <ul> <li><code>--mix</code></li> <li><code>--include_singletons</code></li> <li><code>--cutoffs</code></li> </ul> <p>And the following parameters are not allowed:</p> <ul> <li><code>--inputdir</code></li> <li><code>--outputdir</code></li> <li><code>--pfam_dir</code></li> </ul> <p>If BigScape parameter <code>--mibig</code> is set, make sure setting the  <code>mibig.to_use</code> to true in your config file <code>nplinker.toml</code> and <code>mibig.version</code> to the version of mibig used by bigscape.</p> <p>See the default configurations for the default  parameters of BigScape.</p>"},{"location":"concepts/config_file/","title":"Config File","text":""},{"location":"concepts/config_file/#configuration-template","title":"Configuration Template","text":"<pre><code>#############################\n# NPLinker configuration file\n#############################\n\n# The root directory of the NPLinker project. You need to create it first.\n# The value is required and must be a full path.\nroot_dir = \"&lt;NPLinker root directory&gt;\"\n# The mode for preparing dataset.\n# The available modes are \"podp\" and \"local\".\n# \"podp\" mode is for using the PODP platform (https://pairedomicsdata.bioinformatics.nl/) to prepare the dataset.\n# \"local\" mode is for preparing the dataset locally. So uers do not need to upload their data to the PODP platform.\n# The value is required.\nmode = \"podp\"\n# The PODP project identifier.\n# The value is required if the mode is \"podp\".\npodp_id = \"\"\n\n\n[log]\n# Log level. The available levels are same as the levels in python package `logging`:\n# \"NOTSET\", \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\".\n# The default value is \"INFO\".\nlevel = \"INFO\"\n# Redirect the log messages from stdout to a log file. If not set, the log messages will only be\n# printed to stdout.\n# The value is optional and must be a full path if set.\nfile = \"path/to/logfile\"\n# Whether to print log messages to stdout in addition to writing to the logfile.\n# The default value is true.\nto_stdout = true\n\n\n[mibig]\n# Whether to use mibig metadta (json).\n# The default value is true.\nto_use = true\n# The version of mibig metadata.\n# Make sure using the same version of mibig in bigscape.\n# The default value is \"3.1\"\nversion = \"3.1\"\n\n\n[bigscape]\n# The parameters to use for running BiG-SCAPE.\n# Required bigscape parameters are `--mix`, `--include_singletons` and `--cutoffs`. NPLinker needs\n# them to run the analysis properly.\n# Parameters that must NOT exist: `--inputdir`, `--outputdir`, `--pfam_dir`. NPLinker will\n# automatically configure them.\n# If parameter `--mibig` is set, make sure setting the config `mibig.to_use` to true and\n# `mibig.version` to the version of mibig in bigscape.\n# The default value is \"--mibig --clans-off --mix --include_singletons --cutoffs 0.30\".\nparameters = \"--mibig --clans-off --mix --include_singletons --cutoffs 0.30\"\n# Which bigscape cutoff to use for NPLinker analysis.\n# There might be multiple cutoffs in bigscape output.\n# Note that this value must be a string.\n# The default value is \"0.30\".\ncutoff = \"0.30\"\n\n\n[scoring]\n# Scoring methods.\n# Valid values are \"metcalf\" and \"rosetta\".\n# The default value is \"metcalf\".\nmethods = [\"metcalf\"]\n</code></pre>"},{"location":"concepts/config_file/#default-configurations","title":"Default Configurations","text":"<p>The default configurations are automatically used by NPLinker if you don't set them in your config file.</p> <pre><code># NPLinker default configurations\n\n[log]\nlevel = \"INFO\"\nto_stdout = true\n\n[mibig]\nto_use = true\nversion = \"3.1\"\n\n[bigscape]\nparameters = \"--mibig --clans-off --mix --include_singletons --cutoffs 0.30\"\ncutoff = \"0.30\"\n\n[scoring]\nmethods = [\"metcalf\"]\n</code></pre>"},{"location":"concepts/gnps_data/","title":"GNPS Data","text":"<p>NPLinker requires GNPS molecular networking data as input. It currently accepts data from the following  GNPS workflows:</p> <ul> <li><code>METABOLOMICS-SNETS</code> (data should be downloaded from the option <code>Download Clustered Spectra as MGF</code>)</li> <li><code>METABOLOMICS-SNETS-V2</code> (<code>Download Clustered Spectra as MGF</code>)</li> <li><code>FEATURE-BASED-MOLECULAR-NETWORKING</code> (<code>Download Cytoscape Data</code>)</li> </ul>"},{"location":"concepts/gnps_data/#mappings-from-gnps-data-to-nplinker-input","title":"Mappings from GNPS data to NPLinker input","text":"<code>METABOLOMICS-SNETS</code> workflow<code>METABOLOMICS-SNETS-V2</code><code>FEATURE-BASED-MOLECULAR-NETWORKING</code> NPLinker input GNPS file in the archive of <code>Download Clustered Spectra as MGF</code> spectra.mgf METABOLOMICS-SNETS*.mgf molecular_families.tsv networkedges_selfloop/*.pairsinfo annotations.tsv result_specnets_DB/*.tsv file_mappings.tsv clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.tsv <p>For example, the file <code>METABOLOMICS-SNETS*.mgf</code> from the downloaded zip archive is used as  the <code>spectra.mgf</code> input file of NPLinker. </p> <p>When manually preparing GNPS data for NPLinker, the <code>METABOLOMICS-SNETS*.mgf</code> must be renamed to <code>spectra.mgf</code> and placed in the <code>gnps</code> sub-directory of the NPLinker working directory.</p> NPLinker input GNPS file in the archive of <code>Download Clustered Spectra as MGF</code> spectra.mgf METABOLOMICS-SNETS-V2*.mgf molecular_families.tsv networkedges_selfloop/*.selfloop annotations.tsv result_specnets_DB/*.tsv file_mappings.tsv clusterinfosummarygroup_attributes_withIDs_withcomponentID/*.clustersummary NPLinker input GNPS file in the archive of <code>Download Cytoscape Data</code> spectra.mgf spectra/*.mgf molecular_families.tsv networkedges_selfloop/*.selfloop annotations.tsv DB_result/*.tsv file_mappings.csv quantification_table/*.csv <p>Note that <code>file_mappings.csv</code> is a CSV file, not a TSV file, different from the other workflows.</p>"},{"location":"concepts/working_dir_structure/","title":"Working Directory Structure","text":"<p>NPLinker requires a fixed structure of working directory with fixed names for the input and output data.</p> <pre><code>root_dir # (1)!\n    \u2502\n    \u251c\u2500\u2500 nplinker.toml                       [F] # (2)!\n    \u251c\u2500\u2500 strain_mappings.JSON                [F] # (3)!\n    \u251c\u2500\u2500 strains_selected.json               [F][O] # (4)!\n    \u2502\n    \u251c\u2500\u2500 gnps                                [F] # (5)!\n    \u2502       \u251c\u2500\u2500 spectra.mgf                 [F]\n    \u2502       \u251c\u2500\u2500 molecular_families.tsv      [F]\n    \u2502       \u251c\u2500\u2500 annotations.tsv             [F]\n    \u2502       \u2514\u2500\u2500 file_mappings.tsv (.csv)    [F] # (6)!\n    \u2502\n    \u251c\u2500\u2500 antismash                           [F] # (7)!\n    \u2502   \u251c\u2500\u2500 GCF_000514975.1\n    \u2502   \u2502   \u251c\u2500\u2500 xxx.region001.gbk\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 GCF_000016425.1\n    \u2502   \u2502   \u251c\u2500\u2500 xxxx.region001.gbk\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 ...\n    \u2502\n    \u251c\u2500\u2500 bigscape                            [F][O] # (8)!\n    \u2502   \u251c\u2500\u2500 mix_clustering_c0.30.tsv        [F]    # (9)!\n    \u2502   \u2514\u2500\u2500 bigscape_running_output\n    \u2502       \u2514\u2500\u2500 ...\n    \u2502\n    \u251c\u2500\u2500 downloads                           [F][A] # (10)!\n    \u2502       \u251c\u2500\u2500 paired_datarecord_4b29ddc3-26d0-40d7-80c5-44fb6631dbf9.4.json # (11)!\n    \u2502       \u251c\u2500\u2500 GCF_000016425.1.zip\n    \u2502       \u251c\u2500\u2500 GCF_0000514975.1.zip\n    \u2502       \u251c\u2500\u2500 c22f44b14a3d450eb836d607cb9521bb.zip\n    \u2502       \u251c\u2500\u2500 genome_status.json\n    \u2502       \u2514\u2500\u2500 mibig_json_3.1.tar.gz\n    \u2502\n    \u251c\u2500\u2500 mibig                               [F][A] # (12)!\n    \u2502   \u251c\u2500\u2500 BGC0000001.json\n    \u2502   \u251c\u2500\u2500 BGC0000002.json\n    \u2502   \u2514\u2500\u2500 ...\n    \u2502\n    \u251c\u2500\u2500 output                              [F][A] # (13)!\n    \u2502   \u2514\u2500\u2500 ...\n    \u2502\n    \u2514\u2500\u2500 ...                                        # (14)!\n</code></pre> <ol> <li><code>root_dir</code> is the working directory you created, used as the root directory for NPLinker.</li> <li><code>nplinker.toml</code> is the configuration file provided by the user for running NPLinker.  <code>[F]</code> means the file name <code>nplinker.toml</code> is a fixed name (including the extension) and must be     named as shown.</li> <li><code>strain_mappings.json</code> contains the mappings from strain to genomics and metabolomics data. It is     generated by NPLinker for <code>podp</code> mode; for <code>local</code> mode, users need to create it manually.</li> <li><code>strains_selected.json</code> is an optional file containing the list of strains to be used in the analysis.     If it is not provided, NPLinker will use all strains detected from the input data.  <code>[O]</code> means the file <code>strains_selected.json</code> is optional for users to provide.</li> <li><code>gnps</code> directory contains the GNPS data. The files in this directory must be named as shown.     See XXX for more information about the GNPS data.</li> <li>This file could be <code>.tsv</code> or <code>.csv</code> format.</li> <li><code>antismash</code> directory contains a collection of AntiSMASH BGC data. The BGC data (<code>*.region*.gbk</code>      files) must be stored in subdirectories named after NCBI accession number (e.g. <code>GCF_000514975.1</code>).</li> <li><code>bigscape</code> directory is optional and contains the output of BigScape. If the directory is not     provided, NPLinker will run BigScape automatically to generate the data using the AntiSMASH BGC     data.</li> <li><code>mix_clustering_c0.30.tsv</code> is an example output of BigScape. The file name must follow the pattern     <code>mix_clustering_c{cutoff}.tsv</code>, where <code>{cutoff}</code> is the cutoff value used in the BigScape run.</li> <li><code>downloads</code> directory is automatically created and managed by NPLinker. It stores the downloaded data    from the internet. Users can also use it to store their own downloaded data.  <code>[A]</code> means the directory is automatically created and/or managed by NPLinker.</li> <li>This is an example file, the actual file would be different. Same as the other files in     the <code>downloads</code> directory.</li> <li><code>mibig</code> directory contains the MIBiG metadata, which is automatically created and downloaded by      NPLinker. Users should not interfere with this directory and its content.</li> <li><code>output</code> directory is automatically created by NPLinker. It stores the output data of NPLinker.</li> <li>It's flexible to extend NPLinker by adding other types of data.</li> </ol> <p>Tip</p> <ul> <li><code>[F]</code> means the file or directory name is fixed and must be named as shown.</li> <li><code>[O]</code> means the file or directory is optional for users to provide. It does not mean the file or directory is optional for NPLinker to use. If it's not provided by the user, NPLinker may generate it.</li> <li><code>[A]</code> means the directory is automatically created and/or managed by NPLinker.</li> </ul>"},{"location":"diagrams/arranger/","title":"Dataset Arranging Pipeline","text":"<p>The DatasetArranger is implemented according to the following flowcharts.</p>"},{"location":"diagrams/arranger/#strain-mappings-file","title":"Strain mappings file","text":"<pre><code>flowchart TD\n    StrainMappings[`strain_mappings.json`] --&gt; SM{Is the mode PODP?}\n    SM --&gt; |No |SM0[Validate the file]\n    SM --&gt; |Yes|SM1[Generate the file] --&gt; SM0</code></pre>"},{"location":"diagrams/arranger/#strain-selection-file","title":"Strain selection file","text":"<pre><code>flowchart TD\n    StrainsSelected[`strains_selected.json`] --&gt; S{Does the file exist?}\n    S --&gt; |No | S0[Nothing to do]\n    S --&gt; |Yes| S1[Validate the file]</code></pre>"},{"location":"diagrams/arranger/#podp-project-metadata-json-file","title":"PODP project metadata json file","text":"<pre><code>flowchart TD\n    podp[PODP project metadata json file] --&gt; A{Is the mode PODP?}\n    A --&gt; |No | A0[Nothing to do]\n    A --&gt; |Yes| P{Does the file exist?}\n    P --&gt; |No | P0[Download the file] --&gt; P1\n    P --&gt; |Yes| P1[Validate the file]</code></pre>"},{"location":"diagrams/arranger/#gnps-antismash-and-bigscape","title":"GNPS, AntiSMASH and BigScape","text":"<pre><code>flowchart TD\n    ConfigError[Dynaconf config validation error]\n    DataError[Data validation error]\n    UseIt[Use the data]\n    Download[First remove existing data if relevent, then download or generate data]\n\n    A[GNPS, antiSMASH and BigSCape] --&gt; B{Pass Dynaconf config validation?}\n    B --&gt;|No | ConfigError\n    B --&gt;|Yes| G{Is the mode PODP?}\n\n    G --&gt;|No, local mode| G1{Does data dir exist?}\n    G1 --&gt;|No | DataError\n    G1 --&gt;|Yes| H{Pass data validation?}\n    H --&gt; |No | DataError\n    H --&gt; |Yes| UseIt \n\n    G --&gt;|Yes, podp mode| G2{Does data dir exist?}\n    G2 --&gt; |No | Download\n    G2 --&gt; |Yes | J{Pass data validation?}\n    J --&gt;|No | Download --&gt; |try max 2 times| J\n    J --&gt;|Yes| UseIt</code></pre>"},{"location":"diagrams/arranger/#mibig-data","title":"MIBiG Data","text":"<p>MIBiG data is always downloaded automatically. Users cannot provide their own MIBiG data.</p> <pre><code>flowchart TD\n    Mibig[MIBiG] --&gt; M0{Pass Dynaconf config validation?}\n    M0 --&gt;|No | M01[Dynaconf config validation error]\n    M0 --&gt;|Yes | MibigDownload[First remove existing data if relevant and then download data]</code></pre>"}]}